<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Network | jintao's blog]]></title>
  <link href="http://jintao-zero.github.io/blog/tags/network/atom.xml" rel="self"/>
  <link href="http://jintao-zero.github.io/"/>
  <updated>2015-11-20T20:14:57+08:00</updated>
  <id>http://jintao-zero.github.io/</id>
  <author>
    <name><![CDATA[jintao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[libnetfilter_queue学习笔记]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/06/16/libnetfilter-queuexue-xi-bi-ji/"/>
    <updated>2015-06-16T18:53:42+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/06/16/libnetfilter-queuexue-xi-bi-ji</id>
    <content type="html"><![CDATA[<p>项目需要将某个进程发出的报文截获，待用户态业务处理后再将报文送回系统协议栈中处理。调研后，采用iptables添加防火墙规则截获报文传递到用户态队列，用户态进程使用libnetfilter_queue库接受报文，待业务处理后，将报文送回系统协议栈这样的方案完成。下面介绍下，学习的几个知识点：</p>

<h1>netfilter</h1>

<h2>简介</h2>

<p><a href="http://netfilter.org/">netfilter</a>是Linux 2.4.x开始以后内核版本中的报文过滤框架。netfilter框架的用户态工具是<a href="http://netfilter.org/projects/iptables/index.html">iptables</a>防火墙配置工具。<br/>
netfilter框架实现了报文过滤，网络地址转换，端口转换和其他报文管理。它是对Linux 2.2.x ipchains和Linux 2.0.x ipfwadm系统的重构和显著改进。<br/>
netfilter是位于Linux内核中的一组回调函数集合，各个模块可以在网络协议栈注册回调函数。每个报文到来时，将会遍历协议栈中的回调函数并调用。<br/>
iptables定义了规则集的通用格式。IP table中的每一条规则是有一些分类和相关的目标组成。<br/>
netfilter，ip_tables，connection tracking（ip_conntrack，nf_conntrack）和NAT子系统是netfilter的主要组成部分。</p>

<h2>主要特性</h2>

<ul>
<li>无状态报文过滤</li>
<li>有状态报文过滤</li>
<li>所有网络地址和端口转换，比如NAT/NAPT(IPv4和IPv6)</li>
<li>灵活的、可扩展的框架</li>
<li>为第三方扩展提供API访问网络各层</li>
</ul>


<h2>可以做哪些</h2>

<ul>
<li>基于无状态或者有状态报文过滤构建互联网防火墙</li>
<li>托管高可用有状态或者无状态的防火墙集群</li>
<li>使用NAT和ip伪装共享共网ip</li>
<li>使用NAT实现传输代理</li>
<li>与tc和iproute2系统一起构建复杂Qos和策略路由器</li>
<li>更进一步进行报文管理，比如修改IP头中的TOS/DSCP/ECN</li>
</ul>


<!-- more -->


<h1>libnetfilter_queue</h1>

<p>libnetfilter_queue是一个用户态库，它提供了接口用来访问已经被内核报文过滤模块放置在队列中的报文。libnetfilter_queue的前身是libnfnetlink_queue。<br/>
libnetfilter_queue需要libnfnetlink和包含nfnetlink_queue子系统的内核（2.6.14以后）。<br/>
主要特性：</p>

<ul>
<li>接收来自内核nfnetlink_queue子系统的报文</li>
<li>做出裁决，是否将修改后的报文重新注入内核nfnetlink_queue子系统</li>
</ul>


<h2>安装</h2>

<p>libnetfilter_queue需要libnfnetlink和libmnl库<br/>
1、安装libnfnetlink</p>

<pre><code>#wget http://netfilter.org/projects/libnfnetlink/files/libnfnetlink-1.0.1.tar.bz2
#tar jxvf libnfnetlink-1.0.1.tar.bz2
#cd libnfnetlink-1.0.1  
#./configure
#make &amp;&amp; make install
</code></pre>

<p>2、安装libmnl</p>

<pre><code>#wget http://netfilter.org/projects/libmnl/files/libmnl-1.0.3.tar.bz2
#tar jxvf libmnl-1.0.3.tar.bz2
#cd libmnl-1.0.3
#./configure
#make &amp;&amp; make install
</code></pre>

<p>3、安装libnetfilter_queue</p>

<pre><code>#wget http://netfilter.org/projects/libnetfilter_queue/files/libnetfilter_queue-1.0.2.tar.bz2
#tar jxvf libnetfilter_queue-1.0.2.tar.bz2
#cd libnetfilter_queue-1.0.2
#./configure
#make &amp;&amp; make install
</code></pre>

<h2>接口</h2>

<h3>初始化库</h3>

<pre><code>struct nfq_handle *     nfq_open (void)
int     nfq_close (struct nfq_handle *h)
int     nfq_bind_pf (struct nfq_handle *h, u_int16_t pf)
int     nfq_unbind_pf (struct nfq_handle *h, u_int16_t pf)
</code></pre>

<p>libnetfilter_queue初始化分为两部。<br/>
第一步调用nfq_open打开一个NFQUEUE句柄。<br/>
第二部通知内核指定协议的用户空间队列由NFQUEUE处理。调用nfq_unbind_pf()和nfq_bind接口进行绑定。<br/>
以下为初始化片段：</p>

<pre><code>h = nfq_open();
if (!h) {
    fprintf(stderr, "error during nfq_open()\n");
    exit(1);
}

printf("unbinding existing nf_queue handler for AF_INET (if any)\n");
if (nfq_unbind_pf(h, AF_INET) &lt; 0) {
    fprintf(stderr, "error during nfq_unbind_pf()\n");
    exit(1);
}

printf("binding nfnetlink_queue as nf_queue handler for AF_INET\n");
if (nfq_bind_pf(h, AF_INET) &lt; 0) {
    fprintf(stderr, "error during nfq_bind_pf()\n");
    exit(1);
}
</code></pre>

<p>nfq_open</p>

<pre><code>struct nfq_handle* nfq_open (void) 
</code></pre>

<p>nfq_open返回一个netfilter队列连接句柄。当使用完毕，调用nfq_close销毁句柄。在系统内部，会新创建一个netlink连接并与队列连接句柄相关联。</p>

<p>nfq_bind_pf</p>

<pre><code>int nfq_bind_pf (   struct nfq_handle *     h,
                    u_int16_t   pf   
                )   
</code></pre>

<p>nfq_bind_pf绑定一个协议域到nfqueue句柄。<br/>
队列连接句柄将会处理属于pf指定的协议报文，比如（PF_INET，PF_INET6）。
<a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/group__LibrarySetup.html">详细说明文档</a></p>

<h3>处理报文队列</h3>

<pre><code>int     nfq_fd (struct nfq_handle *h)   
struct nfq_q_handle *   nfq_create_queue (struct nfq_handle *h, u_int16_t num, nfq_callback *cb, void *data)
int     nfq_destroy_queue (struct nfq_q_handle *qh)
int     nfq_handle_packet (struct nfq_handle *h, char *buf,     int len)
int     nfq_set_mode (struct nfq_q_handle *qh, u_int8_t mode, u_int32_t range)
int     nfq_set_queue_maxlen (struct nfq_q_handle *qh, u_int32_t queuelen)
int     nfq_set_verdict (struct nfq_q_handle *qh, u_int32_t id, u_int32_t verdict, u_int32_t data_len, const unsigned char *buf)
int     nfq_set_verdict2 (struct nfq_q_handle *qh, u_int32_t id, u_int32_t verdict, u_int32_t mark, u_int32_t data_len, const unsigned char *buf)
int     nfq_set_verdict_mark (struct nfq_q_handle *qh, u_int32_t id, u_int32_t verdict, u_int32_t mark, u_int32_t data_len, const unsigned char *buf)
</code></pre>

<p>libnetfilter_queue库初始化后，可以绑定程序到具体报文队列。使用nfq_create_queue()进行绑定。
使用nfq_set_mode()和nfq_set_queue_maxlen()函数设置队列。
下面是创建队列0的代码片段：</p>

<pre><code>printf("binding this socket to queue '0'\n");
qh = nfq_create_queue(h,  0, &amp;cb, NULL);
if (!qh) {
    fprintf(stderr, "error during nfq_create_queue()\n");
    exit(1);
}

printf("setting copy_packet mode\n");
if (nfq_set_mode(qh, NFQNL_COPY_PACKET, 0xffff) &lt; 0) {
    fprintf(stderr, "can't set packet_copy mode\n");
    exit(1);
}
</code></pre>

<p>下一步是，循环读取队列中的报文进行处理：</p>

<pre><code>fd = nfq_fd(h);

while ((rv = recv(fd, buf, sizeof(buf), 0)) &gt;= 0) {
    printf("pkt received\n");
    nfq_handle_packet(h, buf, rv);
}
</code></pre>

<p>我们需要对每一个报文做出处理结果，通过nfq_set_verdict()或者nfq_set_verdict2()结果来传递结果。裁决结果决定报文的命运：</p>

<ul>
<li>NF_DROP   丢弃此报文</li>
<li>NF_ACCEPT 接受报文，继续迭代</li>
<li>NF_STOLEN gone away</li>
<li>NF_QUEUE  将报文注入另外一个队列</li>
<li>NF_REPEAT 重新再迭代一次</li>
<li>NF_STOP 接受，但是不再继续迭代</li>
</ul>


<p><a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/group__Queue.html">详细说明文档</a></p>

<h3>解析报文</h3>

<p>主要涉及以下接口：</p>

<pre><code>struct nfqnl_msg_packet_hdr *   nfq_get_msg_packet_hdr (struct nfq_data *nfad)
uint32_t    nfq_get_nfmark (struct nfq_data *nfad)
int     nfq_get_timestamp (struct nfq_data *nfad, struct timeval *tv)
u_int32_t   nfq_get_indev (struct nfq_data *nfad)
u_int32_t   nfq_get_physindev (struct nfq_data *nfad)
u_int32_t   nfq_get_outdev (struct nfq_data *nfad)
u_int32_t   nfq_get_physoutdev (struct nfq_data *nfad)
int     nfq_get_indev_name (struct nlif_handle *nlif_handle,    struct nfq_data *nfad, char *name)
int     nfq_get_physindev_name (struct nlif_handle *nlif_handle, struct nfq_data *nfad, char *name)
int     nfq_get_outdev_name (struct nlif_handle *nlif_handle, struct nfq_data *nfad, char *name)
int     nfq_get_physoutdev_name (struct nlif_handle *nlif_handle, struct nfq_data *nfad, char *name)
struct nfqnl_msg_packet_hw *    nfq_get_packet_hw (struct nfq_data *nfad)
int     nfq_get_payload (struct nfq_data *nfad, unsigned char **data)
</code></pre>

<p><a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/group__Parsing.html">详细说明文档</a></p>

<h2>示例</h2>

<p>示例源自libnetfilter_queue源代码中的例子<a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/nfqnl__test_8c_source.html">nfqnl_test.c</a></p>

<pre><code>00001 
00002 #include &lt;stdio.h&gt;
00003 #include &lt;stdlib.h&gt;
00004 #include &lt;unistd.h&gt;
00005 #include &lt;netinet/in.h&gt;
00006 #include &lt;linux/types.h&gt;
00007 #include &lt;linux/netfilter.h&gt;            /* for NF_ACCEPT */
00008 
00009 #include &lt;libnetfilter_queue/libnetfilter_queue.h&gt;
00010 
00011 /* returns packet id */
00012 static u_int32_t print_pkt (struct nfq_data *tb)
00013 {
00014         int id = 0;
00015         struct nfqnl_msg_packet_hdr *ph;
00016         struct nfqnl_msg_packet_hw *hwph;
00017         u_int32_t mark,ifi; 
00018         int ret;
00019         unsigned char *data;
00020 
00021         ph = nfq_get_msg_packet_hdr(tb); // 返回报文头信息
00022         if (ph) {
00023                 id = ntohl(ph-&gt;packet_id);
00024                 printf("hw_protocol=0x%04x hook=%u id=%u ",
00025                         ntohs(ph-&gt;hw_protocol), ph-&gt;hook, id);
00026         }
00027 
00028         hwph = nfq_get_packet_hw(tb); // 返回报文硬件地址信息
00029         if (hwph) {
00030                 int i, hlen = ntohs(hwph-&gt;hw_addrlen);
00031 
00032                 printf("hw_src_addr=");
00033                 for (i = 0; i &lt; hlen-1; i++)
00034                         printf("%02x:", hwph-&gt;hw_addr[i]);
00035                 printf("%02x ", hwph-&gt;hw_addr[hlen-1]);
00036         }
00037 
00038         mark = nfq_get_nfmark(tb); // 返回报文标记
00039         if (mark)
00040                 printf("mark=%u ", mark);
00041 
00042         ifi = nfq_get_indev(tb); // 获取报文接收网卡的索引
00043         if (ifi)
00044                 printf("indev=%u ", ifi);
00045 
00046         ifi = nfq_get_outdev(tb); // 获取报文发送网卡的索引
00047         if (ifi)
00048                 printf("outdev=%u ", ifi); 
00049         ifi = nfq_get_physindev(tb); // 接收此报文的物理网卡索引
00050         if (ifi)
00051                 printf("physindev=%u ", ifi);
00052  
00053         ifi = nfq_get_physoutdev(tb); // 发送次报文的物理网卡索引
00054         if (ifi)
00055                 printf("physoutdev=%u ", ifi);
00056 
00057         ret = nfq_get_payload(tb, &amp;data); // 获取报文内容的长度
00058         if (ret &gt;= 0)
00059                 printf("payload_len=%d ", ret);
00060 
00061         fputc('\n', stdout);
00062 
00063         return id;
00064 }
00065         
00066 
00067 static int cb(struct nfq_q_handle *qh, struct nfgenmsg *nfmsg,
00068               struct nfq_data *nfa, void *data)
00069 {
00070         u_int32_t id = print_pkt(nfa);
00071         printf("entering callback\n");
00072         return nfq_set_verdict(qh, id, NF_ACCEPT, 0, NULL);
00073 }
00074 
00075 int main(int argc, char **argv)
00076 {
00077         struct nfq_handle *h;
00078         struct nfq_q_handle *qh;
00079         struct nfnl_handle *nh;
00080         int fd;
00081         int rv;
00082         char buf[4096] __attribute__ ((aligned));
00083           
00084         printf("opening library handle\n");
00085         h = nfq_open(); // 获取libnetfilter 队列句柄
00086         if (!h) {
00087                 fprintf(stderr, "error during nfq_open()\n");
00088                 exit(1);
00089         }
00090 
00091         printf("unbinding existing nf_queue handler for AF_INET (if any)\n");
00092         if (nfq_unbind_pf(h, AF_INET) &lt; 0) { // 先将队列连接句柄与AF_INET地址域解绑
00093                 fprintf(stderr, "error during nfq_unbind_pf()\n");
00094                 exit(1);
00095         }
00096 
00097         printf("binding nfnetlink_queue as nf_queue handler for AF_INET\n");
00098         if (nfq_bind_pf(h, AF_INET) &lt; 0) { // 将队列连接句柄与AF_INET地址绑定
00099                 fprintf(stderr, "error during nfq_bind_pf()\n");
00100                 exit(1);
00101         }
00102 
00103         printf("binding this socket to queue '0'\n");
00104         qh = nfq_create_queue(h,  0, &amp;cb, NULL); // 创建队列句柄用来处理0队列报文
00105         if (!qh) {
00106                 fprintf(stderr, "error during nfq_create_queue()\n");
00107                 exit(1);
00108         }
00109 
00110         printf("setting copy_packet mode\n");
00111         if (nfq_set_mode(qh, NFQNL_COPY_PACKET, 0xffff) &lt; 0) { // 设置队列工作模式
00112                 fprintf(stderr, "can't set packet_copy mode\n");
00113                 exit(1);
00114         }
00115 
00116         fd = nfq_fd(h); // 获取队列连接描述符
00117 
00118         while ((rv = recv(fd, buf, sizeof(buf), 0)) &amp;&amp; rv &gt;= 0) {
00119                 printf("pkt received\n");
00120                 nfq_handle_packet(h, buf, rv); // 处理收到的每个报文
00121         }
00122 
00123         printf("unbinding from queue 0\n");
00124         nfq_destroy_queue(qh); // 从队列0解绑
00125 
00126 #ifdef INSANE
00127         /* normally, applications SHOULD NOT issue this command, since
00128          * it detaches other programs/sockets from AF_INET, too ! */
00129         printf("unbinding from AF_INET\n");
00130         nfq_unbind_pf(h, AF_INET);
00131 #endif
00132 
00133         printf("closing library handle\n");
00134         nfq_close(h); // 关闭库句柄
00135 
00136         exit(0);
00137 }
</code></pre>

<p>编译程序：</p>

<pre><code>gcc nfqnl_test.c -lnetfilter_queue -o nfqnl_test
</code></pre>

<p>配置iptables：
使用iptables命令修改防火墙规则，将发送到112.80.248.74的报文放置到队列0中：</p>

<pre><code>#iptables -t filter -A OUTPUT -d 112.80.248.74 -j NFQUEUE --queue-num 0
#iptables -t filter -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
NFQUEUE    all  --  anywhere             112.80.248.74  NFQUEUE num 0
</code></pre>

<p>启动ping程序：</p>

<pre><code># ping 112.80.248.74
PING 112.80.248.74 (112.80.248.74) 56(84) bytes of data.
</code></pre>

<p>当nfqnl_test程序未启动时，ping程序不能正确发送出icmp报文，也就不能收到icmp回复。</p>

<p>启动nfqnl_test程序：</p>

<pre><code># ./nfqnl_test
opening library handle
unbinding existing nf_queue handler for AF_INET (if any)
binding nfnetlink_queue as nf_queue handler for AF_INET
binding this socket to queue '0'
setting copy_packet mode
pkt received
hw_protocol=0x0000 hook=3 id=1 outdev=2 payload_len=84
entering callback
pkt received
hw_protocol=0x0000 hook=3 id=2 outdev=2 payload_len=84
</code></pre>

<p>nfqnl_test程序从nfqueue_num 0中读取报文，打印相关信息后，将报文送回协议栈，完成了发送流程，从目标收到了icmp报文回复</p>

<pre><code>64 bytes from 112.80.248.74: icmp_req=612 ttl=58 time=1.84 ms
64 bytes from 112.80.248.74: icmp_req=613 ttl=58 time=1.83 ms
64 bytes from 112.80.248.74: icmp_req=614 ttl=58 time=1.76 ms
64 bytes from 112.80.248.74: icmp_req=615 ttl=58 time=1.83 ms
64 bytes from 112.80.248.74: icmp_req=616 ttl=58 time=1.97 ms
64 bytes from 112.80.248.74: icmp_req=617 ttl=58 time=1.92 ms
64 bytes from 112.80.248.74: icmp_req=618 ttl=58 time=2.07 ms   
</code></pre>

<h1>参考</h1>

<p>netfilter<a href="http://www.netfilter.org/">官网</a><br/>
<a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=1196223">用户态修改网络数据包例子</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp server使用kevent进行io复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong/"/>
    <updated>2015-05-01T08:40:25+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong</id>
    <content type="html"><![CDATA[<h1>概述</h1>

<p>kqueue是FreeBSD系统中引入的可扩展事件通知接口，NetBSD，OpenBSD，DragonflyBSD和OS X这些系统也支持此接口。传统的io复用接口如select和poll存在以下两个缺点：</p>

<ul>
<li>支持的文件描述符数量有限，select接口支持的文件描述个数与进程能够打开的最大文件个数有关</li>
<li>当大量描述符需要复用时，select和poll的效率会降低，内核是采用轮询方式判断描述符是否可用</li>
</ul>


<p>kqueue是针对传统传统的select/poll处理大量的文件描述符性能较低效而开发出来的。注册一批描述符到kqueue以后，当其中的描述符状态发生变化时，kqueue将一次性通知应用程序哪些描述符可读、可写或者发生错误。</p>

<p>kqueue支持多种类型的文件描述符，包括socket、信号、定时器、AIO、VNODE、PIPE。</p>

<h1>接口</h1>

<h2>kqueue</h2>

<pre><code>int kqueue(void);
</code></pre>

<p>kqueue系统调用创建一个新的内核事件队列并返回该队列描述符。调用fork时，子进程不继承该描述符。<br/>
失败时，系统调用返回-1,同时设置errno</p>

<h2>kevent</h2>

<pre><code>int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout);
</code></pre>

<p>使用kevent系统调用向队列注册事件和返回已经触发的事件。</p>

<ul>
<li>kq<br/>
kq为kqueue系统调用返回的队列描述符</li>
<li>changelist<br/>
changelist指向一个kevnet结构体数组，kevent定义在&lt;sys/event.h>头文件中，这个数组描述了需要对kq队列中的事件进行修改，包括添加、删除、修改等操作</li>
<li>nchanges<br/>
nchanges是changelist参数指向数组的大小</li>
<li>evnetlist<br/>
指向一个kevnet结构体数组，用于返回已经触发的事件</li>
<li>nevents<br/>
定义eventlist数组大小</li>
<li><p>timeout
timeout是非NULL指针时，定义系统调用超时间隔。timeout为NULL时，kevent系统调用无限阻塞等待事件发生。timeout非NULL，但是值为0时，kevent立即返回</p>

<p><!-- more --></p></li>
</ul>


<h2>EV_SET</h2>

<pre><code>EV_SET(&amp;kev, ident, filter, flags, fflags, data, udata);
struct kevent {
         uintptr_t       ident;          /* identifier for this event */
         int16_t         filter;         /* filter for event */
         uint16_t        flags;          /* general flags */
         uint32_t        fflags;         /* filter-specific flags */
         intptr_t        data;           /* filter-specific data */
         void            *udata;         /* opaque user data identifier */
 };
</code></pre>

<p>EV_SET宏用来初始化一个kevent结构体。下面详细介绍结构体各个字段</p>

<ul>
<li>ident
ident用来标识一个事件，ident值的准确含义由filter字段进行解释，ident值通常代表一个文件描述符</li>
<li><p>filter
内核filter定义的过滤器处理发生在ident上的事件。预定义的系统过滤器如下，通过kevent结构体中的flags和data字段传递参数</p>

<p>EVFILT_READ <br/>
文件描述符作为标识符，当描述符可读时返回。根据文件描述符类型不同，触发该过滤器的情况不同：</p>

<ul>
<li>sockets:<br/>
处于监听状态的socket套接字，当存在待处理的建立连接请求（已经完成三次握手）时，该套接字可读，kevent结构体的data字段包含监听队列中待处理的请求个数。<br/>
非监听状态的套接字缓冲区中有可读数据时，触发该过滤器，每个套接字对应的缓冲区可读低水位可以通过传递fflags和data参数进行设置。</li>
<li>Vnodes<br/>
当文件指针没有达到文件末尾时，触发该过滤器。</li>
<li>Fifos, Pipes<br/>
当管道可读时，触发该过滤器。data字段返回可读字节数。</li>
</ul>


<p>EVFILT_WRITE  <br/>
文件描述符作为标识符。当文件描述符可写时，触发该过滤器。对于sockets、pipes和fifos，data字段返回缓冲区中可写字节数。  <br/>
EVFILT_AIO<br/>
尚不支持<br/>
EVFILT_VNODE  <br/>
文件描述符作为标识符，fflags字段定义需要监测的事件类型：</p>

<ul>
<li>NOTE_DELETE  <br/>
unlink系统调用操作文件描述符指向的文件时，事件发生。</li>
<li>NOTE_WRITE  <br/>
对文件描述指向的文件进行写操作</li>
<li>NOTE_EXTEND<br/>
对文件描述符指向的文件进行extened</li>
<li>NOTE_ATTRIB<br/>
文件描述符指向的文件属性发生了变化</li>
<li>NOTE_LINK<br/>
文件描述指向的文件链接数发生了变化</li>
<li>NOTE_RENAME<br/>
文件描述符指向的文件发生了重命名</li>
<li>NOTE_REVOKE</li>
</ul>


<p>kevent系统调用返回时，fflags包含触发过滤器的事件</p>

<p>EVFILT_PROC
进程id作为标识符。监测的事件定义在fflags字段中：</p>

<ul>
<li>NOTE_EXIT  <br/>
进程退出</li>
<li>NOTE_EXITSTATUS<br/>
进程已经退出，退出状态</li>
<li>NOTE_FORK<br/>
进程调用fork或者其他类似接口创建子进程</li>
<li>NOTE_EXEC<br/>
进程调用execve或者其他类似接口执行一个新进程</li>
<li>NOTE_SIGNAL<br/>
进程被发送一个信号。<br/>
kevent系统调用返回时，fflags包含触发过滤器的事件。</li>
</ul>


<p>EVFILT_SIGNAL<br/>
将信号值作为标识符，当该信号发送到进程时，触发过滤器。</p>

<p>EVFILT_TIMER
设置一个ident标识的定时器。当添加一个定时器时，data字段指定定时器超时事件，fflags字段可以为以下设置：</p>

<ul>
<li>NOTE_SECONDS <br/>
data字段值单位为秒</li>
<li>NOTE_USECONDS<br/>
data字段值单位为毫米</li>
<li>NOTE_NSECONDS<br/>
data字段值单位为纳秒</li>
<li>NOTE_ABSOLUTE
data字段值为绝对时间</li>
</ul>
</li>
<li><p>flags<br/>
flags表示对该事件做的操作。</p>

<ul>
<li>EV_ADD<br/>
添加该事件到kqueue队列</li>
<li>EV_ENABLE
如果该事件触发，kevent返回该事件</li>
<li>EV_DISABLE
如果该事件触发，kevent不返回该事件</li>
<li>EV_DELETE
从kevent队列中删除该事件</li>
<li>EV_RECEIPT
对kqueue进行批量修改，而不返回任何待处理事件。</li>
<li>EV_ONESHOT
过滤器第一次触发后，返回该事件，然后将该事件从kqueue队列中删除。</li>
<li>EV_CLEAR
用户获取事件后，重置事件状态。</li>
<li>EV_ERROR
系统处理事件出错时，返回该事件event，将flags设置为EV_ERROR</li>
</ul>
</li>
<li><p>fflags<br/>
过滤器相关的标志</p></li>
<li>data<br/>
过滤器相关的数值</li>
<li>udata
用户定义的值，当事件返回时，内核将该值带给用户</li>
</ul>


<p>将<a href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/">tcp server 使用select进行并发</a>示例改为使用kevent进行i/o多路复用，主要涉及以下方面的修改：</p>

<p>1、套接字注册</p>

<pre><code>int Register(int kq, int fd)
{
    struct kevent ke;
    EV_SET(&amp;ke, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
    return kevent(kq, &amp;ke, 1, NULL, 0, NULL);
}
</code></pre>

<p> 目前只是对监听套接字和客户端连接套接字注册可读过滤器</p>

<p>2、循环调用kevent处理套接字事件</p>

<pre><code>struct kevent events[MAX_EVENT_COUNT];
for ( ; ; ) {
    int nevent = kevent(kq, NULL, 0, events, MAX_EVENT_COUNT, NULL);
}

void HandleEvent(int kq, struct kevent * events, int nevents) 
{
    for (int i = 0; i &lt; nevents; i++) {
        int sock = events[i].ident;
        int data = events[i].data;
        if (sock == serv_sock) {
            Accept(kq, data);
        } else {
            Receive(sock, data);
        }
    }
}

void Accept(int kq, int conn_size)
{
    for (int i = 0; i &lt; conn_size; i++) {
      int client = accept(serv_sock, NULL, NULL);
      Register(kq, client);
    }
}

void Receive(int sock, int buf_size)
{
    if (0 == buf_size)
        close(sock); // close a file descriptor removing any            kevents that reference the descriptor

    char *buf = malloc(buf_size);
    if (NULL == buf)
        return;

    recv(sock, buf, buf_size, 0);
    for (int i = 0; i &lt; buf_size; i++)
        buf[i] = toupper(buf[i]);
    send(sock, buf, buf_size, 0);
    free(buf);
}    
</code></pre>

<h1>结束语</h1>

<p>这篇文章只是简单介绍了kevent I/O多路复用模型的简单用法，上述例子中涉及的一些细节，如错误处理等都尚须完善<br/>
参考<a href="http://www.ibm.com/developerworks/cn/aix/library/1105_huangrg_kqueue/index.html#resources">使用 kqueue 在 FreeBSD 上开发高性能应用服务器</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp server使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-12T15:30:03+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi/">tcp服务端示例</a>中，接受一个客户端连接，调用accept函数返回后就对本客户端连接进行业务处理，如果业务逻辑比较复杂，那么服务端有可能就阻塞在某个客户端上，而不能处理其他客户端连接，解决此问题有多种模型可以采用：<br/>
1）多进程，为每个客户端连接创建一个子进程进行处理<br/>
2）多线程，多个线程并行处理客户端连接<br/>
3）I/O多路复用，服务器采用单线程模型，使用I/O多路复用模型，并行处理多个客户端文件描述符<br/>
在上一篇示例<a href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/">tcp client 使用select进行I/O复用</a>中介绍的select函数同样适用与对tcp server端的I/O多路复用改造，改造主要设计几下几个方面：</p>

<!-- more -->


<p>1）将server监听套接字添加到readfds套接字集合中，监听套接字可读时，调用accept接受一个新的客户端连接，并将添加到readfds套接字集合中</p>

<pre><code>if (FD_ISSET(sock_server, &amp;rfds)) {
    struct sockaddr_in clientaddr;
    socklen_t clientaddr_len = sizeof(clientaddr);
    int clientsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len);
    if (clientsock &lt; 0) {
        printf("accept fail. reason:%s\n", strerror(errno));
        continue;
    } else {
        printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);
        int i;
        for (i = 0; i &lt; FD_SETSIZE; i++) {
            if (clientfds[i] == -1) {
                clientfds[i] = clientsock;
                break;
            }
        }
        if (i == FD_SETSIZE) {
            printf("too many clients, ignore this one\n");
            continue;
        }
        FD_SET(clientfds[i], &amp;allfds);
        if (clientfds[i] &gt; maxfd)
            maxfd = clientfds[i];
    }
}  
</code></pre>

<p>2）当每个客户端套接字可读时，对套接字调用read操作后，继续执行select判断是否有套接字可读，而不是处理一个客户端连接直到完成。</p>

<pre><code>for (int i = 0; i &lt; FD_SETSIZE; i++) {
    if (readyfds == 0)
        break;
    if (clientfds[i] &lt; 0)
        continue;
    if (FD_ISSET(clientfds[i], &amp;rfds)) {
        readyfds--;
        char buf[LINE_MAX];
        ssize_t rcv_len;
        rcv_len = recv(clientfds[i], buf, sizeof(buf), 0);
        if (rcv_len &gt; 0) {
            buf[rcv_len] = '\0';
            printf("recv from client(%d), msg: %s\n", clientfds[i], buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
                send(clientfds[i], buf, rcv_len, 0);
                printf("send to client:%s", buf);
            } else if (rcv_len == 0) {
                printf("recv_len == 0 from client:%d\n", clientfds[i]);
                FD_CLR(clientfds[i], &amp;allfds);
                close(clientfds[i]);
                clientfds[i] = -1;
            } else {
                printf("recv from clientfd(%d) fail. reason:%s\n", clientfds[i], strerror(errno));
                continue;
            }
        }
    }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp client 使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-06T11:49:13+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/10/tcp-ke-hu-duan-shi-li/">tcp客户端示例</a>中，存在以下两种情况时，示例代码不能更好的处理：</p>

<ul>
<li><p>目前客户端输入一条数据并发送，等待服务端返回后，才能继续输入数据
 这样的设计不利于客户端批量处理数据</p></li>
<li><p>当服务端出现异常（如服务进程退出或者服务端主机崩溃等异常情况），客户端也许正在等待输入，不能够及时感知到服务端的异常，只有从send返回错误时，才能感知到服务端故障</p></li>
</ul>


<h1>select 函数介绍</h1>

<p>上面实现的客户端示例中，客户端只能处理一个描述符，我们需要修改客户端使其可以同时处理标准输入和套接字描述符，达到I/O复用：</p>

<pre><code>#include &lt;sys/select.h&gt;
#include &lt;sys/time.h&gt;
int select(int nfds, fd_set *restrict readfds, fd_set *restrict writefds, fd_set *restrict errorfds,
     struct timeval *restrict timeout);
</code></pre>

<ul>
<li><p>timeout参数，它告知内核等待所有描述符中的一个就绪可花多长时间<br/>
  struct timeval {<br/>
      long tv_sec;  /* seconds*/<br/>
      long tv_usec; /* microseconds*/<br/>
  }   <br/>
  (1) 永远等待下去：仅在有一个描述符准备好时返回。为此，将timeout设置为   空指针<br/>
  (2) 等待一段固定时间：在有一个描述符准备好I/O时返回，但是不超过由该参数所指向的timeval结构中指定的秒数和微秒数<br/>
  (3) 根本不等待：检查描述符后立即返回，这称为轮询。为此，该参数必须指向一个timeval结构，而且其中的定时器值必须为0</p></li>
<li><p>readfds, writefds, errorfds参数，这三个参数指定了，需要内核测试读、写     和异常条件的描述符集合。
系统提供了以下宏来对描述符进行操作：<br/>
void FD_CLR(fd, fd_set <em>fdset)<br/>
void FD_COPY(fd_set </em>fdest_orig, fd_set <em>fdset_copy)<br/>
int FD_ISSET(fd, fd_set </em>fdset)<br/>
void FD_SET(fd, fd_set <em>fdset)<br/>
void FD_ZERO(fd_set </em>fdset)</p></li>
<li><p>nfds参数，指定待测试的描述符个数，它的值是待测试的最大描述符加1</p></li>
</ul>


<!-- more -->


<h1>描述符就绪条件</h1>

<ul>
<li><p>满足下列四个条件中的任何一个时，一个套接字准备好读<br/>
a) 该套接字接收缓冲区中的数据字节数大于等于套接字接收缓冲区低水位标记的当前大小。对这样的套接字执行读操作不会阻塞并将返回一个大于0的值（也就是返回准备好读入的数据）。对于TCP和UDP套接字，其默认值为1。<br/>
b) 该连接的读半部关闭（也就是接收了FIN的TCP连接）。对这样的套接字的读操作将不阻塞并返回0（也就是返回EOF）<br/>
c) 该套接字是一个监听套接字且已完成的连接数大于0。对于这样的套接字调用accept通常不会阻塞。  <br/>
d) 其上有一个套接字错误待处理。对这样的套接字的读操作将不阻塞并返回－1（也就是返回一个错误），同时把errno设置成确切的错误条件。</p></li>
<li><p>下列四个条件中的任何一个满足时，一个套接字准备好写。<br/>
a) 该套接字发送缓冲区中的可用空间字节数大于等于套接字发送缓冲区低水位表姐的当前大小，并且或者该套接字已连接，或者该套接字不需要连接（如UDP套接字）。  <br/>
b) 该连接的写半部关闭。对这样的套接字的写操作将产生SIGPIPE信号。<br/>
c) 使用非阻塞式connect的套接字已建立连接，或者connect已经以失败告终。<br/>
d) 其上有一个套接字错误待处理。</p></li>
<li>如果一个套接字存在带外数据或者仍处于带外标记，那么它有异常条件待处理。</li>
</ul>


<h1>修改tcp客户端程序</h1>

<ul>
<li><p>批量输入<br/>
a) 用户输入数据，标准输入可读，read返回读入字节数，发送到服务器<br/>
b) 用户输入Ctrl-D时，标准输入可读，read返回字节数为0，此时关闭套接字写端</p></li>
<li><p>套接字增加处理情况：
a) 如果对端TCP发送数据，那么该套接字变为可读，并且read返回一个大于0的值（即读入数据的字节数）。<br/>
b) 如果对端TCP发送一个FIN（对端进程终止），那么该套接字变为可读，并且read返回0（EOF）。<br/>
c) 如果对端TCP发送一个RST（对端主机崩溃并重新启动），那么该套接字变为可读，并且read返回－1，而errno中含有确切的错误码。</p></li>
</ul>


<p>以下是相关修改：<br/>
1、将标准输入、套接字添加到客服描述符集合</p>

<pre><code>fd_set readfds;
FD_ZERO(&amp;readfds);
FD_SET(sock_client, &amp;readfds);
FD_SET(STDIN_FILENO, &amp;readfds);
int maxfd = (STDIN_FILENO &gt; sock_client ? STDIN_FILENO: sock_client) + 1;
select(maxfd, &amp;readfds, NULL, NULL, NULL)
</code></pre>

<p>2、标准输入可读</p>

<pre><code>if (FD_ISSET(STDIN_FILENO , &amp;readfds)) {
    ssize_t rd_size = read(STDIN_FILENO , buf, sizeof(buf));
    if (rd_size &gt; 0) {
        ssize_t sd_size = send(sock_client, buf, rd_size, 0);
        buf[rd_size] = '\0';
        printf("send msg(len:%ld):%s to server \n", sd_size, buf);
    } else if (rd_size == 0) {
    // client finish send msg to server, we close write part of the full-duplex connection
        shutdown(sock_client, SHUT_WR); // send FIN to server
        stdineof = 1;
        continue;
    } else {
    // client finish send msg to server, we close write part of the full-duplex connection
        printf("read fail from console, reason:%s \n", strerror(errno));
        break;
    }
}  
</code></pre>

<p>3、套接字可读</p>

<pre><code>if (FD_ISSET(sock_client, &amp;readfds)) {
    ssize_t rd_size = read(sock_client, buf, sizeof(buf));
    if (rd_size &gt; 0) {
        printf("receive from server:%s", buf);
    } else if (rd_size == 0) { // client receive FIN
            printf("receive 0 from server\n");
            if (stdineof == 1) {
                printf("normal finish\n");
            } else {// server crash and reset, client receive RST
                printf("server terminate prematurely\n");
            }
            break;
        } else {
            printf("read fail.reason: %s\n", strerror(errno));
            break;
        }
    }
}
</code></pre>

<p>下面是使用select进行I/O复用的tcp客户端代码：</p>

<pre><code>int main(int argc, char *argv[])
{
    // judge the legal arguments
    if (2 &gt; argc) {
        printf("please input a hostname \n");
        return -1;
    }

// create a socket
    int sock_client = socket(AF_INET, SOCK_STREAM, 0);
    if (0 &gt; sock_client) {
        perror("create socket error");
        return -1;
    }


    // construct server sockaddr
        struct hostent *p = gethostbyname(argv[1]);
        if (p == NULL) {
            printf("gethostbyname:%s fail.reason:%s \n", argv[1], strerror(errno));
            return -1;
        }
        struct sockaddr_in servaddr;
        bzero(&amp;servaddr, sizeof(servaddr));
        servaddr.sin_family = AF_INET;
        servaddr.sin_addr = *(struct in_addr *)p-&gt;h_addr;
        servaddr.sin_port = htons(SERVER_PORT);

        // connect to server
        if (connect(sock_client, (struct sockaddr *)&amp;servaddr, sizeof(servaddr)) == -1) {
            printf("connect to server:%s port:%d fail.reason:%s \n", inet_ntoa(servaddr.sin_addr), SERVER_PORT,
                strerror(errno));
            return -1;
        }
        printf("connect to server:%s port:%d success\n", inet_ntoa(servaddr.sin_addr), SERVER_PORT );


        int stdineof = 0;
        for (; ; ) {
            fd_set readfds;
            FD_ZERO(&amp;readfds);
            FD_SET(sock_client, &amp;readfds);
            if (stdineof == 0)
                FD_SET(STDIN_FILENO, &amp;readfds);
            int maxfd = (STDIN_FILENO &gt; sock_client ? STDIN_FILENO: sock_client) + 1;
            if (select(maxfd, &amp;readfds, NULL, NULL, NULL) &gt; 0) {
                char buf[LINE_MAX]={0};
                if (FD_ISSET(STDIN_FILENO , &amp;readfds)) {
                    ssize_t rd_size = read(STDIN_FILENO , buf, sizeof(buf));
                    if (rd_size &gt; 0) {
                        ssize_t sd_size = send(sock_client, buf, rd_size, 0);
                        buf[rd_size] = '\0';
                        printf("send msg(len:%ld):%s to server \n", sd_size, buf);
                    } else if (rd_size == 0) {
                        // client finish send msg to server, we close write part of the full-duplex connection
                        shutdown(sock_client, SHUT_WR); // send FIN to server
                        stdineof = 1;
                        continue;
                    } else {
                        // client finish send msg to server, we close write part of the full-duplex connection
                        printf("read fail from console, reason:%s \n", strerror(errno));
                        break;
                    }
                } else if (FD_ISSET(sock_client, &amp;readfds)) {
                    ssize_t rd_size = read(sock_client, buf, sizeof(buf));
                    if (rd_size &gt; 0) {
                        printf("receive from server:%s", buf);
                    } else if (rd_size == 0) { // client receive FIN
                        printf("receive 0 from server\n");
                        if (stdineof == 1) {
                            printf("normal finish\n");
                        } else {// server crash and reset, client receive RST
                            printf("server terminate prematurely\n");
                        }
                        break;
                    } else {
                        printf("read fail.reason: %s\n", strerror(errno));
                        break;
                    }
                }
            }
        }
        close(sock_client);
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[tcp服务端并发之子进程]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/02/17/tcpfu-wu-duan-bing-fa-zhi-zi-jin-cheng/"/>
    <updated>2015-02-17T14:56:56+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/02/17/tcpfu-wu-duan-bing-fa-zhi-zi-jin-cheng</id>
    <content type="html"><![CDATA[<p>上一篇博客中实现的tcp服务端每次只能处理一个客户端连接，如果有多个客户端同时与服务端进行通讯的话，那只能进行等待上一个客户端通讯完成才可以，那么如何修改服务端为并发服务器程序呢？Unix中编写并发服务器程序最简单的方法就是fork一个子进程来服务每个客户。以下为一个典型的并发服务器的轮廓：</p>

<pre><code>pid_t pid;
int listenfd, connfd;
listenfd = Socket(...);
    /* fill in sockaddr_in{} with server's well-known port */
Bind*(listenfd, ...);
Listen(listenfd, LISTENQ);
for ( ; ; ) {
    connfd = Accept(listenfd, ...);  /* probably blocks */
    if ((pid = Fork()) == 0) {
        Close(listenfd);    /* child close listening socket */
        doit(connfd);       /* process the request */
        close(connfd);      /* child terminates */
        exit(0);            /* child terminate */
    }
    Close(connfd);          /* parent closes connected socket */
}
</code></pre>

<p>fork函数</p>

<pre><code>pid_t fork(void);
</code></pre>

<p>fork创建一个新进程。新进程是当前运行进程的一个拷贝。fork函数一次调用，两次返回。对于子进程返回值为0，对于父进程，返回值为子进程id。</p>

<p>以下为根据上面的并发服务器轮廓代码修改的tcp客户端代码：</p>

<!-- more -->


<pre><code>for (; ; ) {

    if ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) &lt; 0) {
        if (errno == EINTR) {
            continue;
        }
        else {
            printf("accept fail. reason:%s \n", strerror(errno));
            close(sock_server);
            exit(0);
        }
    }
    printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);

    int child = fork();
    if (child == 0) { // child process

        close(sock_server);
        printf("child process:%d \n", getpid());
        // recv data from client
        char buf[1024];
        ssize_t rcv_len;
        while ((rcv_len = recv(connsock, buf, sizeof(buf), 0)) &gt; 0) {
            printf("recv from client:%s\n", buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
            send(connsock, buf, rcv_len, 0);
            printf("send to  client:%s\n", buf);
        }
        printf("len:%ld \n", rcv_len);
        close(connsock);
        exit(0);
    }

    // parent process
    close(connsock);
}
</code></pre>

<p>以上tcp服务端程序可以并发处理多个客户端请求，但是在通讯结束，子进程调用exit函数退出后，会产生僵死进程, 使用ps命令可以看到多个server进程状态为Z+：<br/>
<img src="/images/zombie.png">
设置僵死状态的目的是维护子进程的信息，以便父进程在以后某个时候获取。这些信息包括子进程的进程ID、终止状态以及资源利用信息（CPU时间、内存使用量等等）。如果一个进程终止，而该进程有子进程处于僵死状态，那么它的所有僵死子进程的富进程ID将被重置为1（init进程）。继承这些子进程的init进程将清理它们。<br/>
僵死进程占用内核中的空间，最终可能导致我们耗尽进程资源，因此我们需要及时的清理僵死子进程，无论何时我们fork子进程都得wait它们，以防它们变成僵死进程。每个子进程在退出时，内核都会向父进程发送SIGCHLD信号，因此父进程需要建立一个俘获SIGCHLD信号的信号处理函数，在函数体中调用wait。</p>

<p>wait函数</p>

<pre><code>pid_t wait(int *stat_loc);
</code></pre>

<p>wait函数阻塞调用进程直到获取到一个退出子进程的信息，成功返回后，stat_loc返回退出子进程的退出信息。SIGCHILD信号处理函数为：</p>

<pre><code>void chld_sig_handler(int signo)
{
    int stat_loc;
    wait(&amp;stat_loc);
    return ;
}
</code></pre>

<p>sigaction函数</p>

<pre><code>struct  sigaction {
         union __sigaction_u __sigaction_u;  /* signal handler */
         sigset_t sa_mask;               /* signal mask to apply */
         int     sa_flags;               /* see signal options below */
 };

 union __sigaction_u {
         void    (*__sa_handler)(int);
         void    (*__sa_sigaction)(int, struct __siginfo *,
                        void *);
 };

#define sa_handler      __sigaction_u.__sa_handler
#define sa_sigaction    __sigaction_u.__sa_sigaction

int sigaction(int sig, const struct sigaction *restrict act, struct sigaction *restrict oact);
</code></pre>

<p>sigaction函数用来修改进程对于某个信号的处理函数
参数sig为需要修改处理函数的信号id
参数act说明了该信号处理函数，以及进入处理函数时的信号掩码</p>

<pre><code>// register SIGCHLD process handler
struct sigaction act;
struct sigaction oact;
act.sa_handler = chld_sig_handler;
sigemptyset(&amp;act.sa_mask);
act.sa_flags = 0;
if (sigaction(SIGCHLD, &amp;act, &amp;oact) &lt; 0) {
    printf("set SIGCHLD handler fail. reason:%s \n", strerror(errno));
    close(sock_server);
    return -1;
}
</code></pre>

<p>在tcp服务代码中添加了SIGCHLD信号处理函数后，重新测试，但是发现仍然会出现僵死子进程：<br/>
<img src="/images/zombie_wait.png"><br/>
Unix信号处理机制中，<br/>
1、当一个信号达到后，进入该信号的处理函数后，进程将屏蔽该信号 <br/>
2、多个同样类型信号同时到达时，信号处理函数只执行一次，其他的信号不进行排队</p>

<p>这样当多个客户端连接同时结束时，会导致服务端子进程同时退出，多个SIGCHLD信号同时产生递送到服务端父进程，那么一种可能的情况是，在信号处理函数被调用之前，信号都到达，那么只有一个信号被处理，其他信号不会排队，将会被丢弃，这样仍然产生僵死子进程。<br/>
为了解决这样问题，可以在信号处理函数中多次执行wait操作，获取已经结束子进程的退出信息。但是上面的wait接口是阻塞等待、直到有新退出进程，这样的话就会影响进程的正常执行，下面介绍另外一个</p>

<pre><code>pid_t waitpid(pid_t pid, int *stat_loc, int options);
</code></pre>

<p>参数pid指定需要等待的进程集。pid为-1时，等待任何子进程。pid为0时，等待调用者进程组里面的任何子进程。pid大于0时，等待进程号为pid的子进程。
参数stat_loc用来保存进程退出状态。
参数options,为WNOHANG、WUNTRACED选项的或。指定WNOHANG操作时，如果尚没有退出的子进程，你们waitp函数不阻塞等待，会返回0。指定WUNTRACED选项时，如果子进程是由于SIGTTIN,SIGTTOU,SIGTSTP,SIGSTOP信号而处于stopped状态，那么wait操作也会获取到这些进程的状态信息。<br/>
需要修改SIGCHLD信号处理函数，使用waitp函数，设置options位WNOHANG来读取所有已经退出子进程的进程状态，防止变为僵尸进程。</p>

<pre><code>void chld_sig_handler(int signo)
{
    int stat_loc;
    int options = WNOHANG;
    int pid;
    while ((pid = waitpid(-1, &amp;stat_loc, options)) &gt; 0) {
        printf("wait pid:%d suc\n", pid);   // normally we should call i/o func in signal handler
    }
return ;
}
</code></pre>

<p>对于服务器主进程而言，大部分的时间都是阻塞在accept系统调用上，这一类的系统调用也称为慢系统调用（slow system call），适用与慢系统调用的一个规则是：当阻塞与某个慢系统调用的一个进程捕获某个信号且相应信号处理函数返回时，该系统调用可能返回一个EINTR错误。有些内核自动重启某些被中断的系统调用。为了便于移植，当我们编写捕获信号的程序时（多数并发服务器捕获SIGCHLD），我们必须对慢系统调用返回EINTR有所准备。修改服务器代码为：</p>

<pre><code>if ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) &lt; 0) {
    if (errno == EINTR) {
        continue;
    } else {
        printf("accept fail. reason:%s \n", strerror(errno));
        close(sock_server);
        exit(0);
    }
}
</code></pre>

<p>本片文章的目的时总结我们在网络编程时可能会遇到的三种情况：
(1) 当fork子进程时，必须捕获SIGCHLD信号
(2) 当捕获信号时，必须处理被中断的系统调用
(3) SIGCHLD的信号处理函数必须正确编写，应适用waitpid函数以免留下僵尸进程</p>
]]></content>
  </entry>
  
</feed>
