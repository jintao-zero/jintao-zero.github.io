<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 网络 | 金涛的博客]]></title>
  <link href="http://jintao-zero.github.io/blog/categories/网络/atom.xml" rel="self"/>
  <link href="http://jintao-zero.github.io/"/>
  <updated>2015-05-01T20:40:34+08:00</updated>
  <id>http://jintao-zero.github.io/</id>
  <author>
    <name><![CDATA[jintao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tcp Server使用kevent进行io复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong/"/>
    <updated>2015-05-01T08:40:25+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong</id>
    <content type="html"><![CDATA[<h1>概述</h1>

<p>kqueue是FreeBSD系统中引入的可扩展事件通知接口，NetBSD，OpenBSD，DragonflyBSD和OS X这些系统也支持此接口。传统的io复用接口如select和poll存在以下两个缺点：</p>

<ul>
<li>支持的文件描述符数量有限，select接口支持的文件描述个数与进程能够打开的最大文件个数有关</li>
<li>当大量描述符需要复用时，select和poll的效率会降低，内核是采用轮询方式判断描述符是否可用</li>
</ul>


<p>kqueue是针对传统传统的select/poll处理大量的文件描述符性能较低效而开发出来的。注册一批描述符到kqueue以后，当其中的描述符状态发生变化时，kqueue将一次性通知应用程序哪些描述符可读、可写或者发生错误。</p>

<p>kqueue支持多种类型的文件描述符，包括socket、信号、定时器、AIO、VNODE、PIPE。</p>

<h1>接口</h1>

<h2>kqueue</h2>

<pre><code>int kqueue(void);
</code></pre>

<p>kqueue系统调用创建一个新的内核事件队列并返回该队列描述符。调用fork时，子进程不继承该描述符。<br/>
失败时，系统调用返回-1,同时设置errno</p>

<h2>kevent</h2>

<pre><code>int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout);
</code></pre>

<p>使用kevent系统调用向队列注册事件和返回已经触发的事件。</p>

<ul>
<li>kq<br/>
kq为kqueue系统调用返回的队列描述符</li>
<li>changelist<br/>
changelist指向一个kevnet结构体数组，kevent定义在&lt;sys/event.h>头文件中，这个数组描述了需要对kq队列中的事件进行修改，包括添加、删除、修改等操作</li>
<li>nchanges<br/>
nchanges是changelist参数指向数组的大小</li>
<li>evnetlist<br/>
指向一个kevnet结构体数组，用于返回已经触发的事件</li>
<li>nevents<br/>
定义eventlist数组大小</li>
<li><p>timeout
timeout是非NULL指针时，定义系统调用超时间隔。timeout为NULL时，kevent系统调用无限阻塞等待事件发生。timeout非NULL，但是值为0时，kevent立即返回</p>

<p><!-- more --></p></li>
</ul>


<h2>EV_SET</h2>

<pre><code>EV_SET(&amp;kev, ident, filter, flags, fflags, data, udata);
struct kevent {
         uintptr_t       ident;          /* identifier for this event */
         int16_t         filter;         /* filter for event */
         uint16_t        flags;          /* general flags */
         uint32_t        fflags;         /* filter-specific flags */
         intptr_t        data;           /* filter-specific data */
         void            *udata;         /* opaque user data identifier */
 };
</code></pre>

<p>EV_SET宏用来初始化一个kevent结构体。下面详细介绍结构体各个字段</p>

<ul>
<li>ident
ident用来标识一个事件，ident值的准确含义由filter字段进行解释，ident值通常代表一个文件描述符</li>
<li><p>filter
内核filter定义的过滤器处理发生在ident上的事件。预定义的系统过滤器如下，通过kevent结构体中的flags和data字段传递参数</p>

<p>EVFILT_READ <br/>
文件描述符作为标识符，当描述符可读时返回。根据文件描述符类型不同，触发该过滤器的情况不同：</p>

<ul>
<li>sockets:<br/>
处于监听状态的socket套接字，当存在待处理的建立连接请求（已经完成三次握手）时，该套接字可读，kevent结构体的data字段包含监听队列中待处理的请求个数。<br/>
非监听状态的套接字缓冲区中有可读数据时，触发该过滤器，每个套接字对应的缓冲区可读低水位可以通过传递fflags和data参数进行设置。</li>
<li>Vnodes<br/>
当文件指针没有达到文件末尾时，触发该过滤器。</li>
<li>Fifos, Pipes<br/>
当管道可读时，触发该过滤器。data字段返回可读字节数。</li>
</ul>


<p>EVFILT_WRITE  <br/>
文件描述符作为标识符。当文件描述符可写时，触发该过滤器。对于sockets、pipes和fifos，data字段返回缓冲区中可写字节数。  <br/>
EVFILT_AIO<br/>
尚不支持<br/>
EVFILT_VNODE  <br/>
文件描述符作为标识符，fflags字段定义需要监测的事件类型：</p>

<ul>
<li>NOTE_DELETE  <br/>
unlink系统调用操作文件描述符指向的文件时，事件发生。</li>
<li>NOTE_WRITE  <br/>
对文件描述指向的文件进行写操作</li>
<li>NOTE_EXTEND<br/>
对文件描述符指向的文件进行extened</li>
<li>NOTE_ATTRIB<br/>
文件描述符指向的文件属性发生了变化</li>
<li>NOTE_LINK<br/>
文件描述指向的文件链接数发生了变化</li>
<li>NOTE_RENAME<br/>
文件描述符指向的文件发生了重命名</li>
<li>NOTE_REVOKE</li>
</ul>


<p>kevent系统调用返回时，fflags包含触发过滤器的事件</p>

<p>EVFILT_PROC
进程id作为标识符。监测的事件定义在fflags字段中：</p>

<ul>
<li>NOTE_EXIT  <br/>
进程退出</li>
<li>NOTE_EXITSTATUS<br/>
进程已经退出，退出状态</li>
<li>NOTE_FORK<br/>
进程调用fork或者其他类似接口创建子进程</li>
<li>NOTE_EXEC<br/>
进程调用execve或者其他类似接口执行一个新进程</li>
<li>NOTE_SIGNAL<br/>
进程被发送一个信号。<br/>
kevent系统调用返回时，fflags包含触发过滤器的事件。</li>
</ul>


<p>EVFILT_SIGNAL<br/>
将信号值作为标识符，当该信号发送到进程时，触发过滤器。</p>

<p>EVFILT_TIMER
设置一个ident标识的定时器。当添加一个定时器时，data字段指定定时器超时事件，fflags字段可以为以下设置：</p>

<ul>
<li>NOTE_SECONDS <br/>
data字段值单位为秒</li>
<li>NOTE_USECONDS<br/>
data字段值单位为毫米</li>
<li>NOTE_NSECONDS<br/>
data字段值单位为纳秒</li>
<li>NOTE_ABSOLUTE
data字段值为绝对时间</li>
</ul>
</li>
<li><p>flags<br/>
flags表示对该事件做的操作。</p>

<ul>
<li>EV_ADD<br/>
添加该事件到kqueue队列</li>
<li>EV_ENABLE
如果该事件触发，kevent返回该事件</li>
<li>EV_DISABLE
如果该事件触发，kevent不返回该事件</li>
<li>EV_DELETE
从kevent队列中删除该事件</li>
<li>EV_RECEIPT
对kqueue进行批量修改，而不返回任何待处理事件。</li>
<li>EV_ONESHOT
过滤器第一次触发后，返回该事件，然后将该事件从kqueue队列中删除。</li>
<li>EV_CLEAR
用户获取事件后，重置事件状态。</li>
<li>EV_ERROR
系统处理事件出错时，返回该事件event，将flags设置为EV_ERROR</li>
</ul>
</li>
<li><p>fflags<br/>
过滤器相关的标志</p></li>
<li>data<br/>
过滤器相关的数值</li>
<li>udata
用户定义的值，当事件返回时，内核将该值带给用户</li>
</ul>


<p>将<a href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/">tcp server 使用select进行并发</a>示例改为使用kevent进行i/o多路复用，主要涉及以下方面的修改：</p>

<p>1、套接字注册</p>

<pre><code>int Register(int kq, int fd)
{
    struct kevent ke;
    EV_SET(&amp;ke, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
    return kevent(kq, &amp;ke, 1, NULL, 0, NULL);
}
</code></pre>

<p> 目前只是对监听套接字和客户端连接套接字注册可读过滤器</p>

<p>2、循环调用kevent处理套接字事件</p>

<pre><code>struct kevent events[MAX_EVENT_COUNT];
for ( ; ; ) {
    int nevent = kevent(kq, NULL, 0, events, MAX_EVENT_COUNT, NULL);
}

void HandleEvent(int kq, struct kevent * events, int nevents) 
{
    for (int i = 0; i &lt; nevents; i++) {
        int sock = events[i].ident;
        int data = events[i].data;
        if (sock == serv_sock) {
            Accept(kq, data);
        } else {
            Receive(sock, data);
        }
    }
}

void Accept(int kq, int conn_size)
{
    for (int i = 0; i &lt; conn_size; i++) {
      int client = accept(serv_sock, NULL, NULL);
      Register(kq, client);
    }
}

void Receive(int sock, int buf_size)
{
    if (0 == buf_size)
        close(sock); // close a file descriptor removing any            kevents that reference the descriptor

    char *buf = malloc(buf_size);
    if (NULL == buf)
        return;

    recv(sock, buf, buf_size, 0);
    for (int i = 0; i &lt; buf_size; i++)
        buf[i] = toupper(buf[i]);
    send(sock, buf, buf_size, 0);
    free(buf);
}    
</code></pre>

<h1>结束语</h1>

<p>这篇文章只是简单介绍了kevent I/O多路复用模型的简单用法，上述例子中涉及的一些细节，如错误处理等都尚须完善<br/>
参考<a href="http://www.ibm.com/developerworks/cn/aix/library/1105_huangrg_kqueue/index.html#resources">使用 kqueue 在 FreeBSD 上开发高性能应用服务器</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp server使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-12T15:30:03+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi/">tcp服务端示例</a>中，接受一个客户端连接，调用accept函数返回后就对本客户端连接进行业务处理，如果业务逻辑比较复杂，那么服务端有可能就阻塞在某个客户端上，而不能处理其他客户端连接，解决此问题有多种模型可以采用：<br/>
1）多进程，为每个客户端连接创建一个子进程进行处理<br/>
2）多线程，多个线程并行处理客户端连接<br/>
3）I/O多路复用，服务器采用单线程模型，使用I/O多路复用模型，并行处理多个客户端文件描述符<br/>
在上一篇示例<a href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/">tcp client 使用select进行I/O复用</a>中介绍的select函数同样适用与对tcp server端的I/O多路复用改造，改造主要设计几下几个方面：</p>

<!-- more -->


<p>1）将server监听套接字添加到readfds套接字集合中，监听套接字可读时，调用accept接受一个新的客户端连接，并将添加到readfds套接字集合中</p>

<pre><code>if (FD_ISSET(sock_server, &amp;rfds)) {
    struct sockaddr_in clientaddr;
    socklen_t clientaddr_len = sizeof(clientaddr);
    int clientsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len);
    if (clientsock &lt; 0) {
        printf("accept fail. reason:%s\n", strerror(errno));
        continue;
    } else {
        printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);
        int i;
        for (i = 0; i &lt; FD_SETSIZE; i++) {
            if (clientfds[i] == -1) {
                clientfds[i] = clientsock;
                break;
            }
        }
        if (i == FD_SETSIZE) {
            printf("too many clients, ignore this one\n");
            continue;
        }
        FD_SET(clientfds[i], &amp;allfds);
        if (clientfds[i] &gt; maxfd)
            maxfd = clientfds[i];
    }
}  
</code></pre>

<p>2）当每个客户端套接字可读时，对套接字调用read操作后，继续执行select判断是否有套接字可读，而不是处理一个客户端连接直到完成。</p>

<pre><code>for (int i = 0; i &lt; FD_SETSIZE; i++) {
    if (readyfds == 0)
        break;
    if (clientfds[i] &lt; 0)
        continue;
    if (FD_ISSET(clientfds[i], &amp;rfds)) {
        readyfds--;
        char buf[LINE_MAX];
        ssize_t rcv_len;
        rcv_len = recv(clientfds[i], buf, sizeof(buf), 0);
        if (rcv_len &gt; 0) {
            buf[rcv_len] = '\0';
            printf("recv from client(%d), msg: %s\n", clientfds[i], buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
                send(clientfds[i], buf, rcv_len, 0);
                printf("send to client:%s", buf);
            } else if (rcv_len == 0) {
                printf("recv_len == 0 from client:%d\n", clientfds[i]);
                FD_CLR(clientfds[i], &amp;allfds);
                close(clientfds[i]);
                clientfds[i] = -1;
            } else {
                printf("recv from clientfd(%d) fail. reason:%s\n", clientfds[i], strerror(errno));
                continue;
            }
        }
    }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp Client 使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-06T11:49:13+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/10/tcp-ke-hu-duan-shi-li/">tcp客户端示例</a>中，存在以下两种情况时，示例代码不能更好的处理：</p>

<ul>
<li><p>目前客户端输入一条数据并发送，等待服务端返回后，才能继续输入数据
 这样的设计不利于客户端批量处理数据</p></li>
<li><p>当服务端出现异常（如服务进程退出或者服务端主机崩溃等异常情况），客户端也许正在等待输入，不能够及时感知到服务端的异常，只有从send返回错误时，才能感知到服务端故障</p></li>
</ul>


<h1>select 函数介绍</h1>

<p>上面实现的客户端示例中，客户端只能处理一个描述符，我们需要修改客户端使其可以同时处理标准输入和套接字描述符，达到I/O复用：</p>

<pre><code>#include &lt;sys/select.h&gt;
#include &lt;sys/time.h&gt;
int select(int nfds, fd_set *restrict readfds, fd_set *restrict writefds, fd_set *restrict errorfds,
     struct timeval *restrict timeout);
</code></pre>

<ul>
<li><p>timeout参数，它告知内核等待所有描述符中的一个就绪可花多长时间<br/>
  struct timeval {<br/>
      long tv_sec;  /* seconds*/<br/>
      long tv_usec; /* microseconds*/<br/>
  }   <br/>
  (1) 永远等待下去：仅在有一个描述符准备好时返回。为此，将timeout设置为   空指针<br/>
  (2) 等待一段固定时间：在有一个描述符准备好I/O时返回，但是不超过由该参数所指向的timeval结构中指定的秒数和微秒数<br/>
  (3) 根本不等待：检查描述符后立即返回，这称为轮询。为此，该参数必须指向一个timeval结构，而且其中的定时器值必须为0</p></li>
<li><p>readfds, writefds, errorfds参数，这三个参数指定了，需要内核测试读、写     和异常条件的描述符集合。
系统提供了以下宏来对描述符进行操作：<br/>
void FD_CLR(fd, fd_set <em>fdset)<br/>
void FD_COPY(fd_set </em>fdest_orig, fd_set <em>fdset_copy)<br/>
int FD_ISSET(fd, fd_set </em>fdset)<br/>
void FD_SET(fd, fd_set <em>fdset)<br/>
void FD_ZERO(fd_set </em>fdset)</p></li>
<li><p>nfds参数，指定待测试的描述符个数，它的值是待测试的最大描述符加1</p></li>
</ul>


<!-- more -->


<h1>描述符就绪条件</h1>

<ul>
<li><p>满足下列四个条件中的任何一个时，一个套接字准备好读<br/>
a) 该套接字接收缓冲区中的数据字节数大于等于套接字接收缓冲区低水位标记的当前大小。对这样的套接字执行读操作不会阻塞并将返回一个大于0的值（也就是返回准备好读入的数据）。对于TCP和UDP套接字，其默认值为1。<br/>
b) 该连接的读半部关闭（也就是接收了FIN的TCP连接）。对这样的套接字的读操作将不阻塞并返回0（也就是返回EOF）<br/>
c) 该套接字是一个监听套接字且已完成的连接数大于0。对于这样的套接字调用accept通常不会阻塞。  <br/>
d) 其上有一个套接字错误待处理。对这样的套接字的读操作将不阻塞并返回－1（也就是返回一个错误），同时把errno设置成确切的错误条件。</p></li>
<li><p>下列四个条件中的任何一个满足时，一个套接字准备好写。<br/>
a) 该套接字发送缓冲区中的可用空间字节数大于等于套接字发送缓冲区低水位表姐的当前大小，并且或者该套接字已连接，或者该套接字不需要连接（如UDP套接字）。  <br/>
b) 该连接的写半部关闭。对这样的套接字的写操作将产生SIGPIPE信号。<br/>
c) 使用非阻塞式connect的套接字已建立连接，或者connect已经以失败告终。<br/>
d) 其上有一个套接字错误待处理。</p></li>
<li>如果一个套接字存在带外数据或者仍处于带外标记，那么它有异常条件待处理。</li>
</ul>


<h1>修改tcp客户端程序</h1>

<ul>
<li><p>批量输入<br/>
a) 用户输入数据，标准输入可读，read返回读入字节数，发送到服务器<br/>
b) 用户输入Ctrl-D时，标准输入可读，read返回字节数为0，此时关闭套接字写端</p></li>
<li><p>套接字增加处理情况：
a) 如果对端TCP发送数据，那么该套接字变为可读，并且read返回一个大于0的值（即读入数据的字节数）。<br/>
b) 如果对端TCP发送一个FIN（对端进程终止），那么该套接字变为可读，并且read返回0（EOF）。<br/>
c) 如果对端TCP发送一个RST（对端主机崩溃并重新启动），那么该套接字变为可读，并且read返回－1，而errno中含有确切的错误码。</p></li>
</ul>


<p>以下是相关修改：<br/>
1、将标准输入、套接字添加到客服描述符集合</p>

<pre><code>fd_set readfds;
FD_ZERO(&amp;readfds);
FD_SET(sock_client, &amp;readfds);
FD_SET(STDIN_FILENO, &amp;readfds);
int maxfd = (STDIN_FILENO &gt; sock_client ? STDIN_FILENO: sock_client) + 1;
select(maxfd, &amp;readfds, NULL, NULL, NULL)
</code></pre>

<p>2、标准输入可读</p>

<pre><code>if (FD_ISSET(STDIN_FILENO , &amp;readfds)) {
    ssize_t rd_size = read(STDIN_FILENO , buf, sizeof(buf));
    if (rd_size &gt; 0) {
        ssize_t sd_size = send(sock_client, buf, rd_size, 0);
        buf[rd_size] = '\0';
        printf("send msg(len:%ld):%s to server \n", sd_size, buf);
    } else if (rd_size == 0) {
    // client finish send msg to server, we close write part of the full-duplex connection
        shutdown(sock_client, SHUT_WR); // send FIN to server
        stdineof = 1;
        continue;
    } else {
    // client finish send msg to server, we close write part of the full-duplex connection
        printf("read fail from console, reason:%s \n", strerror(errno));
        break;
    }
}  
</code></pre>

<p>3、套接字可读</p>

<pre><code>if (FD_ISSET(sock_client, &amp;readfds)) {
    ssize_t rd_size = read(sock_client, buf, sizeof(buf));
    if (rd_size &gt; 0) {
        printf("receive from server:%s", buf);
    } else if (rd_size == 0) { // client receive FIN
            printf("receive 0 from server\n");
            if (stdineof == 1) {
                printf("normal finish\n");
            } else {// server crash and reset, client receive RST
                printf("server terminate prematurely\n");
            }
            break;
        } else {
            printf("read fail.reason: %s\n", strerror(errno));
            break;
        }
    }
}
</code></pre>

<p>下面是使用select进行I/O复用的tcp客户端代码：</p>

<pre><code>int main(int argc, char *argv[])
{
    // judge the legal arguments
    if (2 &gt; argc) {
        printf("please input a hostname \n");
        return -1;
    }

// create a socket
    int sock_client = socket(AF_INET, SOCK_STREAM, 0);
    if (0 &gt; sock_client) {
        perror("create socket error");
        return -1;
    }


    // construct server sockaddr
        struct hostent *p = gethostbyname(argv[1]);
        if (p == NULL) {
            printf("gethostbyname:%s fail.reason:%s \n", argv[1], strerror(errno));
            return -1;
        }
        struct sockaddr_in servaddr;
        bzero(&amp;servaddr, sizeof(servaddr));
        servaddr.sin_family = AF_INET;
        servaddr.sin_addr = *(struct in_addr *)p-&gt;h_addr;
        servaddr.sin_port = htons(SERVER_PORT);

        // connect to server
        if (connect(sock_client, (struct sockaddr *)&amp;servaddr, sizeof(servaddr)) == -1) {
            printf("connect to server:%s port:%d fail.reason:%s \n", inet_ntoa(servaddr.sin_addr), SERVER_PORT,
                strerror(errno));
            return -1;
        }
        printf("connect to server:%s port:%d success\n", inet_ntoa(servaddr.sin_addr), SERVER_PORT );


        int stdineof = 0;
        for (; ; ) {
            fd_set readfds;
            FD_ZERO(&amp;readfds);
            FD_SET(sock_client, &amp;readfds);
            if (stdineof == 0)
                FD_SET(STDIN_FILENO, &amp;readfds);
            int maxfd = (STDIN_FILENO &gt; sock_client ? STDIN_FILENO: sock_client) + 1;
            if (select(maxfd, &amp;readfds, NULL, NULL, NULL) &gt; 0) {
                char buf[LINE_MAX]={0};
                if (FD_ISSET(STDIN_FILENO , &amp;readfds)) {
                    ssize_t rd_size = read(STDIN_FILENO , buf, sizeof(buf));
                    if (rd_size &gt; 0) {
                        ssize_t sd_size = send(sock_client, buf, rd_size, 0);
                        buf[rd_size] = '\0';
                        printf("send msg(len:%ld):%s to server \n", sd_size, buf);
                    } else if (rd_size == 0) {
                        // client finish send msg to server, we close write part of the full-duplex connection
                        shutdown(sock_client, SHUT_WR); // send FIN to server
                        stdineof = 1;
                        continue;
                    } else {
                        // client finish send msg to server, we close write part of the full-duplex connection
                        printf("read fail from console, reason:%s \n", strerror(errno));
                        break;
                    }
                } else if (FD_ISSET(sock_client, &amp;readfds)) {
                    ssize_t rd_size = read(sock_client, buf, sizeof(buf));
                    if (rd_size &gt; 0) {
                        printf("receive from server:%s", buf);
                    } else if (rd_size == 0) { // client receive FIN
                        printf("receive 0 from server\n");
                        if (stdineof == 1) {
                            printf("normal finish\n");
                        } else {// server crash and reset, client receive RST
                            printf("server terminate prematurely\n");
                        }
                        break;
                    } else {
                        printf("read fail.reason: %s\n", strerror(errno));
                        break;
                    }
                }
            }
        }
        close(sock_client);
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp服务端并发之子进程]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/02/17/tcpfu-wu-duan-bing-fa-zhi-zi-jin-cheng/"/>
    <updated>2015-02-17T14:56:56+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/02/17/tcpfu-wu-duan-bing-fa-zhi-zi-jin-cheng</id>
    <content type="html"><![CDATA[<p>上一篇博客中实现的tcp服务端每次只能处理一个客户端连接，如果有多个客户端同时与服务端进行通讯的话，那只能进行等待上一个客户端通讯完成才可以，那么如何修改服务端为并发服务器程序呢？Unix中编写并发服务器程序最简单的方法就是fork一个子进程来服务每个客户。以下为一个典型的并发服务器的轮廓：</p>

<pre><code>pid_t pid;
int listenfd, connfd;
listenfd = Socket(...);
    /* fill in sockaddr_in{} with server's well-known port */
Bind*(listenfd, ...);
Listen(listenfd, LISTENQ);
for ( ; ; ) {
    connfd = Accept(listenfd, ...);  /* probably blocks */
    if ((pid = Fork()) == 0) {
        Close(listenfd);    /* child close listening socket */
        doit(connfd);       /* process the request */
        close(connfd);      /* child terminates */
        exit(0);            /* child terminate */
    }
    Close(connfd);          /* parent closes connected socket */
}
</code></pre>

<p>fork函数</p>

<pre><code>pid_t fork(void);
</code></pre>

<p>fork创建一个新进程。新进程是当前运行进程的一个拷贝。fork函数一次调用，两次返回。对于子进程返回值为0，对于父进程，返回值为子进程id。</p>

<p>以下为根据上面的并发服务器轮廓代码修改的tcp客户端代码：</p>

<!-- more -->


<pre><code>for (; ; ) {

    if ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) &lt; 0) {
        if (errno == EINTR) {
            continue;
        }
        else {
            printf("accept fail. reason:%s \n", strerror(errno));
            close(sock_server);
            exit(0);
        }
    }
    printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);

    int child = fork();
    if (child == 0) { // child process

        close(sock_server);
        printf("child process:%d \n", getpid());
        // recv data from client
        char buf[1024];
        ssize_t rcv_len;
        while ((rcv_len = recv(connsock, buf, sizeof(buf), 0)) &gt; 0) {
            printf("recv from client:%s\n", buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
            send(connsock, buf, rcv_len, 0);
            printf("send to  client:%s\n", buf);
        }
        printf("len:%ld \n", rcv_len);
        close(connsock);
        exit(0);
    }

    // parent process
    close(connsock);
}
</code></pre>

<p>以上tcp服务端程序可以并发处理多个客户端请求，但是在通讯结束，子进程调用exit函数退出后，会产生僵死进程, 使用ps命令可以看到多个server进程状态为Z+：<br/>
<img src="/images/zombie.png">
设置僵死状态的目的是维护子进程的信息，以便父进程在以后某个时候获取。这些信息包括子进程的进程ID、终止状态以及资源利用信息（CPU时间、内存使用量等等）。如果一个进程终止，而该进程有子进程处于僵死状态，那么它的所有僵死子进程的富进程ID将被重置为1（init进程）。继承这些子进程的init进程将清理它们。<br/>
僵死进程占用内核中的空间，最终可能导致我们耗尽进程资源，因此我们需要及时的清理僵死子进程，无论何时我们fork子进程都得wait它们，以防它们变成僵死进程。每个子进程在退出时，内核都会向父进程发送SIGCHLD信号，因此父进程需要建立一个俘获SIGCHLD信号的信号处理函数，在函数体中调用wait。</p>

<p>wait函数</p>

<pre><code>pid_t wait(int *stat_loc);
</code></pre>

<p>wait函数阻塞调用进程直到获取到一个退出子进程的信息，成功返回后，stat_loc返回退出子进程的退出信息。SIGCHILD信号处理函数为：</p>

<pre><code>void chld_sig_handler(int signo)
{
    int stat_loc;
    wait(&amp;stat_loc);
    return ;
}
</code></pre>

<p>sigaction函数</p>

<pre><code>struct  sigaction {
         union __sigaction_u __sigaction_u;  /* signal handler */
         sigset_t sa_mask;               /* signal mask to apply */
         int     sa_flags;               /* see signal options below */
 };

 union __sigaction_u {
         void    (*__sa_handler)(int);
         void    (*__sa_sigaction)(int, struct __siginfo *,
                        void *);
 };

#define sa_handler      __sigaction_u.__sa_handler
#define sa_sigaction    __sigaction_u.__sa_sigaction

int sigaction(int sig, const struct sigaction *restrict act, struct sigaction *restrict oact);
</code></pre>

<p>sigaction函数用来修改进程对于某个信号的处理函数
参数sig为需要修改处理函数的信号id
参数act说明了该信号处理函数，以及进入处理函数时的信号掩码</p>

<pre><code>// register SIGCHLD process handler
struct sigaction act;
struct sigaction oact;
act.sa_handler = chld_sig_handler;
sigemptyset(&amp;act.sa_mask);
act.sa_flags = 0;
if (sigaction(SIGCHLD, &amp;act, &amp;oact) &lt; 0) {
    printf("set SIGCHLD handler fail. reason:%s \n", strerror(errno));
    close(sock_server);
    return -1;
}
</code></pre>

<p>在tcp服务代码中添加了SIGCHLD信号处理函数后，重新测试，但是发现仍然会出现僵死子进程：<br/>
<img src="/images/zombie_wait.png"><br/>
Unix信号处理机制中，<br/>
1、当一个信号达到后，进入该信号的处理函数后，进程将屏蔽该信号 <br/>
2、多个同样类型信号同时到达时，信号处理函数只执行一次，其他的信号不进行排队</p>

<p>这样当多个客户端连接同时结束时，会导致服务端子进程同时退出，多个SIGCHLD信号同时产生递送到服务端父进程，那么一种可能的情况是，在信号处理函数被调用之前，信号都到达，那么只有一个信号被处理，其他信号不会排队，将会被丢弃，这样仍然产生僵死子进程。<br/>
为了解决这样问题，可以在信号处理函数中多次执行wait操作，获取已经结束子进程的退出信息。但是上面的wait接口是阻塞等待、直到有新退出进程，这样的话就会影响进程的正常执行，下面介绍另外一个</p>

<pre><code>pid_t waitpid(pid_t pid, int *stat_loc, int options);
</code></pre>

<p>参数pid指定需要等待的进程集。pid为-1时，等待任何子进程。pid为0时，等待调用者进程组里面的任何子进程。pid大于0时，等待进程号为pid的子进程。
参数stat_loc用来保存进程退出状态。
参数options,为WNOHANG、WUNTRACED选项的或。指定WNOHANG操作时，如果尚没有退出的子进程，你们waitp函数不阻塞等待，会返回0。指定WUNTRACED选项时，如果子进程是由于SIGTTIN,SIGTTOU,SIGTSTP,SIGSTOP信号而处于stopped状态，那么wait操作也会获取到这些进程的状态信息。<br/>
需要修改SIGCHLD信号处理函数，使用waitp函数，设置options位WNOHANG来读取所有已经退出子进程的进程状态，防止变为僵尸进程。</p>

<pre><code>void chld_sig_handler(int signo)
{
    int stat_loc;
    int options = WNOHANG;
    int pid;
    while ((pid = waitpid(-1, &amp;stat_loc, options)) &gt; 0) {
        printf("wait pid:%d suc\n", pid);   // normally we should call i/o func in signal handler
    }
return ;
}
</code></pre>

<p>对于服务器主进程而言，大部分的时间都是阻塞在accept系统调用上，这一类的系统调用也称为慢系统调用（slow system call），适用与慢系统调用的一个规则是：当阻塞与某个慢系统调用的一个进程捕获某个信号且相应信号处理函数返回时，该系统调用可能返回一个EINTR错误。有些内核自动重启某些被中断的系统调用。为了便于移植，当我们编写捕获信号的程序时（多数并发服务器捕获SIGCHLD），我们必须对慢系统调用返回EINTR有所准备。修改服务器代码为：</p>

<pre><code>if ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) &lt; 0) {
    if (errno == EINTR) {
        continue;
    } else {
        printf("accept fail. reason:%s \n", strerror(errno));
        close(sock_server);
        exit(0);
    }
}
</code></pre>

<p>本片文章的目的时总结我们在网络编程时可能会遇到的三种情况：
(1) 当fork子进程时，必须捕获SIGCHLD信号
(2) 当捕获信号时，必须处理被中断的系统调用
(3) SIGCHLD的信号处理函数必须正确编写，应适用waitpid函数以免留下僵尸进程</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp 服务端例子]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi/"/>
    <updated>2015-02-13T11:04:43+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi</id>
    <content type="html"><![CDATA[<p>实现了一个简单服务端，使用tcp与客户端进行通讯。本片文章主要介绍在tcp服务端网络编程中用到的几个基本函数：<br/>
1、创建套接字socket函数</p>

<pre><code>int socket(int domain, int type, int protocol)
</code></pre>

<p>socket函数创建一个通讯端并返回这个终端的文件描述符。</p>

<p>参数domain指定发生整个通讯过程所在的协议域。协议域定义在&lt;sys/socket.h>头文件中，常用协议域名为：</p>

<pre><code>PF_LOCAL 主机内部通讯协议，之前名为PF_UNIX
PF_UNIX  主机内部通讯协议，废弃，使用PF_LOCAL
PF_INET  IPV4通讯协议
PF_INET6 IPV6通讯协议
。。。
</code></pre>

<p>本例子中使用PF_INET ipv4通讯协议域</p>

<p>参数type指定socket类型，目前定义类型为：</p>

<pre><code>SOCK_STREAM                                      
SOCK_DGRAM
SOCK_RAW
SOCK_SEQPACKET
SOCK_RDM
</code></pre>

<p>SOCK_STREAM 类型提供序列化、可靠的、全双工字节流套接字
SOCK_DGRAM 类型提供无连接的、不可靠的，有报文最大限制的数据报通讯套接字
SOCK_RAW 类型提供读取内部网络协议和网卡接口的原始套接字，需要有超级用户权限<br/>
本例子中使用SOCK_STREAM流套接字类型</p>

<p>失败时，函数返回-1，成功时，返回套接字文件描述符</p>

<p>2、bind函数</p>

<pre><code>int bind(int socket, const struct sockaddr *address, socklen_t address_len);
</code></pre>

<p>bind函数把一个本地协议地址赋予一个套接字。
参数socket，为需要赋予地址的套接字。
参数address，为赋予到套接字的地址结构，有ip和端口号标识。
参数address_len, 为该地址结构大小</p>

<p>对于服务端来讲，在调用监听函数之前，都需要bind到特定地址和端口</p>

<!-- more -->


<p>3、listen函数</p>

<pre><code>int listen(int socket, int backlog);
</code></pre>

<p>listen函数仅由TCP服务器调用，它做两件事情：<br/>
(1) 当socket函数创建一个套接字时，它被假设为一个主动套接字，也就是说，它是一个将调用connect发起连接的客户套接字。   listen函数把一个未连接的套接字转换成一个被动套接字，指示内核应接受指向该套接字的连接请求。根据tcp状态转换图，调用listen导致套接字从CLOSED状态转换到LISTEN状态。<br/>
(2) 本函数的第二个参数规定了内核应该为相应套接字排队的最大连接个数。
本函数通常应该在调用socket和bind这两个函数之后，并在调用accept函数之前调用。
内核为任何一个给定的监听套接字维护两个队列：<br/>
(1)未完成连接队列，每个这样的SYN分节对应其中一项：已由某个客户发出并到达服务器，而服务器正在等待完成相应的TCP三路握手过程。这些套接字处于SYN_RCVD状态<br/>
(2)已完成连接队列，每个已完成TCP三路握手的客户对应其中一项。这些套接字处于ESTABLISHED状态。
参数backlog规定了以上两个队列的总和大小</p>

<p>4、accept函数</p>

<pre><code>int accept(int socket, struct sockaddr *restrict address, socklen_t *restrict address_len);
</code></pre>

<p>accept函数由TCP服务器调用，用于从已完成连接队列对头返回下一个已完成连接。如果已完成连接队列未空，那么进程被投入睡眠（假定套接字为默认的阻塞方式）。
参数address和address_len返回已连接的对端进程的协议地址。
如果accept成功，那么其返回值是由内核自动生产的一个全新描述符，代表与所返回客户的TCP连接。</p>

<p>5、下面是服务端源代码
此服务端小程序主要完成功能：1、监听服务端口，接收连接 2、接收客户端数据并处理后发回服务端</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;sys/errno.h&gt;
#include &lt;string.h&gt;
#include &lt;netinet/in.h&gt;
#include &lt;arpa/inet.h&gt;
#include &lt;ctype.h&gt;

#define SERVER_PORT 8000

int main()
{
    // create socket
    int sock_server = socket(AF_INET, SOCK_STREAM, 0);
    if (-1 == sock_server) {
        printf("create socket fail.reason:%s\n",                strerror(errno));
        return -1;
    }

    // bind socket to local addr and port
    struct sockaddr_in servaddr;
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr  = INADDR_ANY;
    servaddr.sin_port = htons(SERVER_PORT);
    if (-1 == bind(sock_server, (struct sockaddr *)&amp;servaddr, (socklen_t)sizeof(servaddr))) {
        printf("bind to local addr fail.%s \n", strerror(errno));
        return -1;
    }
    printf("bind to local addr success\n");

    // make socket to listen
    if (-1 == listen(sock_server, 128)) {
        printf("listen fail.%s\n", strerror(errno));
        return -1;
    }

    struct sockaddr_in clientaddr;
    socklen_t clientaddr_len = sizeof(clientaddr);
    int connsock;

    // accept a new client connection
    while ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) != -1) {

        printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr),            clientaddr.sin_port);

        // recv data from client
        char buf[1024];
        ssize_t rcv_len;
        while ((rcv_len = recv(connsock, buf, sizeof(buf), 0)) &gt; 0) {
            printf("recv from client:%s\n", buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
            send(connsock, buf, rcv_len, 0);
            printf("send to  client:%s\n", buf);
        }
        printf("len:%ld \n", rcv_len);
        close(connsock);
    }
    close(sock_server);
}   
</code></pre>

<p>运行效果：
<img src="/images/server.png"></p>
]]></content>
  </entry>
  
</feed>
