<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Bigdata | jintao's blog]]></title>
  <link href="http://jintao-zero.github.io/blog/categories/bigdata/atom.xml" rel="self"/>
  <link href="http://jintao-zero.github.io/"/>
  <updated>2016-09-17T21:56:56+08:00</updated>
  <id>http://jintao-zero.github.io/</id>
  <author>
    <name><![CDATA[jintao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MapReduce WordCount实例]]></title>
    <link href="http://jintao-zero.github.io/blog/2016/03/06/mapreduce-wordcountshi-li/"/>
    <updated>2016-03-06T15:08:34+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2016/03/06/mapreduce-wordcountshi-li</id>
    <content type="html"><![CDATA[<p>搭建开发环境，开发mapreduce job程序，部署mapreduce程序。</p>

<h1>开发环境搭建</h1>

<p>开发mapreduce程序的环境有可能与运行环境不同，我是Windows环境上开发mapreduce job程序，编译没有错误后，打包放到hadoop集群中调试和运行。这样就要求开发环境依赖的jar包，在hadoop集群环境中存在并且版本一致。<br/>
我的开发环境是Windows＋eclipse＋maven，hadoop集群环境上部署的版本号为：</p>

<pre><code>jintao@node0:~/Project$ hadoop  version
Hadoop 2.7.1
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a
Compiled by jenkins on 2015-06-29T06:04Z
Compiled with protoc 2.5.0
From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
This command was run using /usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar
</code></pre>

<p>maven配置文件pom.xml的配置如下：</p>

<pre><code>&lt;dependencies&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
    &lt;version&gt;2.7.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>maven工程会自动将依赖的jar下载到build path中。<br/>
你也可以建立一个普通java工程，手动将依赖的jar添加到build path中，这样需要找依赖的jar，比较麻烦，但是可以让新手更快的了解mapreduce相关类分布情况。</p>

<!-- more -->


<h1>开发mapreduce job程序</h1>

<p>一般情况下，一个job包含一个map类和一个reduce类，但是也可以只有一个类，或者都不包含。本文例子中的WordCount是mapreduce程序开发的helloworld程序，包含map和reduce类，通过WordCount我们可以了解mapreduce程序开发的基本框架。</p>

<h2>Mapper</h2>

<p>map程序的基类为：</p>

<pre><code>@InterfaceAudience.Public
@InterfaceStability.Stable
public class Mapper&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; extends Object
</code></pre>

<p>Maps将输入的key/value对处理并输出一个key/value形式的中间结果集。</p>

<p>Maps是一个个独立的任务线程分别处理不同的记录集合，输出中间结果集。输出结果集的类型不一定与初始的输入记录相同。一个输入key/value对，有可能输出零个或多个key/value结果对。</p>

<p>Hadoop Map-Reduce框架对于job的<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputFormat.html">InputFormat</a>输入数据产生的每个<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputSplit.html">InputSplit</a>派生一个map任务。Mapper的具体实现可以通过<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/JobContext.html#getConfiguration(">JobContext.getConfiguration()</a>)获取job的<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html">Configuration配置</a>。</p>

<p>框架首先调用Mapper实现类的<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Mapper.html#setup(org.apache.hadoop.mapreduce.Mapper.Context">setup(org.apache.hadoop.mapreduce.Mapper.Context)</a>)，然后对于对于获取的每一个输入记录，调用map方法进行处理。最后调用<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Mapper.html#cleanup(org.apache.hadoop.mapreduce.Mapper.Context">cleanup(org.apache.hadoop.mapreduce.Mapper.Context)</a>)。</p>

<p>框架将与某个key相关的所有value值形成一组，传给Reducer来产生最后结果。通过指定key比较类，用户可以控制key/value的排序和分组。</p>

<p>Mapper的输出结果对于每个Reducer进行分片。通过实现一个特定的<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Partitioner.html">Partitioner</a>，用户可以控制keys分片到Reducer。</p>

<p>视情况，通过<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Job.html#setCombinerClass(java.lang.Class">Job.setCombinerClass(Class)</a>)用户可以指定一个combiner，combiner可以在Mapper本地对输出结果进行聚合，这样可以减少从Mapper传输到Reducer的数据量。</p>

<p>通过对Configuration程序可以指定是否并且如何对Mapper结果进行压缩。</p>

<p>如果reducer个数为0，那么不会对Mapper的结果不对结果进行排序，并且直接输出到<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/OutputFormat.html">OutputFormat</a>。<br/>
WordCount对应Mapper实现类如下：</p>

<pre><code>public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
    private static Logger logger = Logger.getLogger(WordCountMapper.class);

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{
        StringTokenizer iTokenizer = new StringTokenizer(value.toString());
        while (iTokenizer.hasMoreTokens()) {
            word.set(iTokenizer.nextToken());
            context.write(word, one);
        }
    }   
}
</code></pre>

<p>应用也可以重写<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Mapper.html#run(org.apache.hadoop.mapreduce.Mapper.Context"> run(org.apache.hadoop.mapreduce.Mapper.Context)</a>)来更多的控制map处理，比如multi-threaded Mapper。</p>

<h2>Reducer</h2>

<p>Reducer的基类定义如下：</p>

<pre><code>@Checkpointable
@InterfaceAudience.Public
@InterfaceStability.Stable
public class Reducer&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; extends Object
</code></pre>

<p>Reducer将Mapper输出的中间结果集处理输出最终结果集。<br/>
Reducer实现类可以通过<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/JobContext.html#getConfiguration("> JobContext.getConfiguration()</a>)获取job的配置信息<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html">Configuration</a>。<br/>
Reducer主要有三个步骤：</p>

<ol>
<li><p>Shuffle<br/>
 Reducer使用Http从个Mapper拷贝传输中间结果集到本地。</p></li>
<li><p>Sort<br/>
 框架将从不同Mapper传输过来的结果集按照key进行合并排序。</p></li>
<li>Reduce
 在这一阶段，对于每个&lt;key, (collection of values)>调用<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Reducer.html#reduce(KEYIN,%20java.lang.Iterable,%20org.apache.hadoop.mapreduce.Reducer.Context">reduce(Object, Iterable, org.apache.hadoop.mapreduce.Reducer.Context)</a>)方法进行处理。<br/>
 reduce任务调用<a href="TaskInputOutputContext.write(Object,%20Object">TaskInputOutputContext.write(Object, Object)</a>)将最终结果写到<a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/RecordWriter.html">RecordWriter</a>。<br/>
 Reducer的输出不是有序的。</li>
</ol>


<p>WordCount的Reduce实现类为：</p>

<pre><code>public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
            Context context) throws IOException, InterruptedException {
        int total = 0;
        for (IntWritable count : values) {
            total += count.get();
        }
        result.set(total);
        context.write(key, result);
    }
}
</code></pre>

<h2>Driver</h2>

<p>已经完成mapper和reducer的编写。接下来写一个job driver用来提交程序到hadoop中运行。<br/>
可以有两种方式编写job driver：</p>

<ol>
<li>实现一个普通Java类，在类中设置job属性信息，提交job</li>
<li>实现<a href="http://hadoop.apache.org/docs/current/api/index.html">Tool</a>接口</li>
</ol>


<p>实际应用中，我们采用实现Tool接口的方式，这种方式能够更好的控制程序运行。<br/>
WordCount Job的属性设置如下：</p>

<pre><code>public class WordCountDriver extends Configured implements Tool {

    public int run(String[] arg0) throws Exception {
        // TODO Auto-generated method stub
        if (arg0.length != 2) {
            System.err.printf("Usage: %s [generic options] &lt;input&gt; &lt;output&gt;\n", 
                    getClass().getSimpleName());
            ToolRunner.printGenericCommandUsage(System.err);
            return -1;
        }

        //Create new job
        Job job = Job.getInstance();
        job.setJarByClass(getClass());

        job.setJobName("WordCount");

        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        job.setNumReduceTasks(1);

        FileInputFormat.addInputPath(job, new Path(arg0[0]));
        FileOutputFormat.setOutputPath(job, new Path(arg0[1]));

        return job.waitForCompletion(true) ? 0 : 1;
    }   

    public static void main(String[] args) {
        // TODO Auto-generated method stub
        try {
            int exitCode = ToolRunner.run(new WordCountDriver(), args);
            System.exit(exitCode);
        } catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }

}   
</code></pre>

<p>WordCountDriver实现了Tool接口，这样我们运行程序时可以使用GenericOptionParser支持的参数项。<br/>
调用Job.getInstance()获取一个Job实例。设置Job名称。设置输入路径和输出路径。设置mapper和reducer类名。设置输出key和value类型（输入类型由数据源格式确定，数据源缺省为TextInputFormat文本格式，默认key类型为LongWritable，value类型为Text）。</p>

<h2>本地运行WordCount</h2>

<p>将程序打成jar包，放到hadoop集群一台机器上去。在jar包文件所在目录，新建一个input目录，input目录中放置一个文本文件words.txt，运行以下命令：</p>

<pre><code>$ hadoop jar hadoopjob.jar com.hugedata.jobs.WordCountDriver -fs file:/// -jt local input/ output/
</code></pre>

<p>-fs file:///指定使用本地文件系统，-jt local 指定mapreduce任务运行在本地。<br/>
当前目录会产生一个output目录，里面有两个文件（part-r-00000、_SUCCESS），其中part-r-00000即为</p>

<pre><code>jintao@node0:~/Project$ cat output/part-r-00000
is  2
jintao  1
jintao. 1
mx  1
my  2
mz  1
name    2
name.   1
too.    1
whit's  1
your    1
</code></pre>

<h2>集群运行WordCount</h2>

<p>前提是集群环境已经运行正常。将本地目录下的input拷贝到hdfs上：</p>

<pre><code>jintao@node0:~/Project$ hdfs dfs -put input
jintao@node0:~/Project$ hdfs dfs -ls
Found 2 items
drwxr-xr-x   - jintao supergroup          0 2016-03-07 16:51 input
drwxr-xr-x   - jintao supergroup          0 2016-03-05 14:04 tmp
</code></pre>

<p>屏幕上打印的过程信息如下：</p>

<pre><code>16/03/07 16:55:18 INFO client.RMProxy: Connecting to ResourceManager at node0/192.168.88.120:8032
16/03/07 16:55:20 INFO input.FileInputFormat: Total input paths to process : 1
16/03/07 16:55:20 INFO mapreduce.JobSubmitter: number of splits:1
16/03/07 16:55:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1457076326520_0001
16/03/07 16:55:21 INFO impl.YarnClientImpl: Submitted application application_1457076326520_0001
16/03/07 16:55:21 INFO mapreduce.Job: The url to track the job: http://node0:8088/proxy/application_1457076326520_0001/
16/03/07 16:55:21 INFO mapreduce.Job: Running job: job_1457076326520_0001
16/03/07 16:55:32 INFO mapreduce.Job: Job job_1457076326520_0001 running in uber mode : false
16/03/07 16:55:32 INFO mapreduce.Job:  map 0% reduce 0%
16/03/07 16:55:43 INFO mapreduce.Job:  map 100% reduce 0%
16/03/07 16:55:51 INFO mapreduce.Job:  map 100% reduce 100%
16/03/07 16:55:52 INFO mapreduce.Job: Job job_1457076326520_0001 completed successfully
16/03/07 16:55:52 INFO mapreduce.Job: Counters: 49
File System Counters
    FILE: Number of bytes read=156
    FILE: Number of bytes written=230351
    FILE: Number of read operations=0
    FILE: Number of large read operations=0
    FILE: Number of write operations=0
    HDFS: Number of bytes read=177
    HDFS: Number of bytes written=77
    HDFS: Number of read operations=6
    HDFS: Number of large read operations=0
    HDFS: Number of write operations=2
Job Counters
    Launched map tasks=1
    Launched reduce tasks=1
    Data-local map tasks=1
    Total time spent by all maps in occupied slots (ms)=8022
    Total time spent by all reduces in occupied slots (ms)=4886
    Total time spent by all map tasks (ms)=8022
    Total time spent by all reduce tasks (ms)=4886
    Total vcore-seconds taken by all map tasks=8022
    Total vcore-seconds taken by all reduce tasks=4886
    Total megabyte-seconds taken by all map tasks=8214528
    Total megabyte-seconds taken by all reduce tasks=5003264
Map-Reduce Framework
    Map input records=3
    Map output records=14
    Map output bytes=122
    Map output materialized bytes=156
    Input split bytes=110
    Combine input records=0
    Combine output records=0
    Reduce input groups=11
    Reduce shuffle bytes=156
    Reduce input records=14
    Reduce output records=11
    Spilled Records=28
    Shuffled Maps =1
    Failed Shuffles=0
    Merged Map outputs=1
    GC time elapsed (ms)=243
    CPU time spent (ms)=2860
    Physical memory (bytes) snapshot=444420096
    Virtual memory (bytes) snapshot=3891036160
    Total committed heap usage (bytes)=348651520
Shuffle Errors
    BAD_ID=0
    CONNECTION=0
    IO_ERROR=0
    WRONG_LENGTH=0
    WRONG_MAP=0
    WRONG_REDUCE=0
File Input Format Counters
    Bytes Read=67
File Output Format Counters
    Bytes Written=77
jintao@node0:~/Project$ 
</code></pre>

<p>程序执行成功后，会产生一个output目录，执行一下命令，可以产看结果：</p>

<pre><code>jintao@node0:~/Project$ hdfs dfs -cat output/part-r-00000
is  2
jintao  1
jintao. 1
mx  1
my  2
mz  1
name    2
name.   1
too.    1
whit's  1
your    1
</code></pre>

<p>至此，讲述了最基本的mapreduce程序开发、运行过程，后续需要学习的有如果查看日志，如何调试优化程序等方面。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hdfs文件系统java Api介绍]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/12/21/hdfswen-jian-xi-tong-java-apijie-shao/"/>
    <updated>2015-12-21T17:29:19+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/12/21/hdfswen-jian-xi-tong-java-apijie-shao</id>
    <content type="html"><![CDATA[<h1>Hadoop Filesystems</h1>

<p>Hadoop对于文件系统有一个抽象设计，HDFS只是一个实现。客户端使用Java 抽象类org.apache.hadoop.fs.FileSystem来访问Hadoop中的文件系统，有多个具体实现类:</p>

<pre><code>FileSystem      URI scheme      Java implementaion  
Local           file            fs.LocalFileSystem  

HDFS            hdfs            hdfs.DistributedFileSystem

WebHDFS         webhdfs         hdfs.web.WebHdfsFileSystem

Secure
WebHDFs         swebhdfs        hdfs.web.SWebHdfsFileSystem

HAR             har             fs.HarFileSystem

View            viewfs          viewfs.ViewFileSystem

FTP             ftp             fs.ftp.FTPFileSystem

S3              s3a             fs.s3a.S3AFileSystem

Azure           wasb            fs.azure.NativeAzureFileSystem

Swift           swift           fs.swift.snative.SwiftNative
</code></pre>

<p>Hadoop提供多个接口访问文件系统，接口利用URI的scheme来创建对应的实例与文件系统进行通讯。hdfs命令可以用来操作Hadoop各种文件系统，比如列出本地文件系统根目录内容：</p>

<pre><code>[root@yun1 ~]# hdfs dfs -ls file:///
Found 25 items
-rw-r--r--   1 root root          0 2015-10-19 11:06 file:///.autofsck
-rw-r--r--   1 root root          0 2014-07-04 14:16 file:///.autorelabel
dr-xr-xr-x   - root root       4096 2014-07-04 14:36 file:///bin
dr-xr-xr-x   - root root       1024 2014-09-17 15:54 file:///boot
drwxr-xr-x   - root root       4096 2014-07-04 09:17 file:///data
-rw-r--r--   1 root root       1001 2014-07-29 16:28 file:///dataplatform.log
drwxr-xr-x   - root root       3580 2015-10-19 11:08 file:///dev
</code></pre>

<h1>Java Interface</h1>

<!-- more -->


<p>访问HDFS文件系统需要先获取一个文件系统实例，FileSystem是一个文件系统统一访问接口。通过以下接口可以获取访问hdfs文件系统的实例：</p>

<pre><code>public static FileSystem get(Configuration conf) throws IOException
public static FileSystem get(URI uri, Configuration conf) throws IOException
public static FileSystem get(URI uri, Configuration conf, String user) throws IOException
</code></pre>

<p>Configuration对象封装客户端或者服务端的配置信息，这些信息由classpath目录下配置文件初始化，比如etc/hadoop/core-site.xml。第一个接口根据conf中的信息，返回文件系统。第二个接口使用URI的scheme和授权来觉得使用哪个文件系统，如果没有设置scheme，则使用conf中配置的文件系统。第三个接口是以user用户的身份获取文件系统。 使用FileSystem实例，调用open方法可以获取一个输入文件流：</p>

<pre><code>public FSDataInputStream open(Path f) throws IOException
public abstract FSDataInputStream open(Path f, int bufferSize) throws IOException 
</code></pre>

<p>下面的代码实例用于读取hdfs文件输出到标准输出：</p>

<pre><code>public class FileSystemCat {

    public static void main(String[] args) throws Exception {
        String uri = args[0];
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(uri), conf);
        InputStream in = null;
        try {
            in = fs.open(new Path(uri));
            IOUtils.copyBytes(in, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
</code></pre>

<p>编译文件后，使用hadoop命令运行class文件：</p>

<pre><code># hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txt
On the top of the Crumpetty Tree
The Quangle Wangle sat, 
But his face you could not see,
On account of his Beaver Hat.
</code></pre>

<h2>Writing Data</h2>

<p>FileSystem提供多个接口来创建文件，最简单的是输入一个路径作为参数，返回一个输出流：</p>

<pre><code>public FSDataOutputStream create(Path f) throws IOException 存在多个重载接口，允许指定是否重写已存在文件，文件的副本个数，写文件的缓冲区大小，文件的块大小和文件权限等等。存在append接口对已存在文件进行追加：
public FSDataOutputStream append(Path f) throws IOException
</code></pre>

<p>下面的代码实例是拷贝一个本地文件到Hadoop文件系统中：</p>

<pre><code>public class FileCopyWithProgress {
    public static void main(String []args) throws Exception {
        String localSrc = args[0];
        String dst = args[1];

        InputStream in = new BufferedInputStream(new FileInputStream(localSrc));

        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(URI.create(dst), conf);
        OutputStream out = fs.create(new Path(dst), new Progressable() {
            public void progress() {
                System.out.print(".");
            }
        });

        IOUtils.copyBytes(in, out, 4096, true);
    }
}
</code></pre>

<p>运行程序结果如下：
    #hadoop FileCopyWithProgress input/docs/1400-8.txt hdfs://localhost/user/tom/1400-8.txt</p>

<p>创建目录使用下面的接口：</p>

<pre><code>public boolean mkdirs(Path f) throws IOException
</code></pre>

<h2>Quering the FileSystem</h2>

<p>FilsSystem提供了接口来获取文件或者文件夹信息，信息包括文件长度，块大小，复制，修改时间，所有权，访问权限属性。</p>

<pre><code>abstract FileStatus getFileStatus(Path f)
</code></pre>

<p>FileSystem提供了接口来列出一个目录中包含的内容：</p>

<pre><code>abstract FileStatus[]   listStatus(Path f)
FileStatus[]    listStatus(Path[] files)
FileStatus[]    listStatus(Path[] files, PathFilter filter)
FileStatus[]    listStatus(Path f, PathFilter filter)
</code></pre>
]]></content>
  </entry>
  
</feed>
