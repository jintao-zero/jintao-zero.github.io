<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Network | jintao's blog]]></title>
  <link href="http://jintao-zero.github.io/blog/categories/network/atom.xml" rel="self"/>
  <link href="http://jintao-zero.github.io/"/>
  <updated>2017-09-04T20:23:38+08:00</updated>
  <id>http://jintao-zero.github.io/</id>
  <author>
    <name><![CDATA[jintao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[翻译]SOCKS5 协议]]></title>
    <link href="http://jintao-zero.github.io/blog/2017/04/02/socks5-xie-yi/"/>
    <updated>2017-04-02T16:38:00+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2017/04/02/socks5-xie-yi</id>
    <content type="html"><![CDATA[<h2>介绍</h2>

<p>网络防火墙用来隔离组织内部网络和外部网络，比如<code>INTERNET</code>。这些防火墙系统扮演着内外部网络之间的应用层网关，用来控制<code>TELNET</code>、<code>FTP</code>和<code>SMTP</code>等协议的进入。随着更多复杂应用层协议被设计用来加速全局信息发现，需要为这些协议提供一个框架以便透明和安全的通过防火墙。 也需要提供为这种通过提供可靠授权。 <br/>
<code>SOCKS5</code>协议被设计用来为基于TCP或者UDP协议的client-server模式应用程序方便且安全的使用网络防火墙服务。这个协议概念上认为是在应用层和传输层之间，因此它不提供网络层网关服务，比如转发<code>ICMP</code>消息。</p>

<p>已经存在了一个<code>SOCKS4</code>协议，这个版本的协议在4版本的基础上支持UDP协议，包含通用授权范型和支持域名和V6版本IP地址。</p>

<!-- more -->


<h2>TCP客户端处理步骤</h2>

<p>当一个基于TCP协议的客户端希望与位于防火墙后面的目标之间建立一个连接时，它必须打开一个到<code>SOCKS</code>服务端的连接。<code>SOCKS</code>服务按照惯例位于1080TCP端口。 如果连接建立成功后，客户端发送鉴权请求。服务端评估请求，要么建立相应连接或者拒绝这个请求。  <br/>
客户端连接socks服务端，发送一个版本标识符或者方法选择消息：</p>

<pre><code>+----+----------+----------+
|VER | NMETHODS | METHODS  |
+----+----------+----------+
| 1  |    1     | 1 to 255 |
+----+----------+----------+
</code></pre>

<p><code>VER</code>字段设置为<code>X'05'</code>是这个协议的版本。<code>NMETHODS</code>字段包含出现在<code>METHODS</code>字段中的值的字节数。<br/>
socks服务端从<code>METHODS</code>中选择一个方法，发送一个<code>METHOD</code>选择消息：</p>

<pre><code>+----+--------+
|VER | METHOD |
+----+--------+
| 1  |   1    |
+----+--------+
</code></pre>

<p>如果选择的<code>METHOD</code>为<code>X'FF'</code>，那么客户端列出的所有方法都不被接受，这时候客户端必须关闭连接。<br/>
目前定义的<code>METHOD</code>值有：</p>

<pre><code>o  X'00' NO AUTHENTICATION REQUIRED
o  X'01' GSSAPI
o  X'02' USERNAME/PASSWORD
o  X'03' to X'7F' IANA ASSIGNED
o  X'80' to X'FE' RESERVED FOR PRIVATE METHODS
o  X'FF' NO ACCEPTABLE METHODS  
</code></pre>

<p>方法选择之后，客户端和服务端之间进入与方法相关的子协商。</p>

<h2>请求</h2>

<p>子协商完成后，客户端发送请求详情。如果协商结果是需要对发送的消息进行封装加密，那么请求消息必须进行封装加密。<br/>
SOCKS协议请求格式为：</p>

<pre><code>+----+-----+-------+------+----------+----------+
|VER | CMD |  RSV  | ATYP | DST.ADDR | DST.PORT |
+----+-----+-------+------+----------+----------+
| 1  |  1  | X'00' |  1   | Variable |    2     |
+----+-----+-------+------+----------+----------+
</code></pre>

<p>字段说明如下：</p>

<pre><code>o  VER    protocol version: X'05'
o  CMD
o  CONNECT X'01'
o  BIND X'02'
o  UDP ASSOCIATE X'03'
o  RSV    RESERVED
o  ATYP   address type of following address
o  IP V4 address: X'01'
o  DOMAINNAME: X'03'
o  IP V6 address: X'04'
o  DST.ADDR       desired destination address
o  DST.PORT desired destination port in network octet order
</code></pre>

<p>SOCKS服务端将会根据源和目的地址评估请求，并且根据请求类型返回一个或者多个回复消息。</p>

<h2>地址</h2>

<p>在地址字段（<code>DST.ADDR BND.ADDR</code>），<code>ATYP</code>字段指示了地址的类型：</p>

<ul>
<li>X&#8217;01&#8217;
指示地址为IPv4地址，长度为4个字节。</li>
<li>X&#8217;03&#8217;
指示地址为域名。地址的第一个字节为域名的长度，域名不是以<code>NUL</code>结束的。</li>
<li>X&#8217;04&#8217;
地址为IPv6地址，长度为16个字节。</li>
</ul>


<h2>回复</h2>

<p>对应上面的请求，服务端回复以下应答：</p>

<pre><code>+----+-----+-------+------+----------+----------+
|VER | REP |  RSV  | ATYP | BND.ADDR | BND.PORT |
+----+-----+-------+------+----------+----------+
| 1  |  1  | X'00' |  1   | Variable |    2     |
+----+-----+-------+------+----------+----------+ 
</code></pre>

<p> 字段说明如下：</p>

<pre><code> o  VER    protocol version: X'05'
 o  REP    Reply field:
    o  X'00' succeeded
    o  X'01' general SOCKS server failure
    o  X'02' connection not allowed by ruleset
    o  X'03' Network unreachable
    o  X'04' Host unreachable
    o  X'05' Connection refused
    o  X'06' TTL expired
    o  X'07' Command not supported
    o  X'08' Address type not supported
    o  X'09' to X'FF' unassigned

 o  RSV    RESERVED
 o  ATYP   address type of following address
     o  IP V4 address: X'01'
     o  DOMAINNAME: X'03'
     o  IP V6 address: X'04'
 o  BND.ADDR       server bound address
 o  BND.PORT       server bound port in network octet order
</code></pre>

<p>标记为<code>RESERVED</code>的字段必须设置为X&#8217;00&#8217;</p>

<h3>CONNECT</h3>

<p>对于<code>CONNECT</code>请求消息的应答中，<code>BND.PORT</code>字段为服务端绑定的用来连接到目标主机的端口号，<code>BND.ADDR</code>字段为绑定的地址。绑定的地址通常与客户端连接服务端的地址不同。</p>

<h3>BIND</h3>

<p>BIND请求被用来要求客户端接受来自服务端的请求。FTP是一个常见的例子，它使用客户端到服务端的连接发送命令和状态，使用服务端到客户端的连接传输数据。  <br/>
客户端使用<code>CONNECT</code>建立主连接，使用<code>BIND</code>请求只是用来建立第二个连接。<br/>
对于<code>BIND</code>请求，<code>SOCKS</code>服务端将使用<code>DST.ADDR</code>和<code>DST.PORT</code>。<br/>
对于<code>BIND</code>操作，SOCKS服务端将会发送两个回复到客户端。服务端创建和绑定一个新的socket套接字后发送第一个回复。回复中<code>BND.PORT</code>字段值为SOCKS服务端本地绑定用来监听和接受连接的端口号。<code>BND.ADDR</code>字段为SOCKS服务端绑定的IP。客户端通过主控制链路发送这些信息给应用程序服务端。当新的连接建立成功或者失败后，SOCKS服务端会发送第二个应答。 在第二个应答中，<code>BND.PORT</code>和<code>BND.ADDR</code>字段包含连接主机的ip和端口信息。</p>

<h3>UDP</h3>

<p><code>UDP ASSOCIATE</code>请求用来与UDP中继进程建立一个联系来处理UDP数据报。客户端希望发送UDP数据报到<code>DST.ADDR</code>和<code>DST.PORT</code>字段标示的IP和端口号。在建立UDP ASSOCIATE时，如果客户端不拥有这些信息，客户端必须使用全零的端口号和ip地址。  <br/>
当发送UDP ASSOCIATE请求的TCP连接中断时，这个UDP 连接也要中断。<br/>
在对UDP ASSOCIATE请求的应答中，<code>BND.PORT</code>和<code>BND.ADDR</code>字段表示客户端必须将待转发的UDP消息发送到这个地址。</p>

<h3>应答处理</h3>

<p>如果应答消息中结果字段为失败，SOCKS服务端必须在发送应答后立即关闭TCP连接。关闭连接必须在检测到失败后10秒内终止连接。</p>

<p>如果应答消息中结果字段为成功，请求为<code>BIND</code>或者<code>CONNECT</code>的话，客户端就可以开始传输数据。 如果协商了封装加密方法的话，在客户端和SOCKS服务端通信过程中需要进行封装加密。</p>

<h2>UDP客户端</h2>

<p>udp客户端必须想UDP ASSOCIATE应答中<code>BND.PORT</code>标示的字段发送UDP数据报。如果协商了封装方法，那么通讯时需要进行封装。<br/>
每个UDP数据需要携带一个UDP请求头：</p>

<pre><code>+----+------+------+----------+----------+----------+
|RSV | FRAG | ATYP | DST.ADDR | DST.PORT |   DATA   |
+----+------+------+----------+----------+----------+
| 2  |  1   |  1   | Variable |    2     | Variable |
+----+------+------+----------+----------+----------+
</code></pre>

<p> 字段说明如下：</p>

<pre><code> o  RSV  Reserved X'0000'
 o  FRAG    Current fragment number
 o  ATYP    address type of following addresses:
        o  IP V4 address: X'01'
      o  DOMAINNAME: X'03'
      o  IP V6 address: X'04'
 o  DST.ADDR       desired destination address
 o  DST.PORT       desired destination port
 o  DATA     user data
</code></pre>

<p>UDP中继服务端必须从SOCKS服务端获取申请建立UDP ASSOCIATE的客户端ip地址。中继服务端对于其他源ip发送过来的数据包将会丢弃。</p>

<h2>参考</h2>

<p><a href="https://www.ietf.org/rfc/rfc1928.txt">rfc1928</a><br/>
<a href="https://tools.ietf.org/html/rfc1929">rfc1929</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Golang 实现HTTP代理]]></title>
    <link href="http://jintao-zero.github.io/blog/2017/03/29/golang-shi-xian-httpdai-li/"/>
    <updated>2017-03-29T16:51:16+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2017/03/29/golang-shi-xian-httpdai-li</id>
    <content type="html"><![CDATA[<p>使用Golang实现一个HTTP代理程序，程序部署在海外VPS节点，可以实现翻墙功能<br/>
HTTP Proxy的基本原理是将客户端HTTP请求发送到目标主机，将目标主机返回的HTTP应答转发到客户端,对于HTTPS协议略有不同，HTTPS协议需要Proxy先建立一条到目标主机的链路，之后再进行协议本身的通讯，下面简单介绍一个HTTP Proxy的实现过程：</p>

<h3>分析第一条请求</h3>

<ol>
<li><p>启动一个简版HTTP Proxy程序，代码如下：</p>

<pre><code> func handleConn(conn net.Conn) {
     defer conn.Close()
     log.Println("client conn form ", conn.RemoteAddr())
     var buffer [1024]byte
     _, err := conn.Read(buffer[:])
     if err != nil {
         return
     }
     log.Println(string(buffer[:]))
 }

 func main() {
     // listen on tcp port
     l, err := net.Listen("tcp", ":8081")
     if err != nil {
         log.Println(err)
         return
     }
     for {
         conn, err := l.Accept()
         if err != nil {
             log.Println(err)
             return
         }
         go handleConn(conn)
     }
 }  
 go run httpproxy.go
</code></pre>

<p> 这个程序主要是用来接收客户端发送的第一条HTTP请求</p></li>
</ol>


<!-- more -->


<ol>
<li><p>Safari设置HTTP/HTTPS代理<br/>
 设置代理为<code>localhost:8081</code><br/>
 <img src="/images/http-proxy-1.png" alt="proxy" /></p></li>
<li><p>Safari输入框访问<code>http://localhost:8080</code>，我们的proxy程序输出HTTP请求：</p>

<pre><code> GET http://www.localhost.com:8080/ HTTP/1.1
 Host: www.localhost.com:8080
 Proxy-Connection: keep-alive
 Upgrade-Insecure-Requests: 1
 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko)     Version/10.0.3 Safari/602.4.8
 Accept-Language: zh-cn
 Accept-Encoding: gzip, deflate
 Connection: keep-alive
</code></pre>

<p>其中跟代理有关的是前两行：</p>

<pre><code>  GET http://www.localhost.com:8080/ HTTP/1.1
  Host: www.localhost.com:8080
</code></pre>

<p>当访问<code>http://localhost</code>时，输出的前两行为：</p>

<pre><code>  GET http://www.localhost.com/ HTTP/1.1
  Host: www.localhost.com
</code></pre></li>
<li><p>Safari输入框访问<code>https://localhost:8080</code>时，proxy程序输出HTTP请求为：</p>

<pre><code> CONNECT www.localhost.com:8080 HTTP/1.1
 Host: www.localhost.com:8080
 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8
 Connection: keep-alive
 Proxy-Connection: keep-alive
</code></pre>

<p>访问网址为<code>https://localhost</code>时，输出为：</p>

<pre><code> CONNECT www.localhost.com:443 HTTP/1.1
 Host: www.localhost.com
 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8
 Connection: keep-alive
 Proxy-Connection: keep-alive
</code></pre></li>
</ol>


<p>设置代理后，客户端发送HTTP请求之前需要先与Proxy通讯建立到目标主机的通讯隧道，命令消息格式为：</p>

<pre><code>method absoluteURI HTTP/1.1
</code></pre>

<p>使用<code>HTTP</code>协议时，使用<code>GET</code>命令发送请求，使用<code>HTTPS</code>协议时，使用<code>CONNECT</code>命令发送请求,后面跟着<a href="http://greenbytes.de/tech/webdav/rfc2616.html#general.syntax">absoluteURI</a>,再就是HTTP版本信息
第二行为<code>Host</code>头标示了需要访问的目标地址，根据<a href="[Request-URI](http://greenbytes.de/tech/webdav/rfc2616.html#request-uri"><code>HTTP/1.1</code></a>)版本规定，客户端和服务的必须支持<code>Host</code>头，但是默认协议情况下，host头只包含域名，不包含默认端口号(<code>HTTP(80)``HTTPS(443)</code>)</p>

<p>我们的proxy程序需要根据<code>absoluteURI</code>获取目标域名和端口，从而建立到目标主机的连接，示例代码如下：</p>

<pre><code>var buffer [1024]byte
n, err := conn.Read(buffer[:])
if err != nil {
    return
}
log.Println(string(buffer[:]))
_, line, err := bufio.ScanLines(buffer[:], true)
if err != nil {
    log.Println(err)
    return
}
var method string
var absUri string
_, err = fmt.Sscanf(string(line), "%s%s", &amp;method, &amp;absUri)
if err != nil {
    log.Println(err)
    return
}
absUrl, err := url.Parse(absUri)
if err != nil {
    log.Println(err)
    return
}
log.Printf("%+v", *absUrl)
var hostPort string
if absUrl.Scheme == "http" {
    if strings.Index(absUrl.Host, ":") == -1 {
        hostPort = absUrl.Host + ":80"
    } else {
        hostPort = absUrl.Host
    }
} else {
    hostPort = absUrl.Scheme + ":" + absUrl.Opaque
}
log.Println("hostPort", hostPort)
</code></pre>

<h3>建立连接</h3>

<ol>
<li>根据获取的目标地址+端口建立到目标主机的tcp连接</li>
<li><p>根据第一条请求的协议类型(HTTP/HTTPS),做不同应答处理：<br/>
 2.1 HTTP消息，直接转发给目标主机，不对客户端有应答<br/>
 2.2 HTTPS CONNECT消息，对客户端应答<code>200</code>消息：</p>

<pre><code> HTTP/1.1 200 Connection established\r\n\r\n
</code></pre></li>
<li><p>不间断进行消息转发，某一方连接异常后，关闭双方连接</p>

<pre><code> go io.Copy(conn, serverConn)
 io.Copy(serverConn, conn)
</code></pre></li>
</ol>


<h3>部署</h3>

<ol>
<li><a href="https://github.com/jintao-zero/golang-http-proxy">完整代码</a></li>
<li>编译生成二进制可执行文件，部署到海外VPS服务器，浏览器设置对应ip和端口后，即可翻墙</li>
</ol>


<h3>遗留问题</h3>

<p>1、转发功能基本实现，但是依然无法访问google、facebook等被GFW墙的网站
    我猜想是明文HTTP请求中<code>google、facebook</code>类的网站名被匹配到，出口连接被关闭，使用了ssh tunnel功能，将数据加密传输解决了这个问题</p>

<h3>参考</h3>

<ol>
<li><a href="http://greenbytes.de/tech/webdav/rfc2616.html#changes.from.1.0">HTTP1.0 VS HTTP1.1</a></li>
<li><a href="http://stackoverflow.com/questions/7155529/how-does-http-proxy-work">how does http proxy work?</a></li>
<li><a href="https://en.wikipedia.org/wiki/Proxy_server#Web_proxy_servers">Proxy server</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Libnetfilter_queue实战]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/06/16/libnetfilter-queuexue-xi-bi-ji/"/>
    <updated>2015-06-16T18:53:42+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/06/16/libnetfilter-queuexue-xi-bi-ji</id>
    <content type="html"><![CDATA[<p>项目需要将某个进程发出的报文截获，待用户态业务处理后再将报文送回系统协议栈中处理。调研后，采用iptables添加防火墙规则截获报文传递到用户态队列，用户态进程使用libnetfilter_queue库接受报文，待业务处理后，将报文送回系统协议栈这样的方案完成。下面介绍下，学习的几个知识点：</p>

<h1>netfilter</h1>

<h2>简介</h2>

<p><a href="http://netfilter.org/">netfilter</a>是Linux 2.4.x开始以后内核版本中的报文过滤框架。netfilter框架的用户态工具是<a href="http://netfilter.org/projects/iptables/index.html">iptables</a>防火墙配置工具。<br/>
netfilter框架实现了报文过滤，网络地址转换，端口转换和其他报文管理。它是对Linux 2.2.x ipchains和Linux 2.0.x ipfwadm系统的重构和显著改进。<br/>
netfilter是位于Linux内核中的一组回调函数集合，各个模块可以在网络协议栈注册回调函数。每个报文到来时，将会遍历协议栈中的回调函数并调用。<br/>
iptables定义了规则集的通用格式。IP table中的每一条规则是有一些分类和相关的目标组成。<br/>
netfilter，ip_tables，connection tracking（ip_conntrack，nf_conntrack）和NAT子系统是netfilter的主要组成部分。</p>

<h2>主要特性</h2>

<ul>
<li>无状态报文过滤</li>
<li>有状态报文过滤</li>
<li>所有网络地址和端口转换，比如NAT/NAPT(IPv4和IPv6)</li>
<li>灵活的、可扩展的框架</li>
<li>为第三方扩展提供API访问网络各层</li>
</ul>


<h2>可以做哪些</h2>

<ul>
<li>基于无状态或者有状态报文过滤构建互联网防火墙</li>
<li>托管高可用有状态或者无状态的防火墙集群</li>
<li>使用NAT和ip伪装共享共网ip</li>
<li>使用NAT实现传输代理</li>
<li>与tc和iproute2系统一起构建复杂Qos和策略路由器</li>
<li>更进一步进行报文管理，比如修改IP头中的TOS/DSCP/ECN</li>
</ul>


<!-- more -->


<h1>libnetfilter_queue</h1>

<p>libnetfilter_queue是一个用户态库，它提供了接口用来访问已经被内核报文过滤模块放置在队列中的报文。libnetfilter_queue的前身是libnfnetlink_queue。<br/>
libnetfilter_queue需要libnfnetlink和包含nfnetlink_queue子系统的内核（2.6.14以后）。<br/>
主要特性：</p>

<ul>
<li>接收来自内核nfnetlink_queue子系统的报文</li>
<li>做出裁决，是否将修改后的报文重新注入内核nfnetlink_queue子系统</li>
</ul>


<h2>安装</h2>

<p>libnetfilter_queue需要libnfnetlink和libmnl库<br/>
1、安装libnfnetlink</p>

<pre><code>#wget http://netfilter.org/projects/libnfnetlink/files/libnfnetlink-1.0.1.tar.bz2
#tar jxvf libnfnetlink-1.0.1.tar.bz2
#cd libnfnetlink-1.0.1  
#./configure
#make &amp;&amp; make install
</code></pre>

<p>2、安装libmnl</p>

<pre><code>#wget http://netfilter.org/projects/libmnl/files/libmnl-1.0.3.tar.bz2
#tar jxvf libmnl-1.0.3.tar.bz2
#cd libmnl-1.0.3
#./configure
#make &amp;&amp; make install
</code></pre>

<p>3、安装libnetfilter_queue</p>

<pre><code>#wget http://netfilter.org/projects/libnetfilter_queue/files/libnetfilter_queue-1.0.2.tar.bz2
#tar jxvf libnetfilter_queue-1.0.2.tar.bz2
#cd libnetfilter_queue-1.0.2
#./configure
#make &amp;&amp; make install
</code></pre>

<h2>接口</h2>

<h3>初始化库</h3>

<pre><code>struct nfq_handle *     nfq_open (void)
int     nfq_close (struct nfq_handle *h)
int     nfq_bind_pf (struct nfq_handle *h, u_int16_t pf)
int     nfq_unbind_pf (struct nfq_handle *h, u_int16_t pf)
</code></pre>

<p>libnetfilter_queue初始化分为两部。<br/>
第一步调用nfq_open打开一个NFQUEUE句柄。<br/>
第二部通知内核指定协议的用户空间队列由NFQUEUE处理。调用nfq_unbind_pf()和nfq_bind接口进行绑定。<br/>
以下为初始化片段：</p>

<pre><code>h = nfq_open();
if (!h) {
    fprintf(stderr, "error during nfq_open()\n");
    exit(1);
}

printf("unbinding existing nf_queue handler for AF_INET (if any)\n");
if (nfq_unbind_pf(h, AF_INET) &lt; 0) {
    fprintf(stderr, "error during nfq_unbind_pf()\n");
    exit(1);
}

printf("binding nfnetlink_queue as nf_queue handler for AF_INET\n");
if (nfq_bind_pf(h, AF_INET) &lt; 0) {
    fprintf(stderr, "error during nfq_bind_pf()\n");
    exit(1);
}
</code></pre>

<p>nfq_open</p>

<pre><code>struct nfq_handle* nfq_open (void) 
</code></pre>

<p>nfq_open返回一个netfilter队列连接句柄。当使用完毕，调用nfq_close销毁句柄。在系统内部，会新创建一个netlink连接并与队列连接句柄相关联。</p>

<p>nfq_bind_pf</p>

<pre><code>int nfq_bind_pf (   struct nfq_handle *     h,
                    u_int16_t   pf   
                )   
</code></pre>

<p>nfq_bind_pf绑定一个协议域到nfqueue句柄。<br/>
队列连接句柄将会处理属于pf指定的协议报文，比如（PF_INET，PF_INET6）。
<a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/group__LibrarySetup.html">详细说明文档</a></p>

<h3>处理报文队列</h3>

<pre><code>int     nfq_fd (struct nfq_handle *h)   
struct nfq_q_handle *   nfq_create_queue (struct nfq_handle *h, u_int16_t num, nfq_callback *cb, void *data)
int     nfq_destroy_queue (struct nfq_q_handle *qh)
int     nfq_handle_packet (struct nfq_handle *h, char *buf,     int len)
int     nfq_set_mode (struct nfq_q_handle *qh, u_int8_t mode, u_int32_t range)
int     nfq_set_queue_maxlen (struct nfq_q_handle *qh, u_int32_t queuelen)
int     nfq_set_verdict (struct nfq_q_handle *qh, u_int32_t id, u_int32_t verdict, u_int32_t data_len, const unsigned char *buf)
int     nfq_set_verdict2 (struct nfq_q_handle *qh, u_int32_t id, u_int32_t verdict, u_int32_t mark, u_int32_t data_len, const unsigned char *buf)
int     nfq_set_verdict_mark (struct nfq_q_handle *qh, u_int32_t id, u_int32_t verdict, u_int32_t mark, u_int32_t data_len, const unsigned char *buf)
</code></pre>

<p>libnetfilter_queue库初始化后，可以绑定程序到具体报文队列。使用nfq_create_queue()进行绑定。
使用nfq_set_mode()和nfq_set_queue_maxlen()函数设置队列。
下面是创建队列0的代码片段：</p>

<pre><code>printf("binding this socket to queue '0'\n");
qh = nfq_create_queue(h,  0, &amp;cb, NULL);
if (!qh) {
    fprintf(stderr, "error during nfq_create_queue()\n");
    exit(1);
}

printf("setting copy_packet mode\n");
if (nfq_set_mode(qh, NFQNL_COPY_PACKET, 0xffff) &lt; 0) {
    fprintf(stderr, "can't set packet_copy mode\n");
    exit(1);
}
</code></pre>

<p>下一步是，循环读取队列中的报文进行处理：</p>

<pre><code>fd = nfq_fd(h);

while ((rv = recv(fd, buf, sizeof(buf), 0)) &gt;= 0) {
    printf("pkt received\n");
    nfq_handle_packet(h, buf, rv);
}
</code></pre>

<p>我们需要对每一个报文做出处理结果，通过nfq_set_verdict()或者nfq_set_verdict2()结果来传递结果。裁决结果决定报文的命运：</p>

<ul>
<li>NF_DROP   丢弃此报文</li>
<li>NF_ACCEPT 接受报文，继续迭代</li>
<li>NF_STOLEN gone away</li>
<li>NF_QUEUE  将报文注入另外一个队列</li>
<li>NF_REPEAT 重新再迭代一次</li>
<li>NF_STOP 接受，但是不再继续迭代</li>
</ul>


<p><a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/group__Queue.html">详细说明文档</a></p>

<h3>解析报文</h3>

<p>主要涉及以下接口：</p>

<pre><code>struct nfqnl_msg_packet_hdr *   nfq_get_msg_packet_hdr (struct nfq_data *nfad)
uint32_t    nfq_get_nfmark (struct nfq_data *nfad)
int     nfq_get_timestamp (struct nfq_data *nfad, struct timeval *tv)
u_int32_t   nfq_get_indev (struct nfq_data *nfad)
u_int32_t   nfq_get_physindev (struct nfq_data *nfad)
u_int32_t   nfq_get_outdev (struct nfq_data *nfad)
u_int32_t   nfq_get_physoutdev (struct nfq_data *nfad)
int     nfq_get_indev_name (struct nlif_handle *nlif_handle,    struct nfq_data *nfad, char *name)
int     nfq_get_physindev_name (struct nlif_handle *nlif_handle, struct nfq_data *nfad, char *name)
int     nfq_get_outdev_name (struct nlif_handle *nlif_handle, struct nfq_data *nfad, char *name)
int     nfq_get_physoutdev_name (struct nlif_handle *nlif_handle, struct nfq_data *nfad, char *name)
struct nfqnl_msg_packet_hw *    nfq_get_packet_hw (struct nfq_data *nfad)
int     nfq_get_payload (struct nfq_data *nfad, unsigned char **data)
</code></pre>

<p><a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/group__Parsing.html">详细说明文档</a></p>

<h2>示例</h2>

<p>示例源自libnetfilter_queue源代码中的例子<a href="http://netfilter.org/projects/libnetfilter_queue/doxygen/nfqnl__test_8c_source.html">nfqnl_test.c</a></p>

<pre><code>00001 
00002 #include &lt;stdio.h&gt;
00003 #include &lt;stdlib.h&gt;
00004 #include &lt;unistd.h&gt;
00005 #include &lt;netinet/in.h&gt;
00006 #include &lt;linux/types.h&gt;
00007 #include &lt;linux/netfilter.h&gt;            /* for NF_ACCEPT */
00008 
00009 #include &lt;libnetfilter_queue/libnetfilter_queue.h&gt;
00010 
00011 /* returns packet id */
00012 static u_int32_t print_pkt (struct nfq_data *tb)
00013 {
00014         int id = 0;
00015         struct nfqnl_msg_packet_hdr *ph;
00016         struct nfqnl_msg_packet_hw *hwph;
00017         u_int32_t mark,ifi; 
00018         int ret;
00019         unsigned char *data;
00020 
00021         ph = nfq_get_msg_packet_hdr(tb); // 返回报文头信息
00022         if (ph) {
00023                 id = ntohl(ph-&gt;packet_id);
00024                 printf("hw_protocol=0x%04x hook=%u id=%u ",
00025                         ntohs(ph-&gt;hw_protocol), ph-&gt;hook, id);
00026         }
00027 
00028         hwph = nfq_get_packet_hw(tb); // 返回报文硬件地址信息
00029         if (hwph) {
00030                 int i, hlen = ntohs(hwph-&gt;hw_addrlen);
00031 
00032                 printf("hw_src_addr=");
00033                 for (i = 0; i &lt; hlen-1; i++)
00034                         printf("%02x:", hwph-&gt;hw_addr[i]);
00035                 printf("%02x ", hwph-&gt;hw_addr[hlen-1]);
00036         }
00037 
00038         mark = nfq_get_nfmark(tb); // 返回报文标记
00039         if (mark)
00040                 printf("mark=%u ", mark);
00041 
00042         ifi = nfq_get_indev(tb); // 获取报文接收网卡的索引
00043         if (ifi)
00044                 printf("indev=%u ", ifi);
00045 
00046         ifi = nfq_get_outdev(tb); // 获取报文发送网卡的索引
00047         if (ifi)
00048                 printf("outdev=%u ", ifi); 
00049         ifi = nfq_get_physindev(tb); // 接收此报文的物理网卡索引
00050         if (ifi)
00051                 printf("physindev=%u ", ifi);
00052  
00053         ifi = nfq_get_physoutdev(tb); // 发送次报文的物理网卡索引
00054         if (ifi)
00055                 printf("physoutdev=%u ", ifi);
00056 
00057         ret = nfq_get_payload(tb, &amp;data); // 获取报文内容的长度
00058         if (ret &gt;= 0)
00059                 printf("payload_len=%d ", ret);
00060 
00061         fputc('\n', stdout);
00062 
00063         return id;
00064 }
00065         
00066 
00067 static int cb(struct nfq_q_handle *qh, struct nfgenmsg *nfmsg,
00068               struct nfq_data *nfa, void *data)
00069 {
00070         u_int32_t id = print_pkt(nfa);
00071         printf("entering callback\n");
00072         return nfq_set_verdict(qh, id, NF_ACCEPT, 0, NULL);
00073 }
00074 
00075 int main(int argc, char **argv)
00076 {
00077         struct nfq_handle *h;
00078         struct nfq_q_handle *qh;
00079         struct nfnl_handle *nh;
00080         int fd;
00081         int rv;
00082         char buf[4096] __attribute__ ((aligned));
00083           
00084         printf("opening library handle\n");
00085         h = nfq_open(); // 获取libnetfilter 队列句柄
00086         if (!h) {
00087                 fprintf(stderr, "error during nfq_open()\n");
00088                 exit(1);
00089         }
00090 
00091         printf("unbinding existing nf_queue handler for AF_INET (if any)\n");
00092         if (nfq_unbind_pf(h, AF_INET) &lt; 0) { // 先将队列连接句柄与AF_INET地址域解绑
00093                 fprintf(stderr, "error during nfq_unbind_pf()\n");
00094                 exit(1);
00095         }
00096 
00097         printf("binding nfnetlink_queue as nf_queue handler for AF_INET\n");
00098         if (nfq_bind_pf(h, AF_INET) &lt; 0) { // 将队列连接句柄与AF_INET地址绑定
00099                 fprintf(stderr, "error during nfq_bind_pf()\n");
00100                 exit(1);
00101         }
00102 
00103         printf("binding this socket to queue '0'\n");
00104         qh = nfq_create_queue(h,  0, &amp;cb, NULL); // 创建队列句柄用来处理0队列报文
00105         if (!qh) {
00106                 fprintf(stderr, "error during nfq_create_queue()\n");
00107                 exit(1);
00108         }
00109 
00110         printf("setting copy_packet mode\n");
00111         if (nfq_set_mode(qh, NFQNL_COPY_PACKET, 0xffff) &lt; 0) { // 设置队列工作模式
00112                 fprintf(stderr, "can't set packet_copy mode\n");
00113                 exit(1);
00114         }
00115 
00116         fd = nfq_fd(h); // 获取队列连接描述符
00117 
00118         while ((rv = recv(fd, buf, sizeof(buf), 0)) &amp;&amp; rv &gt;= 0) {
00119                 printf("pkt received\n");
00120                 nfq_handle_packet(h, buf, rv); // 处理收到的每个报文
00121         }
00122 
00123         printf("unbinding from queue 0\n");
00124         nfq_destroy_queue(qh); // 从队列0解绑
00125 
00126 #ifdef INSANE
00127         /* normally, applications SHOULD NOT issue this command, since
00128          * it detaches other programs/sockets from AF_INET, too ! */
00129         printf("unbinding from AF_INET\n");
00130         nfq_unbind_pf(h, AF_INET);
00131 #endif
00132 
00133         printf("closing library handle\n");
00134         nfq_close(h); // 关闭库句柄
00135 
00136         exit(0);
00137 }
</code></pre>

<p>编译程序：</p>

<pre><code>gcc nfqnl_test.c -lnetfilter_queue -o nfqnl_test
</code></pre>

<p>配置iptables：
使用iptables命令修改防火墙规则，将发送到112.80.248.74的报文放置到队列0中：</p>

<pre><code>#iptables -t filter -A OUTPUT -d 112.80.248.74 -j NFQUEUE --queue-num 0
#iptables -t filter -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
NFQUEUE    all  --  anywhere             112.80.248.74  NFQUEUE num 0
</code></pre>

<p>启动ping程序：</p>

<pre><code># ping 112.80.248.74
PING 112.80.248.74 (112.80.248.74) 56(84) bytes of data.
</code></pre>

<p>当nfqnl_test程序未启动时，ping程序不能正确发送出icmp报文，也就不能收到icmp回复。</p>

<p>启动nfqnl_test程序：</p>

<pre><code># ./nfqnl_test
opening library handle
unbinding existing nf_queue handler for AF_INET (if any)
binding nfnetlink_queue as nf_queue handler for AF_INET
binding this socket to queue '0'
setting copy_packet mode
pkt received
hw_protocol=0x0000 hook=3 id=1 outdev=2 payload_len=84
entering callback
pkt received
hw_protocol=0x0000 hook=3 id=2 outdev=2 payload_len=84
</code></pre>

<p>nfqnl_test程序从nfqueue_num 0中读取报文，打印相关信息后，将报文送回协议栈，完成了发送流程，从目标收到了icmp报文回复</p>

<pre><code>64 bytes from 112.80.248.74: icmp_req=612 ttl=58 time=1.84 ms
64 bytes from 112.80.248.74: icmp_req=613 ttl=58 time=1.83 ms
64 bytes from 112.80.248.74: icmp_req=614 ttl=58 time=1.76 ms
64 bytes from 112.80.248.74: icmp_req=615 ttl=58 time=1.83 ms
64 bytes from 112.80.248.74: icmp_req=616 ttl=58 time=1.97 ms
64 bytes from 112.80.248.74: icmp_req=617 ttl=58 time=1.92 ms
64 bytes from 112.80.248.74: icmp_req=618 ttl=58 time=2.07 ms   
</code></pre>

<h1>参考</h1>

<p>netfilter<a href="http://www.netfilter.org/">官网</a><br/>
<a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=1196223">用户态修改网络数据包例子</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp Server使用kevent进行io复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong/"/>
    <updated>2015-05-01T08:40:25+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong</id>
    <content type="html"><![CDATA[<h1>概述</h1>

<p>kqueue是FreeBSD系统中引入的可扩展事件通知接口，NetBSD，OpenBSD，DragonflyBSD和OS X这些系统也支持此接口。传统的io复用接口如select和poll存在以下两个缺点：</p>

<ul>
<li>支持的文件描述符数量有限，select接口支持的文件描述个数与进程能够打开的最大文件个数有关</li>
<li>当大量描述符需要复用时，select和poll的效率会降低，内核是采用轮询方式判断描述符是否可用</li>
</ul>


<p>kqueue是针对传统传统的select/poll处理大量的文件描述符性能较低效而开发出来的。注册一批描述符到kqueue以后，当其中的描述符状态发生变化时，kqueue将一次性通知应用程序哪些描述符可读、可写或者发生错误。</p>

<p>kqueue支持多种类型的文件描述符，包括socket、信号、定时器、AIO、VNODE、PIPE。</p>

<h1>接口</h1>

<h2>kqueue</h2>

<pre><code>int kqueue(void);
</code></pre>

<p>kqueue系统调用创建一个新的内核事件队列并返回该队列描述符。调用fork时，子进程不继承该描述符。<br/>
失败时，系统调用返回-1,同时设置errno</p>

<h2>kevent</h2>

<pre><code>int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout);
</code></pre>

<p>使用kevent系统调用向队列注册事件和返回已经触发的事件。</p>

<ul>
<li>kq<br/>
kq为kqueue系统调用返回的队列描述符</li>
<li>changelist<br/>
changelist指向一个kevnet结构体数组，kevent定义在&lt;sys/event.h>头文件中，这个数组描述了需要对kq队列中的事件进行修改，包括添加、删除、修改等操作</li>
<li>nchanges<br/>
nchanges是changelist参数指向数组的大小</li>
<li>evnetlist<br/>
指向一个kevnet结构体数组，用于返回已经触发的事件</li>
<li>nevents<br/>
定义eventlist数组大小</li>
<li><p>timeout
timeout是非NULL指针时，定义系统调用超时间隔。timeout为NULL时，kevent系统调用无限阻塞等待事件发生。timeout非NULL，但是值为0时，kevent立即返回</p>

<p><!-- more --></p></li>
</ul>


<h2>EV_SET</h2>

<pre><code>EV_SET(&amp;kev, ident, filter, flags, fflags, data, udata);
struct kevent {
         uintptr_t       ident;          /* identifier for this event */
         int16_t         filter;         /* filter for event */
         uint16_t        flags;          /* general flags */
         uint32_t        fflags;         /* filter-specific flags */
         intptr_t        data;           /* filter-specific data */
         void            *udata;         /* opaque user data identifier */
 };
</code></pre>

<p>EV_SET宏用来初始化一个kevent结构体。下面详细介绍结构体各个字段</p>

<ul>
<li>ident
ident用来标识一个事件，ident值的准确含义由filter字段进行解释，ident值通常代表一个文件描述符</li>
<li><p>filter
内核filter定义的过滤器处理发生在ident上的事件。预定义的系统过滤器如下，通过kevent结构体中的flags和data字段传递参数</p>

<p>EVFILT_READ <br/>
文件描述符作为标识符，当描述符可读时返回。根据文件描述符类型不同，触发该过滤器的情况不同：</p>

<ul>
<li>sockets:<br/>
处于监听状态的socket套接字，当存在待处理的建立连接请求（已经完成三次握手）时，该套接字可读，kevent结构体的data字段包含监听队列中待处理的请求个数。<br/>
非监听状态的套接字缓冲区中有可读数据时，触发该过滤器，每个套接字对应的缓冲区可读低水位可以通过传递fflags和data参数进行设置。</li>
<li>Vnodes<br/>
当文件指针没有达到文件末尾时，触发该过滤器。</li>
<li>Fifos, Pipes<br/>
当管道可读时，触发该过滤器。data字段返回可读字节数。</li>
</ul>


<p>EVFILT_WRITE  <br/>
文件描述符作为标识符。当文件描述符可写时，触发该过滤器。对于sockets、pipes和fifos，data字段返回缓冲区中可写字节数。  <br/>
EVFILT_AIO<br/>
尚不支持<br/>
EVFILT_VNODE  <br/>
文件描述符作为标识符，fflags字段定义需要监测的事件类型：</p>

<ul>
<li>NOTE_DELETE  <br/>
unlink系统调用操作文件描述符指向的文件时，事件发生。</li>
<li>NOTE_WRITE  <br/>
对文件描述指向的文件进行写操作</li>
<li>NOTE_EXTEND<br/>
对文件描述符指向的文件进行extened</li>
<li>NOTE_ATTRIB<br/>
文件描述符指向的文件属性发生了变化</li>
<li>NOTE_LINK<br/>
文件描述指向的文件链接数发生了变化</li>
<li>NOTE_RENAME<br/>
文件描述符指向的文件发生了重命名</li>
<li>NOTE_REVOKE</li>
</ul>


<p>kevent系统调用返回时，fflags包含触发过滤器的事件</p>

<p>EVFILT_PROC
进程id作为标识符。监测的事件定义在fflags字段中：</p>

<ul>
<li>NOTE_EXIT  <br/>
进程退出</li>
<li>NOTE_EXITSTATUS<br/>
进程已经退出，退出状态</li>
<li>NOTE_FORK<br/>
进程调用fork或者其他类似接口创建子进程</li>
<li>NOTE_EXEC<br/>
进程调用execve或者其他类似接口执行一个新进程</li>
<li>NOTE_SIGNAL<br/>
进程被发送一个信号。<br/>
kevent系统调用返回时，fflags包含触发过滤器的事件。</li>
</ul>


<p>EVFILT_SIGNAL<br/>
将信号值作为标识符，当该信号发送到进程时，触发过滤器。</p>

<p>EVFILT_TIMER
设置一个ident标识的定时器。当添加一个定时器时，data字段指定定时器超时事件，fflags字段可以为以下设置：</p>

<ul>
<li>NOTE_SECONDS <br/>
data字段值单位为秒</li>
<li>NOTE_USECONDS<br/>
data字段值单位为毫米</li>
<li>NOTE_NSECONDS<br/>
data字段值单位为纳秒</li>
<li>NOTE_ABSOLUTE
data字段值为绝对时间</li>
</ul>
</li>
<li><p>flags<br/>
flags表示对该事件做的操作。</p>

<ul>
<li>EV_ADD<br/>
添加该事件到kqueue队列</li>
<li>EV_ENABLE
如果该事件触发，kevent返回该事件</li>
<li>EV_DISABLE
如果该事件触发，kevent不返回该事件</li>
<li>EV_DELETE
从kevent队列中删除该事件</li>
<li>EV_RECEIPT
对kqueue进行批量修改，而不返回任何待处理事件。</li>
<li>EV_ONESHOT
过滤器第一次触发后，返回该事件，然后将该事件从kqueue队列中删除。</li>
<li>EV_CLEAR
用户获取事件后，重置事件状态。</li>
<li>EV_ERROR
系统处理事件出错时，返回该事件event，将flags设置为EV_ERROR</li>
</ul>
</li>
<li><p>fflags<br/>
过滤器相关的标志</p></li>
<li>data<br/>
过滤器相关的数值</li>
<li>udata
用户定义的值，当事件返回时，内核将该值带给用户</li>
</ul>


<p>将<a href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/">tcp server 使用select进行并发</a>示例改为使用kevent进行i/o多路复用，主要涉及以下方面的修改：</p>

<p>1、套接字注册</p>

<pre><code>int Register(int kq, int fd)
{
    struct kevent ke;
    EV_SET(&amp;ke, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
    return kevent(kq, &amp;ke, 1, NULL, 0, NULL);
}
</code></pre>

<p> 目前只是对监听套接字和客户端连接套接字注册可读过滤器</p>

<p>2、循环调用kevent处理套接字事件</p>

<pre><code>struct kevent events[MAX_EVENT_COUNT];
for ( ; ; ) {
    int nevent = kevent(kq, NULL, 0, events, MAX_EVENT_COUNT, NULL);
}

void HandleEvent(int kq, struct kevent * events, int nevents) 
{
    for (int i = 0; i &lt; nevents; i++) {
        int sock = events[i].ident;
        int data = events[i].data;
        if (sock == serv_sock) {
            Accept(kq, data);
        } else {
            Receive(sock, data);
        }
    }
}

void Accept(int kq, int conn_size)
{
    for (int i = 0; i &lt; conn_size; i++) {
      int client = accept(serv_sock, NULL, NULL);
      Register(kq, client);
    }
}

void Receive(int sock, int buf_size)
{
    if (0 == buf_size)
        close(sock); // close a file descriptor removing any            kevents that reference the descriptor

    char *buf = malloc(buf_size);
    if (NULL == buf)
        return;

    recv(sock, buf, buf_size, 0);
    for (int i = 0; i &lt; buf_size; i++)
        buf[i] = toupper(buf[i]);
    send(sock, buf, buf_size, 0);
    free(buf);
}    
</code></pre>

<h1>结束语</h1>

<p>这篇文章只是简单介绍了kevent I/O多路复用模型的简单用法，上述例子中涉及的一些细节，如错误处理等都尚须完善<br/>
参考<a href="http://www.ibm.com/developerworks/cn/aix/library/1105_huangrg_kqueue/index.html#resources">使用 kqueue 在 FreeBSD 上开发高性能应用服务器</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp server使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-12T15:30:03+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi/">tcp服务端示例</a>中，接受一个客户端连接，调用accept函数返回后就对本客户端连接进行业务处理，如果业务逻辑比较复杂，那么服务端有可能就阻塞在某个客户端上，而不能处理其他客户端连接，解决此问题有多种模型可以采用：<br/>
1）多进程，为每个客户端连接创建一个子进程进行处理<br/>
2）多线程，多个线程并行处理客户端连接<br/>
3）I/O多路复用，服务器采用单线程模型，使用I/O多路复用模型，并行处理多个客户端文件描述符<br/>
在上一篇示例<a href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/">tcp client 使用select进行I/O复用</a>中介绍的select函数同样适用与对tcp server端的I/O多路复用改造，改造主要设计几下几个方面：</p>

<!-- more -->


<p>1）将server监听套接字添加到readfds套接字集合中，监听套接字可读时，调用accept接受一个新的客户端连接，并将添加到readfds套接字集合中</p>

<pre><code>if (FD_ISSET(sock_server, &amp;rfds)) {
    struct sockaddr_in clientaddr;
    socklen_t clientaddr_len = sizeof(clientaddr);
    int clientsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len);
    if (clientsock &lt; 0) {
        printf("accept fail. reason:%s\n", strerror(errno));
        continue;
    } else {
        printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);
        int i;
        for (i = 0; i &lt; FD_SETSIZE; i++) {
            if (clientfds[i] == -1) {
                clientfds[i] = clientsock;
                break;
            }
        }
        if (i == FD_SETSIZE) {
            printf("too many clients, ignore this one\n");
            continue;
        }
        FD_SET(clientfds[i], &amp;allfds);
        if (clientfds[i] &gt; maxfd)
            maxfd = clientfds[i];
    }
}  
</code></pre>

<p>2）当每个客户端套接字可读时，对套接字调用read操作后，继续执行select判断是否有套接字可读，而不是处理一个客户端连接直到完成。</p>

<pre><code>for (int i = 0; i &lt; FD_SETSIZE; i++) {
    if (readyfds == 0)
        break;
    if (clientfds[i] &lt; 0)
        continue;
    if (FD_ISSET(clientfds[i], &amp;rfds)) {
        readyfds--;
        char buf[LINE_MAX];
        ssize_t rcv_len;
        rcv_len = recv(clientfds[i], buf, sizeof(buf), 0);
        if (rcv_len &gt; 0) {
            buf[rcv_len] = '\0';
            printf("recv from client(%d), msg: %s\n", clientfds[i], buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
                send(clientfds[i], buf, rcv_len, 0);
                printf("send to client:%s", buf);
            } else if (rcv_len == 0) {
                printf("recv_len == 0 from client:%d\n", clientfds[i]);
                FD_CLR(clientfds[i], &amp;allfds);
                close(clientfds[i]);
                clientfds[i] = -1;
            } else {
                printf("recv from clientfd(%d) fail. reason:%s\n", clientfds[i], strerror(errno));
                continue;
            }
        }
    }
}
</code></pre>
]]></content>
  </entry>
  
</feed>
