<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[金涛的博客]]></title>
  <link href="http://jintao-zero.github.io/atom.xml" rel="self"/>
  <link href="http://jintao-zero.github.io/"/>
  <updated>2015-05-01T20:40:34+08:00</updated>
  <id>http://jintao-zero.github.io/</id>
  <author>
    <name><![CDATA[jintao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tcp Server使用kevent进行io复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong/"/>
    <updated>2015-05-01T08:40:25+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/05/01/tcp-servershi-yong-keventjin-xing-iofu-yong</id>
    <content type="html"><![CDATA[<h1>概述</h1>

<p>kqueue是FreeBSD系统中引入的可扩展事件通知接口，NetBSD，OpenBSD，DragonflyBSD和OS X这些系统也支持此接口。传统的io复用接口如select和poll存在以下两个缺点：</p>

<ul>
<li>支持的文件描述符数量有限，select接口支持的文件描述个数与进程能够打开的最大文件个数有关</li>
<li>当大量描述符需要复用时，select和poll的效率会降低，内核是采用轮询方式判断描述符是否可用</li>
</ul>


<p>kqueue是针对传统传统的select/poll处理大量的文件描述符性能较低效而开发出来的。注册一批描述符到kqueue以后，当其中的描述符状态发生变化时，kqueue将一次性通知应用程序哪些描述符可读、可写或者发生错误。</p>

<p>kqueue支持多种类型的文件描述符，包括socket、信号、定时器、AIO、VNODE、PIPE。</p>

<h1>接口</h1>

<h2>kqueue</h2>

<pre><code>int kqueue(void);
</code></pre>

<p>kqueue系统调用创建一个新的内核事件队列并返回该队列描述符。调用fork时，子进程不继承该描述符。<br/>
失败时，系统调用返回-1,同时设置errno</p>

<h2>kevent</h2>

<pre><code>int kevent(int kq, const struct kevent *changelist, int nchanges, struct kevent *eventlist, int nevents, const struct timespec *timeout);
</code></pre>

<p>使用kevent系统调用向队列注册事件和返回已经触发的事件。</p>

<ul>
<li>kq<br/>
kq为kqueue系统调用返回的队列描述符</li>
<li>changelist<br/>
changelist指向一个kevnet结构体数组，kevent定义在&lt;sys/event.h>头文件中，这个数组描述了需要对kq队列中的事件进行修改，包括添加、删除、修改等操作</li>
<li>nchanges<br/>
nchanges是changelist参数指向数组的大小</li>
<li>evnetlist<br/>
指向一个kevnet结构体数组，用于返回已经触发的事件</li>
<li>nevents<br/>
定义eventlist数组大小</li>
<li><p>timeout
timeout是非NULL指针时，定义系统调用超时间隔。timeout为NULL时，kevent系统调用无限阻塞等待事件发生。timeout非NULL，但是值为0时，kevent立即返回</p>

<p><!-- more --></p></li>
</ul>


<h2>EV_SET</h2>

<pre><code>EV_SET(&amp;kev, ident, filter, flags, fflags, data, udata);
struct kevent {
         uintptr_t       ident;          /* identifier for this event */
         int16_t         filter;         /* filter for event */
         uint16_t        flags;          /* general flags */
         uint32_t        fflags;         /* filter-specific flags */
         intptr_t        data;           /* filter-specific data */
         void            *udata;         /* opaque user data identifier */
 };
</code></pre>

<p>EV_SET宏用来初始化一个kevent结构体。下面详细介绍结构体各个字段</p>

<ul>
<li>ident
ident用来标识一个事件，ident值的准确含义由filter字段进行解释，ident值通常代表一个文件描述符</li>
<li><p>filter
内核filter定义的过滤器处理发生在ident上的事件。预定义的系统过滤器如下，通过kevent结构体中的flags和data字段传递参数</p>

<p>EVFILT_READ <br/>
文件描述符作为标识符，当描述符可读时返回。根据文件描述符类型不同，触发该过滤器的情况不同：</p>

<ul>
<li>sockets:<br/>
处于监听状态的socket套接字，当存在待处理的建立连接请求（已经完成三次握手）时，该套接字可读，kevent结构体的data字段包含监听队列中待处理的请求个数。<br/>
非监听状态的套接字缓冲区中有可读数据时，触发该过滤器，每个套接字对应的缓冲区可读低水位可以通过传递fflags和data参数进行设置。</li>
<li>Vnodes<br/>
当文件指针没有达到文件末尾时，触发该过滤器。</li>
<li>Fifos, Pipes<br/>
当管道可读时，触发该过滤器。data字段返回可读字节数。</li>
</ul>


<p>EVFILT_WRITE  <br/>
文件描述符作为标识符。当文件描述符可写时，触发该过滤器。对于sockets、pipes和fifos，data字段返回缓冲区中可写字节数。  <br/>
EVFILT_AIO<br/>
尚不支持<br/>
EVFILT_VNODE  <br/>
文件描述符作为标识符，fflags字段定义需要监测的事件类型：</p>

<ul>
<li>NOTE_DELETE  <br/>
unlink系统调用操作文件描述符指向的文件时，事件发生。</li>
<li>NOTE_WRITE  <br/>
对文件描述指向的文件进行写操作</li>
<li>NOTE_EXTEND<br/>
对文件描述符指向的文件进行extened</li>
<li>NOTE_ATTRIB<br/>
文件描述符指向的文件属性发生了变化</li>
<li>NOTE_LINK<br/>
文件描述指向的文件链接数发生了变化</li>
<li>NOTE_RENAME<br/>
文件描述符指向的文件发生了重命名</li>
<li>NOTE_REVOKE</li>
</ul>


<p>kevent系统调用返回时，fflags包含触发过滤器的事件</p>

<p>EVFILT_PROC
进程id作为标识符。监测的事件定义在fflags字段中：</p>

<ul>
<li>NOTE_EXIT  <br/>
进程退出</li>
<li>NOTE_EXITSTATUS<br/>
进程已经退出，退出状态</li>
<li>NOTE_FORK<br/>
进程调用fork或者其他类似接口创建子进程</li>
<li>NOTE_EXEC<br/>
进程调用execve或者其他类似接口执行一个新进程</li>
<li>NOTE_SIGNAL<br/>
进程被发送一个信号。<br/>
kevent系统调用返回时，fflags包含触发过滤器的事件。</li>
</ul>


<p>EVFILT_SIGNAL<br/>
将信号值作为标识符，当该信号发送到进程时，触发过滤器。</p>

<p>EVFILT_TIMER
设置一个ident标识的定时器。当添加一个定时器时，data字段指定定时器超时事件，fflags字段可以为以下设置：</p>

<ul>
<li>NOTE_SECONDS <br/>
data字段值单位为秒</li>
<li>NOTE_USECONDS<br/>
data字段值单位为毫米</li>
<li>NOTE_NSECONDS<br/>
data字段值单位为纳秒</li>
<li>NOTE_ABSOLUTE
data字段值为绝对时间</li>
</ul>
</li>
<li><p>flags<br/>
flags表示对该事件做的操作。</p>

<ul>
<li>EV_ADD<br/>
添加该事件到kqueue队列</li>
<li>EV_ENABLE
如果该事件触发，kevent返回该事件</li>
<li>EV_DISABLE
如果该事件触发，kevent不返回该事件</li>
<li>EV_DELETE
从kevent队列中删除该事件</li>
<li>EV_RECEIPT
对kqueue进行批量修改，而不返回任何待处理事件。</li>
<li>EV_ONESHOT
过滤器第一次触发后，返回该事件，然后将该事件从kqueue队列中删除。</li>
<li>EV_CLEAR
用户获取事件后，重置事件状态。</li>
<li>EV_ERROR
系统处理事件出错时，返回该事件event，将flags设置为EV_ERROR</li>
</ul>
</li>
<li><p>fflags<br/>
过滤器相关的标志</p></li>
<li>data<br/>
过滤器相关的数值</li>
<li>udata
用户定义的值，当事件返回时，内核将该值带给用户</li>
</ul>


<p>将<a href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/">tcp server 使用select进行并发</a>示例改为使用kevent进行i/o多路复用，主要涉及以下方面的修改：</p>

<p>1、套接字注册</p>

<pre><code>int Register(int kq, int fd)
{
    struct kevent ke;
    EV_SET(&amp;ke, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
    return kevent(kq, &amp;ke, 1, NULL, 0, NULL);
}
</code></pre>

<p> 目前只是对监听套接字和客户端连接套接字注册可读过滤器</p>

<p>2、循环调用kevent处理套接字事件</p>

<pre><code>struct kevent events[MAX_EVENT_COUNT];
for ( ; ; ) {
    int nevent = kevent(kq, NULL, 0, events, MAX_EVENT_COUNT, NULL);
}

void HandleEvent(int kq, struct kevent * events, int nevents) 
{
    for (int i = 0; i &lt; nevents; i++) {
        int sock = events[i].ident;
        int data = events[i].data;
        if (sock == serv_sock) {
            Accept(kq, data);
        } else {
            Receive(sock, data);
        }
    }
}

void Accept(int kq, int conn_size)
{
    for (int i = 0; i &lt; conn_size; i++) {
      int client = accept(serv_sock, NULL, NULL);
      Register(kq, client);
    }
}

void Receive(int sock, int buf_size)
{
    if (0 == buf_size)
        close(sock); // close a file descriptor removing any            kevents that reference the descriptor

    char *buf = malloc(buf_size);
    if (NULL == buf)
        return;

    recv(sock, buf, buf_size, 0);
    for (int i = 0; i &lt; buf_size; i++)
        buf[i] = toupper(buf[i]);
    send(sock, buf, buf_size, 0);
    free(buf);
}    
</code></pre>

<h1>结束语</h1>

<p>这篇文章只是简单介绍了kevent I/O多路复用模型的简单用法，上述例子中涉及的一些细节，如错误处理等都尚须完善<br/>
参考<a href="http://www.ibm.com/developerworks/cn/aix/library/1105_huangrg_kqueue/index.html#resources">使用 kqueue 在 FreeBSD 上开发高性能应用服务器</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp server使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-12T15:30:03+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/12/tcp-servershi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi/">tcp服务端示例</a>中，接受一个客户端连接，调用accept函数返回后就对本客户端连接进行业务处理，如果业务逻辑比较复杂，那么服务端有可能就阻塞在某个客户端上，而不能处理其他客户端连接，解决此问题有多种模型可以采用：<br/>
1）多进程，为每个客户端连接创建一个子进程进行处理<br/>
2）多线程，多个线程并行处理客户端连接<br/>
3）I/O多路复用，服务器采用单线程模型，使用I/O多路复用模型，并行处理多个客户端文件描述符<br/>
在上一篇示例<a href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/">tcp client 使用select进行I/O复用</a>中介绍的select函数同样适用与对tcp server端的I/O多路复用改造，改造主要设计几下几个方面：</p>

<!-- more -->


<p>1）将server监听套接字添加到readfds套接字集合中，监听套接字可读时，调用accept接受一个新的客户端连接，并将添加到readfds套接字集合中</p>

<pre><code>if (FD_ISSET(sock_server, &amp;rfds)) {
    struct sockaddr_in clientaddr;
    socklen_t clientaddr_len = sizeof(clientaddr);
    int clientsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len);
    if (clientsock &lt; 0) {
        printf("accept fail. reason:%s\n", strerror(errno));
        continue;
    } else {
        printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);
        int i;
        for (i = 0; i &lt; FD_SETSIZE; i++) {
            if (clientfds[i] == -1) {
                clientfds[i] = clientsock;
                break;
            }
        }
        if (i == FD_SETSIZE) {
            printf("too many clients, ignore this one\n");
            continue;
        }
        FD_SET(clientfds[i], &amp;allfds);
        if (clientfds[i] &gt; maxfd)
            maxfd = clientfds[i];
    }
}  
</code></pre>

<p>2）当每个客户端套接字可读时，对套接字调用read操作后，继续执行select判断是否有套接字可读，而不是处理一个客户端连接直到完成。</p>

<pre><code>for (int i = 0; i &lt; FD_SETSIZE; i++) {
    if (readyfds == 0)
        break;
    if (clientfds[i] &lt; 0)
        continue;
    if (FD_ISSET(clientfds[i], &amp;rfds)) {
        readyfds--;
        char buf[LINE_MAX];
        ssize_t rcv_len;
        rcv_len = recv(clientfds[i], buf, sizeof(buf), 0);
        if (rcv_len &gt; 0) {
            buf[rcv_len] = '\0';
            printf("recv from client(%d), msg: %s\n", clientfds[i], buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
                send(clientfds[i], buf, rcv_len, 0);
                printf("send to client:%s", buf);
            } else if (rcv_len == 0) {
                printf("recv_len == 0 from client:%d\n", clientfds[i]);
                FD_CLR(clientfds[i], &amp;allfds);
                close(clientfds[i]);
                clientfds[i] = -1;
            } else {
                printf("recv from clientfd(%d) fail. reason:%s\n", clientfds[i], strerror(errno));
                continue;
            }
        }
    }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp Client 使用select进行I/O复用]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong/"/>
    <updated>2015-04-06T11:49:13+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/06/tcp-client-shi-yong-selectjin-xing-i-slash-ofu-yong</id>
    <content type="html"><![CDATA[<p>在<a href="http://jintao-zero.github.io/blog/2015/02/10/tcp-ke-hu-duan-shi-li/">tcp客户端示例</a>中，存在以下两种情况时，示例代码不能更好的处理：</p>

<ul>
<li><p>目前客户端输入一条数据并发送，等待服务端返回后，才能继续输入数据
 这样的设计不利于客户端批量处理数据</p></li>
<li><p>当服务端出现异常（如服务进程退出或者服务端主机崩溃等异常情况），客户端也许正在等待输入，不能够及时感知到服务端的异常，只有从send返回错误时，才能感知到服务端故障</p></li>
</ul>


<h1>select 函数介绍</h1>

<p>上面实现的客户端示例中，客户端只能处理一个描述符，我们需要修改客户端使其可以同时处理标准输入和套接字描述符，达到I/O复用：</p>

<pre><code>#include &lt;sys/select.h&gt;
#include &lt;sys/time.h&gt;
int select(int nfds, fd_set *restrict readfds, fd_set *restrict writefds, fd_set *restrict errorfds,
     struct timeval *restrict timeout);
</code></pre>

<ul>
<li><p>timeout参数，它告知内核等待所有描述符中的一个就绪可花多长时间<br/>
  struct timeval {<br/>
      long tv_sec;  /* seconds*/<br/>
      long tv_usec; /* microseconds*/<br/>
  }   <br/>
  (1) 永远等待下去：仅在有一个描述符准备好时返回。为此，将timeout设置为   空指针<br/>
  (2) 等待一段固定时间：在有一个描述符准备好I/O时返回，但是不超过由该参数所指向的timeval结构中指定的秒数和微秒数<br/>
  (3) 根本不等待：检查描述符后立即返回，这称为轮询。为此，该参数必须指向一个timeval结构，而且其中的定时器值必须为0</p></li>
<li><p>readfds, writefds, errorfds参数，这三个参数指定了，需要内核测试读、写     和异常条件的描述符集合。
系统提供了以下宏来对描述符进行操作：<br/>
void FD_CLR(fd, fd_set <em>fdset)<br/>
void FD_COPY(fd_set </em>fdest_orig, fd_set <em>fdset_copy)<br/>
int FD_ISSET(fd, fd_set </em>fdset)<br/>
void FD_SET(fd, fd_set <em>fdset)<br/>
void FD_ZERO(fd_set </em>fdset)</p></li>
<li><p>nfds参数，指定待测试的描述符个数，它的值是待测试的最大描述符加1</p></li>
</ul>


<!-- more -->


<h1>描述符就绪条件</h1>

<ul>
<li><p>满足下列四个条件中的任何一个时，一个套接字准备好读<br/>
a) 该套接字接收缓冲区中的数据字节数大于等于套接字接收缓冲区低水位标记的当前大小。对这样的套接字执行读操作不会阻塞并将返回一个大于0的值（也就是返回准备好读入的数据）。对于TCP和UDP套接字，其默认值为1。<br/>
b) 该连接的读半部关闭（也就是接收了FIN的TCP连接）。对这样的套接字的读操作将不阻塞并返回0（也就是返回EOF）<br/>
c) 该套接字是一个监听套接字且已完成的连接数大于0。对于这样的套接字调用accept通常不会阻塞。  <br/>
d) 其上有一个套接字错误待处理。对这样的套接字的读操作将不阻塞并返回－1（也就是返回一个错误），同时把errno设置成确切的错误条件。</p></li>
<li><p>下列四个条件中的任何一个满足时，一个套接字准备好写。<br/>
a) 该套接字发送缓冲区中的可用空间字节数大于等于套接字发送缓冲区低水位表姐的当前大小，并且或者该套接字已连接，或者该套接字不需要连接（如UDP套接字）。  <br/>
b) 该连接的写半部关闭。对这样的套接字的写操作将产生SIGPIPE信号。<br/>
c) 使用非阻塞式connect的套接字已建立连接，或者connect已经以失败告终。<br/>
d) 其上有一个套接字错误待处理。</p></li>
<li>如果一个套接字存在带外数据或者仍处于带外标记，那么它有异常条件待处理。</li>
</ul>


<h1>修改tcp客户端程序</h1>

<ul>
<li><p>批量输入<br/>
a) 用户输入数据，标准输入可读，read返回读入字节数，发送到服务器<br/>
b) 用户输入Ctrl-D时，标准输入可读，read返回字节数为0，此时关闭套接字写端</p></li>
<li><p>套接字增加处理情况：
a) 如果对端TCP发送数据，那么该套接字变为可读，并且read返回一个大于0的值（即读入数据的字节数）。<br/>
b) 如果对端TCP发送一个FIN（对端进程终止），那么该套接字变为可读，并且read返回0（EOF）。<br/>
c) 如果对端TCP发送一个RST（对端主机崩溃并重新启动），那么该套接字变为可读，并且read返回－1，而errno中含有确切的错误码。</p></li>
</ul>


<p>以下是相关修改：<br/>
1、将标准输入、套接字添加到客服描述符集合</p>

<pre><code>fd_set readfds;
FD_ZERO(&amp;readfds);
FD_SET(sock_client, &amp;readfds);
FD_SET(STDIN_FILENO, &amp;readfds);
int maxfd = (STDIN_FILENO &gt; sock_client ? STDIN_FILENO: sock_client) + 1;
select(maxfd, &amp;readfds, NULL, NULL, NULL)
</code></pre>

<p>2、标准输入可读</p>

<pre><code>if (FD_ISSET(STDIN_FILENO , &amp;readfds)) {
    ssize_t rd_size = read(STDIN_FILENO , buf, sizeof(buf));
    if (rd_size &gt; 0) {
        ssize_t sd_size = send(sock_client, buf, rd_size, 0);
        buf[rd_size] = '\0';
        printf("send msg(len:%ld):%s to server \n", sd_size, buf);
    } else if (rd_size == 0) {
    // client finish send msg to server, we close write part of the full-duplex connection
        shutdown(sock_client, SHUT_WR); // send FIN to server
        stdineof = 1;
        continue;
    } else {
    // client finish send msg to server, we close write part of the full-duplex connection
        printf("read fail from console, reason:%s \n", strerror(errno));
        break;
    }
}  
</code></pre>

<p>3、套接字可读</p>

<pre><code>if (FD_ISSET(sock_client, &amp;readfds)) {
    ssize_t rd_size = read(sock_client, buf, sizeof(buf));
    if (rd_size &gt; 0) {
        printf("receive from server:%s", buf);
    } else if (rd_size == 0) { // client receive FIN
            printf("receive 0 from server\n");
            if (stdineof == 1) {
                printf("normal finish\n");
            } else {// server crash and reset, client receive RST
                printf("server terminate prematurely\n");
            }
            break;
        } else {
            printf("read fail.reason: %s\n", strerror(errno));
            break;
        }
    }
}
</code></pre>

<p>下面是使用select进行I/O复用的tcp客户端代码：</p>

<pre><code>int main(int argc, char *argv[])
{
    // judge the legal arguments
    if (2 &gt; argc) {
        printf("please input a hostname \n");
        return -1;
    }

// create a socket
    int sock_client = socket(AF_INET, SOCK_STREAM, 0);
    if (0 &gt; sock_client) {
        perror("create socket error");
        return -1;
    }


    // construct server sockaddr
        struct hostent *p = gethostbyname(argv[1]);
        if (p == NULL) {
            printf("gethostbyname:%s fail.reason:%s \n", argv[1], strerror(errno));
            return -1;
        }
        struct sockaddr_in servaddr;
        bzero(&amp;servaddr, sizeof(servaddr));
        servaddr.sin_family = AF_INET;
        servaddr.sin_addr = *(struct in_addr *)p-&gt;h_addr;
        servaddr.sin_port = htons(SERVER_PORT);

        // connect to server
        if (connect(sock_client, (struct sockaddr *)&amp;servaddr, sizeof(servaddr)) == -1) {
            printf("connect to server:%s port:%d fail.reason:%s \n", inet_ntoa(servaddr.sin_addr), SERVER_PORT,
                strerror(errno));
            return -1;
        }
        printf("connect to server:%s port:%d success\n", inet_ntoa(servaddr.sin_addr), SERVER_PORT );


        int stdineof = 0;
        for (; ; ) {
            fd_set readfds;
            FD_ZERO(&amp;readfds);
            FD_SET(sock_client, &amp;readfds);
            if (stdineof == 0)
                FD_SET(STDIN_FILENO, &amp;readfds);
            int maxfd = (STDIN_FILENO &gt; sock_client ? STDIN_FILENO: sock_client) + 1;
            if (select(maxfd, &amp;readfds, NULL, NULL, NULL) &gt; 0) {
                char buf[LINE_MAX]={0};
                if (FD_ISSET(STDIN_FILENO , &amp;readfds)) {
                    ssize_t rd_size = read(STDIN_FILENO , buf, sizeof(buf));
                    if (rd_size &gt; 0) {
                        ssize_t sd_size = send(sock_client, buf, rd_size, 0);
                        buf[rd_size] = '\0';
                        printf("send msg(len:%ld):%s to server \n", sd_size, buf);
                    } else if (rd_size == 0) {
                        // client finish send msg to server, we close write part of the full-duplex connection
                        shutdown(sock_client, SHUT_WR); // send FIN to server
                        stdineof = 1;
                        continue;
                    } else {
                        // client finish send msg to server, we close write part of the full-duplex connection
                        printf("read fail from console, reason:%s \n", strerror(errno));
                        break;
                    }
                } else if (FD_ISSET(sock_client, &amp;readfds)) {
                    ssize_t rd_size = read(sock_client, buf, sizeof(buf));
                    if (rd_size &gt; 0) {
                        printf("receive from server:%s", buf);
                    } else if (rd_size == 0) { // client receive FIN
                        printf("receive 0 from server\n");
                        if (stdineof == 1) {
                            printf("normal finish\n");
                        } else {// server crash and reset, client receive RST
                            printf("server terminate prematurely\n");
                        }
                        break;
                    } else {
                        printf("read fail.reason: %s\n", strerror(errno));
                        break;
                    }
                }
            }
        }
        close(sock_client);
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Property 属性用法]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/04/05/python-property-shu-xing-yong-fa/"/>
    <updated>2015-04-05T17:26:05+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/04/05/python-property-shu-xing-yong-fa</id>
    <content type="html"><![CDATA[<p>python提供了一个property类：</p>

<pre><code>class property([fget[, fset[, fdel[, doc]]]])
</code></pre>

<p>property类为新式类（继承object）返回property属性<br/>
fget函数用来获取属性值。fset设置属性值。fdel用来删除属性。doc为属性创建docstring。<br/>
property的典型应用是应用一个被管理的属性x：</p>

<pre><code>class C(object):
def __init__(self):
    self._x = None

def getx(self):
    return self._x

def setx(self, value):
    self._x = value

def delx(self):
    del self._x

x = property(getx, setx, delx, "I'm the 'x' property.")
</code></pre>

<p>如果c是类C的实例，c.x将会调用getx，c.x＝value将会调用setx，del c.x会调用用delx</p>

<p>将property 用作decorrator可以定义只读properties：</p>

<pre><code>class Parrot(object):
    def __init__(self):
        self._voltage = 100000

    @property
    def voltage(self):
        """Get the current voltage."""
        return self._voltage
</code></pre>

<p>@property修饰符将voltage函数变为voltage函数的同名属性的只读getter函数，同时这只这个属性的docstring与voltage函数相同</p>

<p>一个property对象具有getter，setter和deleter方法可以用当作修饰符将属性同名函数变为属性对应的访问函数，下面是例子：</p>

<pre><code>class C(object):
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        return self._x

    @x.setter
    def x(self, value):
        self._x = value

    @x.deleter
    def x(self):
        del self._x
</code></pre>

<p>上面的用法与第一个列子相同，需要注意的是几个函数的名字都相同</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp服务端并发之子进程]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/02/17/tcpfu-wu-duan-bing-fa-zhi-zi-jin-cheng/"/>
    <updated>2015-02-17T14:56:56+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/02/17/tcpfu-wu-duan-bing-fa-zhi-zi-jin-cheng</id>
    <content type="html"><![CDATA[<p>上一篇博客中实现的tcp服务端每次只能处理一个客户端连接，如果有多个客户端同时与服务端进行通讯的话，那只能进行等待上一个客户端通讯完成才可以，那么如何修改服务端为并发服务器程序呢？Unix中编写并发服务器程序最简单的方法就是fork一个子进程来服务每个客户。以下为一个典型的并发服务器的轮廓：</p>

<pre><code>pid_t pid;
int listenfd, connfd;
listenfd = Socket(...);
    /* fill in sockaddr_in{} with server's well-known port */
Bind*(listenfd, ...);
Listen(listenfd, LISTENQ);
for ( ; ; ) {
    connfd = Accept(listenfd, ...);  /* probably blocks */
    if ((pid = Fork()) == 0) {
        Close(listenfd);    /* child close listening socket */
        doit(connfd);       /* process the request */
        close(connfd);      /* child terminates */
        exit(0);            /* child terminate */
    }
    Close(connfd);          /* parent closes connected socket */
}
</code></pre>

<p>fork函数</p>

<pre><code>pid_t fork(void);
</code></pre>

<p>fork创建一个新进程。新进程是当前运行进程的一个拷贝。fork函数一次调用，两次返回。对于子进程返回值为0，对于父进程，返回值为子进程id。</p>

<p>以下为根据上面的并发服务器轮廓代码修改的tcp客户端代码：</p>

<!-- more -->


<pre><code>for (; ; ) {

    if ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) &lt; 0) {
        if (errno == EINTR) {
            continue;
        }
        else {
            printf("accept fail. reason:%s \n", strerror(errno));
            close(sock_server);
            exit(0);
        }
    }
    printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr), clientaddr.sin_port);

    int child = fork();
    if (child == 0) { // child process

        close(sock_server);
        printf("child process:%d \n", getpid());
        // recv data from client
        char buf[1024];
        ssize_t rcv_len;
        while ((rcv_len = recv(connsock, buf, sizeof(buf), 0)) &gt; 0) {
            printf("recv from client:%s\n", buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
            send(connsock, buf, rcv_len, 0);
            printf("send to  client:%s\n", buf);
        }
        printf("len:%ld \n", rcv_len);
        close(connsock);
        exit(0);
    }

    // parent process
    close(connsock);
}
</code></pre>

<p>以上tcp服务端程序可以并发处理多个客户端请求，但是在通讯结束，子进程调用exit函数退出后，会产生僵死进程, 使用ps命令可以看到多个server进程状态为Z+：<br/>
<img src="http://jintao-zero.github.io/images/zombie.png">
设置僵死状态的目的是维护子进程的信息，以便父进程在以后某个时候获取。这些信息包括子进程的进程ID、终止状态以及资源利用信息（CPU时间、内存使用量等等）。如果一个进程终止，而该进程有子进程处于僵死状态，那么它的所有僵死子进程的富进程ID将被重置为1（init进程）。继承这些子进程的init进程将清理它们。<br/>
僵死进程占用内核中的空间，最终可能导致我们耗尽进程资源，因此我们需要及时的清理僵死子进程，无论何时我们fork子进程都得wait它们，以防它们变成僵死进程。每个子进程在退出时，内核都会向父进程发送SIGCHLD信号，因此父进程需要建立一个俘获SIGCHLD信号的信号处理函数，在函数体中调用wait。</p>

<p>wait函数</p>

<pre><code>pid_t wait(int *stat_loc);
</code></pre>

<p>wait函数阻塞调用进程直到获取到一个退出子进程的信息，成功返回后，stat_loc返回退出子进程的退出信息。SIGCHILD信号处理函数为：</p>

<pre><code>void chld_sig_handler(int signo)
{
    int stat_loc;
    wait(&amp;stat_loc);
    return ;
}
</code></pre>

<p>sigaction函数</p>

<pre><code>struct  sigaction {
         union __sigaction_u __sigaction_u;  /* signal handler */
         sigset_t sa_mask;               /* signal mask to apply */
         int     sa_flags;               /* see signal options below */
 };

 union __sigaction_u {
         void    (*__sa_handler)(int);
         void    (*__sa_sigaction)(int, struct __siginfo *,
                        void *);
 };

#define sa_handler      __sigaction_u.__sa_handler
#define sa_sigaction    __sigaction_u.__sa_sigaction

int sigaction(int sig, const struct sigaction *restrict act, struct sigaction *restrict oact);
</code></pre>

<p>sigaction函数用来修改进程对于某个信号的处理函数
参数sig为需要修改处理函数的信号id
参数act说明了该信号处理函数，以及进入处理函数时的信号掩码</p>

<pre><code>// register SIGCHLD process handler
struct sigaction act;
struct sigaction oact;
act.sa_handler = chld_sig_handler;
sigemptyset(&amp;act.sa_mask);
act.sa_flags = 0;
if (sigaction(SIGCHLD, &amp;act, &amp;oact) &lt; 0) {
    printf("set SIGCHLD handler fail. reason:%s \n", strerror(errno));
    close(sock_server);
    return -1;
}
</code></pre>

<p>在tcp服务代码中添加了SIGCHLD信号处理函数后，重新测试，但是发现仍然会出现僵死子进程：<br/>
<img src="http://jintao-zero.github.io/images/zombie_wait.png"><br/>
Unix信号处理机制中，<br/>
1、当一个信号达到后，进入该信号的处理函数后，进程将屏蔽该信号 <br/>
2、多个同样类型信号同时到达时，信号处理函数只执行一次，其他的信号不进行排队</p>

<p>这样当多个客户端连接同时结束时，会导致服务端子进程同时退出，多个SIGCHLD信号同时产生递送到服务端父进程，那么一种可能的情况是，在信号处理函数被调用之前，信号都到达，那么只有一个信号被处理，其他信号不会排队，将会被丢弃，这样仍然产生僵死子进程。<br/>
为了解决这样问题，可以在信号处理函数中多次执行wait操作，获取已经结束子进程的退出信息。但是上面的wait接口是阻塞等待、直到有新退出进程，这样的话就会影响进程的正常执行，下面介绍另外一个</p>

<pre><code>pid_t waitpid(pid_t pid, int *stat_loc, int options);
</code></pre>

<p>参数pid指定需要等待的进程集。pid为-1时，等待任何子进程。pid为0时，等待调用者进程组里面的任何子进程。pid大于0时，等待进程号为pid的子进程。
参数stat_loc用来保存进程退出状态。
参数options,为WNOHANG、WUNTRACED选项的或。指定WNOHANG操作时，如果尚没有退出的子进程，你们waitp函数不阻塞等待，会返回0。指定WUNTRACED选项时，如果子进程是由于SIGTTIN,SIGTTOU,SIGTSTP,SIGSTOP信号而处于stopped状态，那么wait操作也会获取到这些进程的状态信息。<br/>
需要修改SIGCHLD信号处理函数，使用waitp函数，设置options位WNOHANG来读取所有已经退出子进程的进程状态，防止变为僵尸进程。</p>

<pre><code>void chld_sig_handler(int signo)
{
    int stat_loc;
    int options = WNOHANG;
    int pid;
    while ((pid = waitpid(-1, &amp;stat_loc, options)) &gt; 0) {
        printf("wait pid:%d suc\n", pid);   // normally we should call i/o func in signal handler
    }
return ;
}
</code></pre>

<p>对于服务器主进程而言，大部分的时间都是阻塞在accept系统调用上，这一类的系统调用也称为慢系统调用（slow system call），适用与慢系统调用的一个规则是：当阻塞与某个慢系统调用的一个进程捕获某个信号且相应信号处理函数返回时，该系统调用可能返回一个EINTR错误。有些内核自动重启某些被中断的系统调用。为了便于移植，当我们编写捕获信号的程序时（多数并发服务器捕获SIGCHLD），我们必须对慢系统调用返回EINTR有所准备。修改服务器代码为：</p>

<pre><code>if ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) &lt; 0) {
    if (errno == EINTR) {
        continue;
    } else {
        printf("accept fail. reason:%s \n", strerror(errno));
        close(sock_server);
        exit(0);
    }
}
</code></pre>

<p>本片文章的目的时总结我们在网络编程时可能会遇到的三种情况：
(1) 当fork子进程时，必须捕获SIGCHLD信号
(2) 当捕获信号时，必须处理被中断的系统调用
(3) SIGCHLD的信号处理函数必须正确编写，应适用waitpid函数以免留下僵尸进程</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp 服务端例子]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi/"/>
    <updated>2015-02-13T11:04:43+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/02/13/tcp-fu-wu-duan-li-zi</id>
    <content type="html"><![CDATA[<p>实现了一个简单服务端，使用tcp与客户端进行通讯。本片文章主要介绍在tcp服务端网络编程中用到的几个基本函数：<br/>
1、创建套接字socket函数</p>

<pre><code>int socket(int domain, int type, int protocol)
</code></pre>

<p>socket函数创建一个通讯端并返回这个终端的文件描述符。</p>

<p>参数domain指定发生整个通讯过程所在的协议域。协议域定义在&lt;sys/socket.h>头文件中，常用协议域名为：</p>

<pre><code>PF_LOCAL 主机内部通讯协议，之前名为PF_UNIX
PF_UNIX  主机内部通讯协议，废弃，使用PF_LOCAL
PF_INET  IPV4通讯协议
PF_INET6 IPV6通讯协议
。。。
</code></pre>

<p>本例子中使用PF_INET ipv4通讯协议域</p>

<p>参数type指定socket类型，目前定义类型为：</p>

<pre><code>SOCK_STREAM                                      
SOCK_DGRAM
SOCK_RAW
SOCK_SEQPACKET
SOCK_RDM
</code></pre>

<p>SOCK_STREAM 类型提供序列化、可靠的、全双工字节流套接字
SOCK_DGRAM 类型提供无连接的、不可靠的，有报文最大限制的数据报通讯套接字
SOCK_RAW 类型提供读取内部网络协议和网卡接口的原始套接字，需要有超级用户权限<br/>
本例子中使用SOCK_STREAM流套接字类型</p>

<p>失败时，函数返回-1，成功时，返回套接字文件描述符</p>

<p>2、bind函数</p>

<pre><code>int bind(int socket, const struct sockaddr *address, socklen_t address_len);
</code></pre>

<p>bind函数把一个本地协议地址赋予一个套接字。
参数socket，为需要赋予地址的套接字。
参数address，为赋予到套接字的地址结构，有ip和端口号标识。
参数address_len, 为该地址结构大小</p>

<p>对于服务端来讲，在调用监听函数之前，都需要bind到特定地址和端口</p>

<!-- more -->


<p>3、listen函数</p>

<pre><code>int listen(int socket, int backlog);
</code></pre>

<p>listen函数仅由TCP服务器调用，它做两件事情：<br/>
(1) 当socket函数创建一个套接字时，它被假设为一个主动套接字，也就是说，它是一个将调用connect发起连接的客户套接字。   listen函数把一个未连接的套接字转换成一个被动套接字，指示内核应接受指向该套接字的连接请求。根据tcp状态转换图，调用listen导致套接字从CLOSED状态转换到LISTEN状态。<br/>
(2) 本函数的第二个参数规定了内核应该为相应套接字排队的最大连接个数。
本函数通常应该在调用socket和bind这两个函数之后，并在调用accept函数之前调用。
内核为任何一个给定的监听套接字维护两个队列：<br/>
(1)未完成连接队列，每个这样的SYN分节对应其中一项：已由某个客户发出并到达服务器，而服务器正在等待完成相应的TCP三路握手过程。这些套接字处于SYN_RCVD状态<br/>
(2)已完成连接队列，每个已完成TCP三路握手的客户对应其中一项。这些套接字处于ESTABLISHED状态。
参数backlog规定了以上两个队列的总和大小</p>

<p>4、accept函数</p>

<pre><code>int accept(int socket, struct sockaddr *restrict address, socklen_t *restrict address_len);
</code></pre>

<p>accept函数由TCP服务器调用，用于从已完成连接队列对头返回下一个已完成连接。如果已完成连接队列未空，那么进程被投入睡眠（假定套接字为默认的阻塞方式）。
参数address和address_len返回已连接的对端进程的协议地址。
如果accept成功，那么其返回值是由内核自动生产的一个全新描述符，代表与所返回客户的TCP连接。</p>

<p>5、下面是服务端源代码
此服务端小程序主要完成功能：1、监听服务端口，接收连接 2、接收客户端数据并处理后发回服务端</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;sys/errno.h&gt;
#include &lt;string.h&gt;
#include &lt;netinet/in.h&gt;
#include &lt;arpa/inet.h&gt;
#include &lt;ctype.h&gt;

#define SERVER_PORT 8000

int main()
{
    // create socket
    int sock_server = socket(AF_INET, SOCK_STREAM, 0);
    if (-1 == sock_server) {
        printf("create socket fail.reason:%s\n",                strerror(errno));
        return -1;
    }

    // bind socket to local addr and port
    struct sockaddr_in servaddr;
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr  = INADDR_ANY;
    servaddr.sin_port = htons(SERVER_PORT);
    if (-1 == bind(sock_server, (struct sockaddr *)&amp;servaddr, (socklen_t)sizeof(servaddr))) {
        printf("bind to local addr fail.%s \n", strerror(errno));
        return -1;
    }
    printf("bind to local addr success\n");

    // make socket to listen
    if (-1 == listen(sock_server, 128)) {
        printf("listen fail.%s\n", strerror(errno));
        return -1;
    }

    struct sockaddr_in clientaddr;
    socklen_t clientaddr_len = sizeof(clientaddr);
    int connsock;

    // accept a new client connection
    while ((connsock = accept(sock_server, (struct sockaddr *)&amp;clientaddr, &amp;clientaddr_len)) != -1) {

        printf("new client connected, %s:%d \n", inet_ntoa(clientaddr.sin_addr),            clientaddr.sin_port);

        // recv data from client
        char buf[1024];
        ssize_t rcv_len;
        while ((rcv_len = recv(connsock, buf, sizeof(buf), 0)) &gt; 0) {
            printf("recv from client:%s\n", buf);
            for (int i = 0; i &lt; rcv_len; i++)
                buf[i] = toupper(buf[i]);
            send(connsock, buf, rcv_len, 0);
            printf("send to  client:%s\n", buf);
        }
        printf("len:%ld \n", rcv_len);
        close(connsock);
    }
    close(sock_server);
}   
</code></pre>

<p>运行效果：
<img src="http://jintao-zero.github.io/images/server.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tcp 客户端示例]]></title>
    <link href="http://jintao-zero.github.io/blog/2015/02/10/tcp-ke-hu-duan-shi-li/"/>
    <updated>2015-02-10T19:07:27+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2015/02/10/tcp-ke-hu-duan-shi-li</id>
    <content type="html"><![CDATA[<p>实现了一个简单客户端，使用tcp与服务端进行通讯。本片文章主要介绍在tcp客户端网络编程中用到的几个基本函数：<br/>
1、创建socket</p>

<pre><code>int socket(int domain, int type, int protocol)
</code></pre>

<p>socket函数创建一个通讯端并返回这个终端的文件描述符。</p>

<p>参数domain指定发生整个通讯过程所在的协议域。协议域定义在&lt;sys/socket.h>头文件中，常用协议域名为：</p>

<pre><code>PF_LOCAL 主机内部通讯协议，之前名为PF_UNIX
PF_UNIX  主机内部通讯协议，废弃，使用PF_LOCAL
PF_INET  IPV4通讯协议
PF_INET6 IPV6通讯协议
。。。
</code></pre>

<p>本例子中使用PF_INET ipv4通讯协议域</p>

<p>参数type指定socket类型，目前定义类型为：</p>

<pre><code>SOCK_STREAM                                      
SOCK_DGRAM
SOCK_RAW
SOCK_SEQPACKET
SOCK_RDM
</code></pre>

<p>SOCK_STREAM 类型提供序列化、可靠的、全双工字节流套接字
SOCK_DGRAM 类型提供无连接的、不可靠的，有报文最大限制的数据报通讯套接字
SOCK_RAW 类型提供读取内部网络协议和网卡接口的原始套接字，需要有超级用户权限<br/>
本例子中使用SOCK_STREAM流套接字类型</p>

<p>失败时，函数返回－1，成功时，返回套接字文件描述符</p>

<p>2、connect连接服务端</p>

<pre><code>int connect(int socket, const struct sockaddr *address, socklen_t address_len);
</code></pre>

<p>connect函数在一个socket上面初始化一个连接</p>

<p>参数socket为要建立连接的套接字<br/>
参数address为要对端连接的地址，不同协议域会有不同的方式解析这个参数内容
参数address_len为传入的address地址所对应地址结构体占据的内存大小</p>

<p>连接成功，函数返回0，失败，则返回－1，同时置错误码errno</p>

<!-- more -->


<p></p>

<p>3、send发送内容</p>

<pre><code>ssize_t send(int socket, const void *buffer, size_t length, int flags);
</code></pre>

<p>send函数发送数据到对端套接字。使用send函数时需要socket处于连接状态。</p>

<p>参数buffer为需要发送内容首地址<br/>
参数length为需要发送的报文内容长度<br/>
参数flags可能包含以下两种：</p>

<pre><code>    #define MSG_OOB        0x1  /* process out-of-band data */
    #define MSG_DONTROUTE  0x4  /* bypass routing, use direct interface */
</code></pre>

<p>本例中，不需要以上两种标志，设置为0即可</p>

<p>发送成功，函数返回发送的字节数，发送失败，返回－1，设置errno错误码</p>

<p>4、recv接受内容</p>

<pre><code>ssize_t recv(int socket, void *buffer, size_t length, int flags);
</code></pre>

<p>recv函数只可以在处于连接状态套接字使用。
默认情况下，如果没有数据可读，recv调用将会一直阻塞等待数据到达，除非socket被设置为非阻塞状态。通常情况下，recv会返回任何可读数据，最大达到请求的length数量，而不是一直等待接收到length数量才返回，可以通过设置套接字属性SO_RCVLOWAT和SO_RCVTIMEO来进行修改。
如果没有可接收数据并且对端已经执行关闭操作，那么recv返回0</p>

<p>参数flags，有以下几种类型：</p>

<pre><code>MSG_OOB        process out-of-band data
MSG_PEEK       peek at incoming message
MSG_WAITALL    wait for full request or error
</code></pre>

<p>MSG_OOB标志请求接收带外数据。<br/>
MSG_PEEK标志使recv只从头读取队列中的数据而不从队列中删除这些数据，这样接下来的读操作会读取相同内容。
MSG_WAITALL标志使recv操作阻塞直到读取的报文达到请求字节数。</p>

<p>recv函数返回接收到的字节数，返回－1表示发生错误，返回0表示对端已经关闭连接。</p>

<p>5、示例代码
示例完成功能：1、与服务端建立tcp连接 2、从控制台读取输入，发送到服务端 3、读取服务端返回数据 4、关闭tcp连接</p>

<pre><code>#include &lt;sys/socket.h&gt;
#include &lt;netdb.h&gt;
#include &lt;arpa/inet.h&gt;
#include &lt;sys/errno.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;

#define SERVER_PORT 8000

int main(int argc, char *argv[])
{
    // judge the legal arguments
    if (2 &gt; argc) {
        printf("please input a hostname \n");
        return -1;
    }

    // create a SOC_STREAM socket
    int sock_client = socket(AF_INET, SOCK_STREAM, 0);
    if (0 &gt; sock_client) {
        perror("create socket error");
        return -1;
    }


    // construct server sockaddr
    struct hostent *p = gethostbyname(argv[1]);
    if (p == NULL) {
        printf("gethostbyname:%s fail.reason:%s \n", argv[1], strerror(errno));
        return -1;
    }
    struct sockaddr_in servaddr;
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr = *(struct in_addr *)p-&gt;h_addr;
    servaddr.sin_port = htons(SERVER_PORT);

    // connect to server
    if (connect(sock_client, (struct sockaddr *)&amp;servaddr, sizeof(servaddr)) == -1) {
        printf("connect to server:%s port:%d fail.reason:%s \n", inet_ntoa(servaddr.sin_addr), SERVER_PORT,
        strerror(errno));
        return -1;
    }
    printf("connect to server:%s port:%d success\n", inet_ntoa(servaddr.sin_addr), SERVER_PORT );

    // send and rev data with server
    char buf[1024];
    while(fgets(buf, sizeof(buf)-1, stdin))
    {
        int len = strlen(buf);
        if (len == 1)
            break;
        buf[len-1] = '\0';
        printf("%d\n", len-1);
        ssize_t send_size = send(sock_client, buf, len, 0);
        if (send_size != len) {
            printf("send fail \n");
            break;
        }
        ssize_t rcv_size = recv(sock_client, buf, sizeof(buf), 0);
        if (rcv_size == -1) {
            printf("rcv fail\n");
            break;
        }
        printf("recv from server:%s \n", buf);
    }
    close(sock_client);
}   
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CentOS 6.5 安装ip Netns]]></title>
    <link href="http://jintao-zero.github.io/blog/2014/12/11/centos-6-dot-5-an-zhuang-ip-netns/"/>
    <updated>2014-12-11T16:42:57+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2014/12/11/centos-6-dot-5-an-zhuang-ip-netns</id>
    <content type="html"><![CDATA[<p>项目需要用到网络空间这一个技术，但是在CentOS 6.x版本中，ip命令并不能够使用netns这个参数：</p>

<pre><code>[root@gaichao ~]# ip netns
Object "netns" is unknown, try "ip help".
[root@gaichao ~]#
</code></pre>

<p>下面分别介绍以下安装失败与成功的经验：</p>

<!-- more -->


<p></p>

<h2>安装ip netns失败</h2>

<p>根据google指引，找到了这个方法<a href="https://github.com/amotoki/openvnet-test-tools/blob/master/README.md">OpenVNet test tools</a> ,也许是由于时间的推移，有的依赖做了改变，导致我在用文中方法安装netns时没有成功，下面把错误信息做一下记录：</p>

<p>CentOS 6.5内核本身是支持网络空间的，只不过系统自带的iproute不支持网络空间(没有netns子命令)</p>

<p>根据指导，需要运行以下命令安装一个支持网络空间的新版本iproute：</p>

<pre><code># yum install http://rdo.fedorapeople.org/rdo-release.rpm
# yum install iproute
</code></pre>

<p>运行第一个命令安装OpenStack RDO发布仓库到/etc/yum.repos.d目录，这个目录里面是yum命令安装、更新软件时需要用到的信息，第一步可以执行成功。</p>

<p>运行第二个命令升级iproute，报下面的错误：</p>

<pre><code>[root@gaichao ~]# yum install iproute
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
* base: mirrors.pubyun.com
* epel: mirror01.idc.hinet.net
* extras: mirrors.btte.net
* updates: mirrors.btte.net
http://repos.fedorapeople.org/repos/openstack/  openstack-juno/epel-6/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - "The requested URL  returned error: 404 Not Found"
Trying other mirror.
Error: Cannot retrieve repository metadata (repomd.xml) for repository: openstack-juno.     Please verify its path and try again
[root@gaichao ~]#  
</code></pre>

<p>根据错误提示，发现报错的文件不存在，路径已经更新为<a href="https://repos.fedorapeople.org/repos/openstack/openstack-juno/epel-7/repodata/">https://repos.fedorapeople.org/repos/openstack/openstack-juno/epel-7/repodata/</a> 所以尝试修改/etc/yum.repos.d/rdo-release.repo文件，将baseurl修改为当前可以找到repomd.xml文件的路径：</p>

<pre><code>[openstack-juno]
name=OpenStack Juno Repository
baseurl=http://repos.fedorapeople.org/repos/openstack/openstack-juno/epel-7/
enabled=1
skip_if_unavailable=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RDO-Juno
</code></pre>

<p>运行安装命令成功</p>

<pre><code>[root@gaichao ~]# yum install iproute
</code></pre>

<p>但是查看iproute安装包，并没有按照设想安装带netns功能的iproute版本</p>

<pre><code>[root@gaichao ~]# rpm -qa | grep iproute
iproute-2.6.32-33.el6_6.x86_64
[root@gaichao ~]#
</code></pre>

<p>实际应该安装到这个版本iproute-2.6.32-130.el6ost.netns.2.x86_64<br/>
后来又尝试把iproute-2.6.32-130.el6ost.netns.2.x86_64 rpm包下载到本地进行安装，但是也没有成功</p>

<h2>安装ip netns 成功</h2>

<p>继续网络中搜索，发现<a href="http://digoal126.wap.blog.163.com/w2/blogDetail.do;jsessionid=C9F7702BA05F4C979FFF72779D38F7D9.blog84-8010?blogId=fks_087070086086089066083081080068072087087064092081081067080086&amp;showRest=true&amp;p=5&amp;hostID=digoal@126">德哥@Digoal的博客</a>的方法是可行的<br/>
根据文中的提示juno下已经没有epel6目录了，所以可以选择havana，路径如下：<a href="https://repos.fedorapeople.org/repos/openstack/openstack-havana/epel-6/">https://repos.fedorapeople.org/repos/openstack/openstack-havana/epel-6/</a> 需要修改/etc/yum.repos.d/rdo-release.repo中的baseurl:</p>

<pre><code>[openstack-juno]
name=OpenStack Juno Repository
baseurl=http://repos.fedorapeople.org/repos/openstack/openstack-havana/epel-6/
enabled=1
skip_if_unavailable=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RDO-Juno
</code></pre>

<p>因为系统已经按照了iproute包，所以可以执行升级命令，进行升级：</p>

<pre><code>yum upgrade iproute 
</code></pre>

<p>执行成功后，运行以下命令查看iproute是否已经升级成为带netns命令</p>

<pre><code># rpm -qa | grep iproute
iproute-2.6.32-130.el6ost.netns.2.x86_64
# ip netns
# ip netns add test
# ip netns
test
# ip netns delete test
</code></pre>

<p>笔者安装成功，主要参考上面两个链接的内容，感谢</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[学习Markdown语法]]></title>
    <link href="http://jintao-zero.github.io/blog/2014/11/03/xue-xi-markdownyu-fa/"/>
    <updated>2014-11-03T20:23:52+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2014/11/03/xue-xi-markdownyu-fa</id>
    <content type="html"><![CDATA[<p>学习Markdown基本语法</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World]]></title>
    <link href="http://jintao-zero.github.io/blog/2014/10/17/hello-world/"/>
    <updated>2014-10-17T16:46:20+08:00</updated>
    <id>http://jintao-zero.github.io/blog/2014/10/17/hello-world</id>
    <content type="html"><![CDATA[<p>属于自己的个人博客终于搭建起来 ：）。
利用这个博客，我首先是做一些自己学习、工作中的笔记，然后也希望能够写一些有价值的思考帮助到别人。</p>

<p>搭建博客感谢以下开源项目：</p>

<h2>Octopress</h2>

<p><a href="http://octopress.org/">Octopress</a>是一款基于<a href="https://github.com/jekyll/jekyll">Jekyll</a>的静态站点生成系统，它很大程度上简化了用Jekyll搭建博客的过程。</p>

<h2>Github</h2>

<p><a href="https://pages.github.com/">Github</a>是一个面向全球开发者提供的免费、开源的代码托管项目。Github用户可以上传静态页面为自己建立个人主页或者项目主页，利用Githbub提供的免费主机以及子域名，我们可以将Octopress生成的静态页面上传到Github上建立个人博客。：）</p>

<h2>Markdown</h2>

<p><a href="http://zh.wikipedia.org/wiki/Markdown">Markdown</a>是一种轻量级标记语言，创始人约翰·格鲁伯（John Gruber）。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。[1]这种语言吸收了很多在电子邮件中已有的纯文本标记的特性。<br/>
Octopress可以将用Markdown编写的文本转化为html静态页面，两者配合可以快捷的撰写博客</p>

<h2>参考博客</h2>

<p>在搭建过程中，参考了以下博客，在此表示感谢：<br/>
<a href="http://812lcl.com/">812lcl的博客</a><br/>
<a href="http://codemacro.com/">loop in codes</a><br/>
<a href="http://yang3wei.github.io/">yang3wei的专栏</a></p>

<p>Thanks to Opene Source! this is my blog, welcome everybody.</p>
]]></content>
  </entry>
  
</feed>
